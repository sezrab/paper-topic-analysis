[{"title": "A Survey on Video Diffusion Models", "authors": ["Zhen Xing", "Qijun Feng", "Haoran Chen", "Qi Dai", "Han Hu", "Hang Xu", "Zuxuan Wu", "Yu-Gang Jiang"], "abstract": "The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10647", "tags": ["Computer Vision", "Natural Language Processing"]}, {"title": "Towards Scenario-based Safety Validation for Autonomous Trains with Deep Generative Models", "authors": ["Thomas Decker", "Ananta R. Bhattarai", "Michael Lebacher"], "abstract": "Modern AI techniques open up ever-increasing possibilities for autonomous vehicles, but how to appropriately verify the reliability of such systems remains unclear. A common approach is to conduct safety validation based on a predefined Operational Design Domain (ODD) describing specific conditions under which a system under test is required to operate properly. However, collecting sufficient realistic test cases to ensure comprehensive ODD coverage is challenging. In this paper, we report our practical experiences regarding the utility of data simulation with deep generative models for scenario-based ODD validation. We consider the specific use case of a camera-based rail-scene segmentation system designed to support autonomous train operation. We demonstrate the capabilities of semantically editing railway scenes with deep generative models to make a limited amount of test data more representative. We also show how our approach helps to analyze the degree to which a system complies with typical ODD requirements. Specifically, we focus on evaluating proper operation under different lighting and weather conditions as well as while transitioning between them.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10635", "tags": ["AI in Autonomous Vehicles", "Cyber Security"]}, {"title": "TacticAI: an AI assistant for football tactics", "authors": ["Zhe Wang", "Petar Veli\u010dkovi\u0107", "Daniel Hennes", "Nenad Toma\u0161ev", "Laurel Prince", "Michael Kaisers", "Yoram Bachrach", "Romuald Elie", "Li Kevin Wenliang", "Federico Piccinini", "William Spearman", "Ian Graham", "Jerome Connor", "Yi Yang", "Adri\u00e0 Recasens", "Mina Khan", "Nathalie Beauguerlange", "Pablo Sprechmann", "Pol Moreno", "Nicolas Heess", "Michael Bowling", "Demis Hassabis", "Karl Tuyls"], "abstract": "Identifying key patterns of tactics implemented by rival teams, and developing effective responses, lies at the heart of modern football. However, doing so algorithmically remains an open research challenge. To address this unmet need, we propose TacticAI, an AI football tactics assistant developed and evaluated in close collaboration with domain experts from Liverpool FC. We focus on analysing corner kicks, as they offer coaches the most direct opportunities for interventions and improvements. TacticAI incorporates both a predictive and a generative component, allowing the coaches to effectively sample and explore alternative player setups for each corner kick routine and to select those with the highest predicted likelihood of success. We validate TacticAI on a number of relevant benchmark tasks: predicting receivers and shot attempts and recommending player position adjustments. The utility of TacticAI is validated by a qualitative study conducted with football domain experts at Liverpool FC. We show that TacticAI's model suggestions are not only indistinguishable from real tactics, but also favoured over existing tactics 90% of the time, and that TacticAI offers an effective corner kick retrieval system. TacticAI achieves these results despite the limited availability of gold-standard data, achieving data efficiency through geometric deep learning.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10553", "tags": []}, {"title": "Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey", "authors": ["Mai Le", "Thien Huynh-The", "Tan Do-Duy", "Thai-Hoc Vu", "Won-Joo Hwang", "Quoc-Viet Pham"], "abstract": "The emergence of new services and applications in emerging wireless networks (e.g., beyond 5G and 6G) has shown a growing demand for the usage of artificial intelligence (AI) in the Internet of Things (IoT). However, the proliferation of massive IoT connections and the availability of computing resources distributed across future IoT systems have strongly demanded the development of distributed AI for better IoT services and applications. Therefore, existing AI-enabled IoT systems can be enhanced by implementing distributed machine learning (aka distributed learning) approaches. This work aims to provide a comprehensive survey on distributed learning for IoT services and applications in emerging networks. In particular, we first provide a background of machine learning and present a preliminary to typical distributed learning approaches, such as federated learning, multi-agent reinforcement learning, and distributed inference. Then, we provide an extensive review of distributed learning for critical IoT services (e.g., data sharing and computation offloading, localization, mobile crowdsensing, and security and privacy) and IoT applications (e.g., smart healthcare, smart grid, autonomous vehicle, aerial IoT networks, and smart industry). From the reviewed literature, we also present critical challenges of distributed learning for IoT and propose several promising solutions and research directions in this emerging area.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10549", "tags": []}, {"title": "Microscaling Data Formats for Deep Learning", "authors": ["Bita Darvish Rouhani", "Ritchie Zhao", "Ankit More", "Mathew Hall", "Alireza Khodamoradi", "Summer Deng", "Dhruv Choudhary", "Marius Cornea", "Eric Dellinger", "Kristof Denolf", "Stosic Dusan", "Venmugil Elango", "Maximilian Golub", "Alexander Heinecke", "Phil James-Roxby", "Dharmesh Jani", "Gaurav Kolhe", "Martin Langhammer", "Ada Li", "Levi Melnick", "Maral Mesmakhosroshahi", "Andres Rodriguez", "Michael Schulte", "Rasoul Shafipour", "Lei Shao", "et al. (7 additional authors not shown)"], "abstract": "Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements.MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10537", "tags": ["AI and Privacy", "Large scale Machine Learning"]}, {"title": "Evaluation and improvement of Segment Anything Model for interactive histopathology image segmentation", "authors": ["SeungKyu Kim", "Hyun-Jic Oh", "Seonghui Min", "Won-Ki Jeong"], "abstract": "With the emergence of the Segment Anything Model (SAM) as a foundational model for image segmentation, its application has been extensively studied across various domains, including the medical field. However, its potential in the context of histopathology data, specifically in region segmentation, has received relatively limited attention. In this paper, we evaluate SAM's performance in zero-shot and fine-tuned scenarios on histopathology data, with a focus on interactive segmentation. Additionally, we compare SAM with other state-of-the-art interactive models to assess its practical potential and evaluate its generalization capability with domain adaptability. In the experimental results, SAM exhibits a weakness in segmentation performance compared to other models while demonstrating relative strengths in terms of inference time and generalization capability. To improve SAM's limited local refinement ability and to enhance prompt stability while preserving its core strengths, we propose a modification of SAM's decoder. The experimental results suggest that the proposed modification is effective to make SAM useful for interactive histology image segmentation. The code is available at \\url{https://github.com/hvcl/SAM_Interactive_Histopathology}", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10493", "tags": ["Computer Vision"]}, {"title": "Measurement of the Born cross sections for $e^+e^-\\to\u03b7\u03c0^+\u03c0^-$ at center-of-mass energies between 2.00 and 3.08 GeV", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "M. R. An", "Q. An", "Y. Bai", "O. Bakina", "I. Balossino", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "et al. (605 additional authors not shown)"], "abstract": "Using data samples collected at center-of-mass energies between 2.000 and 3.080 GeV with the BESIII detector operating at the BEPCII collider, a partial-wave analysis is performed on the process $e^+e^-\\to\u03b7\u03c0^+\u03c0^-$. In addition to the dominant $e^+e^-\\to\u03c1\u03b7$ component, the $e^+e^-\\to a_2(1320)\u03c0$ process is also sizeable, contributing up to 24% of the total reaction. The measured Born cross sections of the process $e^+e^-\\to\u03b7\u03c0^+\u03c0^-$ are systematically higher than those of BaBar by more than $3\u03c3$ at center-of-mass energies between 2.000 and 2.300 GeV. In the Born cross section lineshape for $e^+e^-\\to a_2(1320)\u03c0$, a resonant structure is observed with a significance of $5.7\u03c3$, with $M=(2040\\pm28\\pm2)$ MeV/$c^2$, $\u0393=(160\\pm67\\pm3)$ MeV and $\\mathcal{B_{R}}\\cdot\u0393_{e^+e^-}^{R}=(38.6\\pm21.3\\pm5.0)$ eV or $(123.8\\pm73.0\\pm4.4)$ eV. In the Born cross section lineshape for $e^+e^-\\to\u03c1\u03b7$, an evidence of a dip structure around 2180 MeV/$c^2$ is observed with statistical significance of $3.1\u03c3$.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10452", "tags": []}, {"title": "Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models", "authors": ["Lochan Basyal", "Mihir Sanghvi"], "abstract": "Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large Language Models (LLMs) when applied to different datasets. The assessment of these models' effectiveness contributes valuable insights to researchers and practitioners within the NLP domain. This work serves as a resource for those interested in harnessing the potential of LLMs for text summarization and lays the foundation for the development of advanced Generative AI applications aimed at addressing a wide spectrum of business challenges.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10449", "tags": ["Natural Language Processing"]}, {"title": "Large Language Model-Empowered Agents for Simulating Macroeconomic Activities", "authors": ["Nian Li", "Chen Gao", "Yong Li", "Qingmin Liao"], "abstract": "The advent of the Web has brought about a paradigm shift in traditional economics, particularly in the digital economy era, enabling the precise recording and analysis of individual economic behavior. This has led to a growing emphasis on data-driven modeling in macroeconomics. In macroeconomic research, Agent-based modeling (ABM) emerged as an alternative, evolving through rule-based agents, machine learning-enhanced decision-making, and, more recently, advanced AI agents. However, the existing works are suffering from three main challenges when endowing agents with human-like decision-making, including agent heterogeneity, the influence of macroeconomic trends, and multifaceted economic factors. Large language models (LLMs) have recently gained prominence in offering autonomous human-like characteristics. Therefore, leveraging LLMs in macroeconomic simulation presents an opportunity to overcome traditional limitations. In this work, we take an early step in introducing a novel approach that leverages LLMs in macroeconomic simulation. We design prompt-engineering-driven LLM agents to exhibit human-like decision-making and adaptability in the economic environment, with the abilities of perception, reflection, and decision-making to address the abovementioned challenges. Simulation experiments on macroeconomic activities show that LLM-empowered agents can make realistic work and consumption decisions and emerge more reasonable macroeconomic phenomena than existing rule-based or AI agents. Our work demonstrates the promising potential to simulate macroeconomics based on LLM and its human-like characteristics.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10436", "tags": []}, {"title": "Unlocking Metasurface Practicality for B5G Networks: AI-assisted RIS Planning", "authors": ["Guillermo Encinas-Lago", "Antonio Albanese", "Vincenzo Sciancalepore", "Marco Di Renzo", "Xavier Costa-P\u00e9rez"], "abstract": "The advent of reconfigurable intelligent surfaces(RISs) brings along significant improvements for wireless technology on the verge of beyond-fifth-generation networks (B5G).The proven flexibility in influencing the propagation environment opens up the possibility of programmatically altering the wireless channel to the advantage of network designers, enabling the exploitation of higher-frequency bands for superior throughput overcoming the challenging electromagnetic (EM) propagation properties at these frequency bands.\n  However, RISs are not magic bullets. Their employment comes with significant complexity, requiring ad-hoc deployments and management operations to come to fruition. In this paper, we tackle the open problem of bringing RISs to the field, focusing on areas with little or no coverage. In fact, we present a first-of-its-kind deep reinforcement learning (DRL) solution, dubbed as D-RISA, which trains a DRL agent and, in turn, obtain san optimal RIS deployment. We validate our framework in the indoor scenario of the Rennes railway station in France, assessing the performance of our algorithm against state-of-the-art (SOA) approaches. Our benchmarks showcase better coverage, i.e., 10-dB increase in minimum signal-to-noise ratio (SNR), at lower computational time (up to -25 percent) while improving scalability towards denser network deployments.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10330", "tags": []}, {"title": "Mimicking the Maestro: Exploring the Efficacy of a Virtual AI Teacher in Fine Motor Skill Acquisition", "authors": ["Hadar Mulian", "Segev Shlomov", "Lior Limonad"], "abstract": "Motor skills, especially fine motor skills like handwriting, play an essential role in academic pursuits and everyday life. Traditional methods to teach these skills, although effective, can be time-consuming and inconsistent. With the rise of advanced technologies like robotics and artificial intelligence, there is increasing interest in automating such teaching processes using these technologies, via human-robot and human-computer interactions. In this study, we examine the potential of a virtual AI teacher in emulating the techniques of human educators for motor skill acquisition. We introduce an AI teacher model that captures the distinct characteristics of human instructors. Using a Reinforcement Learning environment tailored to mimic teacher-learner interactions, we tested our AI model against four guiding hypotheses, emphasizing improved learner performance, enhanced rate of skill acquisition, and reduced variability in learning outcomes. Our findings, validated on synthetic learners, revealed significant improvements across all tested hypotheses. Notably, our model showcased robustness across different learners and settings and demonstrated adaptability to handwriting. This research underscores the potential of integrating Reinforcement Learning and Imitation Learning models with robotics in revolutionizing the teaching of critical motor skills.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10280", "tags": ["AI in Education", "AI in Healthcare"]}, {"title": "Enhancing Interpretability using Human Similarity Judgements to Prune Word Embeddings", "authors": ["Natalia Flechas Manrique", "Wanqian Bao", "Aurelie Herbelot", "Uri Hasson"], "abstract": "Interpretability methods in NLP aim to provide insights into the semantics underlying specific system architectures. Focusing on word embeddings, we present a supervised-learning method that, for a given domain (e.g., sports, professions), identifies a subset of model features that strongly improve prediction of human similarity judgments. We show this method keeps only 20-40% of the original embeddings, for 8 independent semantic domains, and that it retains different feature sets across domains. We then present two approaches for interpreting the semantics of the retained features. The first obtains the scores of the domain words (co-hyponyms) on the first principal component of the retained embeddings, and extracts terms whose co-occurrence with the co-hyponyms tracks these scores' profile. This analysis reveals that humans differentiate e.g. sports based on how gender-inclusive and international they are. The second approach uses the retained sets as variables in a probing task that predicts values along 65 semantically annotated dimensions for a dataset of 535 words. The features retained for professions are best at predicting cognitive, emotional and social dimensions, whereas features retained for fruits or vegetables best predict the gustation (taste) dimension. We discuss implications for alignment between AI systems and human knowledge.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10262", "tags": []}, {"title": "Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison", "authors": ["Shuo Sun", "Yuchen Zhang", "Jiahuan Yan", "Yuze Gao", "Donovan Ong", "Bin Chen", "Jian Su"], "abstract": "The success of ChatGPT has ignited an AI race, with researchers striving to develop new large language models (LLMs) that can match or surpass the language understanding and generation abilities of commercial ones. In recent times, a number of models have emerged, claiming performance near that of GPT-3.5 or GPT-4 through various instruction-tuning methods. As practitioners of Text-to-SQL parsing, we are grateful for their valuable contributions to open-source research. However, it is important to approach these claims with a sense of scrutiny and ascertain the actual effectiveness of these models. Therefore, we pit six popular large language models against each other, systematically evaluating their Text-to-SQL parsing capability on nine benchmark datasets with five different prompting strategies, covering both zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell significantly short of the performance achieved by closed-source models like GPT-3.5, highlighting the need for further work to bridge the performance gap between these models.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10190", "tags": []}, {"title": "Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space", "authors": ["Yao Qianxiang", "Bin Jiang"], "abstract": "This study introduces the concept of \"structural beauty\" as an objective computational approach for evaluating the aesthetic appeal of images. Through the utilization of the Segment anything model (SAM), we propose a method that leverages recursive segmentation to extract finer-grained substructures. Additionally, by reconstructing the hierarchical structure, we obtain a more accurate representation of substructure quantity and hierarchy. This approach reproduces and extends our previous research, allowing for the simultaneous assessment of Livingness in full-color images without the need for grayscale conversion or separate computations for foreground and background Livingness. Furthermore, the application of our method to the Scenic or Not dataset, a repository of subjective scenic ratings, demonstrates a high degree of consistency with subjective ratings in the 0-6 score range. This underscores that structural beauty is not solely a subjective perception, but a quantifiable attribute accessible through objective computation. Through our case studies, we have arrived at three significant conclusions. 1) our method demonstrates the capability to accurately segment meaningful objects, including trees, buildings, and windows, as well as abstract substructures within paintings. 2) we observed that the clarity of an image impacts our computational results; clearer images tend to yield higher Livingness scores. However, for equally blurry images, Livingness does not exhibit a significant reduction, aligning with human visual perception. 3) our approach fundamentally differs from methods employing Convolutional Neural Networks (CNNs) for predicting image scores. Our method not only provides computational results but also offers transparency and interpretability, positioning it as a novel avenue in the realm of Explainable AI (XAI).", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10149", "tags": ["Computer Vision"]}, {"title": "Solution to Advanced Manufacturing Process Problems using Cohort Intelligence Algorithm with Improved Constraint Handling Approaches", "authors": ["Aniket Nargundkar", "Madhav Rawal", "Aryaman Patel", "Anand J Kulkarni", "Apoorva S Shastri"], "abstract": "Recently, various Artificial Intelligence (AI) based optimization metaheuristics are proposed and applied for a variety of problems. Cohort Intelligence (CI) algorithm is a socio inspired optimization technique which is successfully applied for solving several unconstrained & constrained real-world problems from the domains such as design, manufacturing, supply chain, healthcare, etc. Generally, real-world problems are constrained in nature. Even though most of the Evolutionary Algorithms (EAs) can efficiently solve unconstrained problems, their performance degenerates when the constraints are involved. In this paper, two novel constraint handling approaches based on modulus and hyperbolic tangent probability distributions are proposed. Constrained CI algorithm with constraint handling approaches based on triangular, modulus and hyperbolic tangent is presented and applied for optimizing advanced manufacturing processes such as Water Jet Machining (WJM), Abrasive Jet Machining (AJM), Ultrasonic Machining (USM) and Grinding process. The solutions obtained using proposed CI algorithm are compared with contemporary algorithms such as Genetic Algorithm, Simulated Annealing, Teaching Learning Based Optimization, etc. The proposed approaches achieved 2%-127% maximization of material removal rate satisfying hard constraints. As compared to the GA, CI with Hyperbolic tangent probability distribution achieved 15%, 2%, 2%, 127%, and 4% improvement in MRR for AJMB, AJMD, WJM, USM, and Grinding processes, respectively contributing to the productivity improvement. The contributions in this paper have opened several avenues for further applicability of the proposed constraint handling approaches for solving complex constrained problems.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10085", "tags": []}, {"title": "Verbosity Bias in Preference Labeling by Large Language Models", "authors": ["Keita Saito", "Akifumi Wachi", "Koki Wataoka", "Youhei Akimoto"], "abstract": "In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.10076", "tags": []}, {"title": "Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets", "authors": ["Shm Garanganao Almeda", "J. D. Zamfirescu-Pereira", "Kyu Won Kim", "Pradeep Mani Rathnam", "Bjoern Hartmann"], "abstract": "Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Minor adjustments to prompt input can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports exploration strategies with LLM-based functions for assisted prompt construction and simultaneous display of generated results, hosted in a spreadsheet interface. The flexible layout and novel generative functions enable experimentation with user-defined workflows. Two studies, a preliminary lab study and a longitudinal study with five expert artists, revealed a set of strategies participants use to tackle the challenges of TTI design space exploration, and the interface features required to support them - like using text-generation to define local \"axes\" of exploration. We distill these insights into a UI mockup to guide future interfaces.", "submitted": "2023-10-15", "link": "https://arxiv.org/pdf/2310.09985", "tags": []}, {"title": "FiLM: Fill-in Language Models for Any-Order Generation", "authors": ["Tianxiao Shen", "Hao Peng", "Ruoqi Shen", "Yao Fu", "Zaid Harchaoui", "Yejin Choi"], "abstract": "Language models have become the backbone of today's AI systems. However, their predominant left-to-right generation limits the use of bidirectional context, which is essential for tasks that involve filling text in the middle. We propose the Fill-in Language Model (FiLM), a new language modeling approach that allows for flexible generation at any position without adhering to a specific generation order. Its training extends the masked language modeling objective by adopting varying mask probabilities sampled from the Beta distribution to enhance the generative capabilities of FiLM. During inference, FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring that the outputs are fluent and are coherent with the surrounding context. In both automatic and human evaluations, FiLM outperforms existing infilling methods that rely on left-to-right language models trained on rearranged text segments. FiLM is easy to implement and can be either trained from scratch or fine-tuned from a left-to-right language model. Notably, as the model size grows, FiLM's perplexity approaches that of strong left-to-right language models of similar sizes, indicating FiLM's scalability and potential as a large language model.", "submitted": "2023-10-15", "link": "https://arxiv.org/pdf/2310.09930", "tags": ["Natural Language Processing"]}, {"title": "Explaining How a Neural Network Play the Go Game and Let People Learn", "authors": ["Huilin Zhou", "Huijie Tang", "Mingjie Li", "Hao Zhang", "Zhenyu Liu", "Quanshi Zhang"], "abstract": "The AI model has surpassed human players in the game of Go, and it is widely believed that the AI model has encoded new knowledge about the Go game beyond human players. In this way, explaining the knowledge encoded by the AI model and using it to teach human players represent a promising-yet-challenging issue in explainable AI. To this end, mathematical supports are required to ensure that human players can learn accurate and verifiable knowledge, rather than specious intuitive analysis. Thus, in this paper, we extract interaction primitives between stones encoded by the value network for the Go game, so as to enable people to learn from the value network. Experiments show the effectiveness of our method.", "submitted": "2023-10-15", "link": "https://arxiv.org/pdf/2310.09838", "tags": ["AI in Healthcare", "AI and Privacy"]}, {"title": "Model Inversion Attacks on Homogeneous and Heterogeneous Graph Neural Networks", "authors": ["Renyang Liu", "Wei Zhou", "Jinhong Zhang", "Xiaoyuan Liu", "Peiyuan Si", "Haoran Li"], "abstract": "Recently, Graph Neural Networks (GNNs), including Homogeneous Graph Neural Networks (HomoGNNs) and Heterogeneous Graph Neural Networks (HeteGNNs), have made remarkable progress in many physical scenarios, especially in communication applications. Despite achieving great success, the privacy issue of such models has also received considerable attention. Previous studies have shown that given a well-fitted target GNN, the attacker can reconstruct the sensitive training graph of this model via model inversion attacks, leading to significant privacy worries for the AI service provider. We advocate that the vulnerability comes from the target GNN itself and the prior knowledge about the shared properties in real-world graphs. Inspired by this, we propose a novel model inversion attack method on HomoGNNs and HeteGNNs, namely HomoGMI and HeteGMI. Specifically, HomoGMI and HeteGMI are gradient-descent-based optimization methods that aim to maximize the cross-entropy loss on the target GNN and the $1^{st}$ and $2^{nd}$-order proximities on the reconstructed graph. Notably, to the best of our knowledge, HeteGMI is the first attempt to perform model inversion attacks on HeteGNNs. Extensive experiments on multiple benchmarks demonstrate that the proposed method can achieve better performance than the competitors.", "submitted": "2023-10-15", "link": "https://arxiv.org/pdf/2310.09800", "tags": ["Deep Learning"]}, {"title": "SCME: A Self-Contrastive Method for Data-free and Query-Limited Model Extraction Attack", "authors": ["Renyang Liu", "Jinhong Zhang", "Kwok-Yan Lam", "Jun Zhao", "Wei Zhou"], "abstract": "Previous studies have revealed that artificial intelligence (AI) systems are vulnerable to adversarial attacks. Among them, model extraction attacks fool the target model by generating adversarial examples on a substitute model. The core of such an attack is training a substitute model as similar to the target model as possible, where the simulation process can be categorized in a data-dependent and data-free manner. Compared with the data-dependent method, the data-free one has been proven to be more practical in the real world since it trains the substitute model with synthesized data. However, the distribution of these fake data lacks diversity and cannot detect the decision boundary of the target model well, resulting in the dissatisfactory simulation effect. Besides, these data-free techniques need a vast number of queries to train the substitute model, increasing the time and computing consumption and the risk of exposure. To solve the aforementioned problems, in this paper, we propose a novel data-free model extraction method named SCME (Self-Contrastive Model Extraction), which considers both the inter- and intra-class diversity in synthesizing fake data. In addition, SCME introduces the Mixup operation to augment the fake data, which can explore the target model's decision boundary effectively and improve the simulating capacity. Extensive experiments show that the proposed method can yield diversified fake data. Moreover, our method has shown superiority in many different attack settings under the query-limited scenario, especially for untargeted attacks, the SCME outperforms SOTA methods by 11.43\\% on average for five baseline datasets.", "submitted": "2023-10-15", "link": "https://arxiv.org/pdf/2310.09792", "tags": ["Generative Adversarial Networks"]}, {"title": "Notes on Applicability of Explainable AI Methods to Machine Learning Models Using Features Extracted by Persistent Homology", "authors": ["Naofumi Hama"], "abstract": "Data analysis that uses the output of topological data analysis as input for machine learning algorithms has been the subject of extensive research. This approach offers a means of capturing the global structure of data. Persistent homology (PH), a common methodology within the field of TDA, has found wide-ranging applications in machine learning. One of the key reasons for the success of the PH-ML pipeline lies in the deterministic nature of feature extraction conducted through PH. The ability to achieve satisfactory levels of accuracy with relatively simple downstream machine learning models, when processing these extracted features, underlines the pipeline's superior interpretability. However, it must be noted that this interpretation has encountered issues. Specifically, it fails to accurately reflect the feasible parameter region in the data generation process, and the physical or chemical constraints that restrict this process. Against this backdrop, we explore the potential application of explainable AI methodologies to this PH-ML pipeline. We apply this approach to the specific problem of predicting gas adsorption in metal-organic frameworks and demonstrate that it can yield suggestive results. The codes to reproduce our results are available at https://github.com/naofumihama/xai_ph_ml", "submitted": "2023-10-15", "link": "https://arxiv.org/pdf/2310.09780", "tags": ["AI and Privacy", "Large scale Machine Learning"]}, {"title": "NTT-PIM: Row-Centric Architecture and Mapping for Efficient Number-Theoretic Transform on PIM", "authors": ["Jaewoo Park", "Sugil Lee", "Jongeun Lee"], "abstract": "Recently DRAM-based PIMs (processing-in-memories) with unmodified cell arrays have demonstrated impressive performance for accelerating AI applications. However, due to the very restrictive hardware constraints, PIM remains an accelerator for simple functions only. In this paper we propose NTT-PIM, which is based on the same principles such as no modification of cell arrays and very restrictive area budget, but shows state-of-the-art performance for a very complex application such as NTT, thanks to features optimized for the application's characteristics, such as in-place update and pipelining via multiple buffers. Our experimental results demonstrate that our NTT-PIM can outperform previous best PIM-based NTT accelerators in terms of runtime by 1.7 ~ 17 times while having negligible area and power overhead.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09715", "tags": []}, {"title": "A Framework For Automated Dissection Along Tissue Boundary", "authors": ["Ki-Hwan Oh", "Leonardo Borgioli", "Milos Zefran", "Liaohai Chen", "Pier Cristoforo Giulianotti"], "abstract": "Robotic surgery promises enhanced precision and adaptability over traditional surgical methods. It also offers the possibility of automating surgical interventions, resulting in reduced stress on the surgeon, better surgical outcomes, and lower costs. Cholecystectomy, the removal of the gallbladder, serves as an ideal model procedure for automation due to its distinct and well-contrasted anatomical features between the gallbladder and liver, along with standardized surgical maneuvers. Dissection is a frequently used subtask in cholecystectomy where the surgeon delivers the energy on the hook to detach the gallbladder from the liver. Hence, dissection along tissue boundaries is a good candidate for surgical automation. For the da Vinci surgical robot to perform the same procedure as a surgeon automatically, it needs to have the ability to (1) recognize and distinguish between the two different tissues (e.g. the liver and the gallbladder), (2) understand where the boundary between the two tissues is located in the 3D workspace, (3) locate the instrument tip relative to the boundary in the 3D space using visual feedback, and (4) move the instrument along the boundary. This paper presents a novel framework that addresses these challenges through AI-assisted image processing and vision-based robot control. We also present the ex-vivo evaluation of the automated procedure on chicken and pork liver specimens that demonstrates the effectiveness of the proposed framework.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09669", "tags": []}, {"title": "Legend at ArAIEval Shared Task: Persuasion Technique Detection using a Language-Agnostic Text Representation Model", "authors": ["Olumide E. Ojo", "Olaronke O. Adebanji", "Hiram Calvo", "Damian O. Dieke", "Olumuyiwa E. Ojo", "Seye E. Akinsanya", "Tolulope O. Abiola", "Anna Feldman"], "abstract": "In this paper, we share our best performing submission to the Arabic AI Tasks Evaluation Challenge (ArAIEval) at ArabicNLP 2023. Our focus was on Task 1, which involves identifying persuasion techniques in excerpts from tweets and news articles. The persuasion technique in Arabic texts was detected using a training loop with XLM-RoBERTa, a language-agnostic text representation model. This approach proved to be potent, leveraging fine-tuning of a multilingual language model. In our evaluation of the test set, we achieved a micro F1 score of 0.64 for subtask A of the competition.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09661", "tags": ["Natural Language Processing"]}, {"title": "Multimodal Federated Learning in Healthcare: a review", "authors": ["Jacob Thrasher", "Alina Devkota", "Prasiddha Siwakotai", "Rohit Chivukula", "Pranav Poudel", "Chaunbo Hu", "Binod Bhattarai", "Prashnna Gyawali"], "abstract": "Recent advancements in multimodal machine learning have empowered the development of accurate and robust AI systems in the medical domain, especially within centralized database systems. Simultaneously, Federated Learning (FL) has progressed, providing a decentralized mechanism where data need not be consolidated, thereby enhancing the privacy and security of sensitive healthcare data. The integration of these two concepts supports the ongoing progress of multimodal learning in healthcare while ensuring the security and privacy of patient records within local data-holding agencies. This paper offers a concise overview of the significance of FL in healthcare and outlines the current state-of-the-art approaches to Multimodal Federated Learning (MMFL) within the healthcare domain. It comprehensively examines the existing challenges in the field, shedding light on the limitations of present models. Finally, the paper outlines potential directions for future advancements in the field, aiming to bridge the gap between cutting-edge AI technology and the imperative need for patient data privacy in healthcare applications.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09650", "tags": ["AI and Privacy", "AI in Healthcare"]}, {"title": "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models", "authors": ["Alex Mei", "Sharon Levy", "William Yang Wang"], "abstract": "As large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. For robust safety evaluation, we apply these methods in the critical domain of AI safety to algorithmically generate a test suite of prompts covering diverse robustness settings -- semantic equivalence, related scenarios, and adversarial. We partition our prompts into four safety domains for a fine-grained analysis of how the domain affects model performance. Despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users' physical safety.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09624", "tags": ["Generative Adversarial Networks", "Natural Language Processing"]}, {"title": "Towards Intelligent Network Management: Leveraging AI for Network Service Detection", "authors": ["Khuong N. Nguyen", "Abhishek Sehgal", "Yuming Zhu", "Junsu Choi", "Guanbo Chen", "Hao Chen", "Boon Loong Ng", "Charlie Zhang"], "abstract": "As the complexity and scale of modern computer networks continue to increase, there has emerged an urgent need for precise traffic analysis, which plays a pivotal role in cutting-edge wireless connectivity technologies. This study focuses on leveraging Machine Learning methodologies to create an advanced network traffic classification system. We introduce a novel data-driven approach that excels in identifying various network service types in real-time, by analyzing patterns within the network traffic. Our method organizes similar kinds of network traffic into distinct categories, referred to as network services, based on latency requirement. Furthermore, it decomposes the network traffic stream into multiple, smaller traffic flows, with each flow uniquely carrying a specific service. Our ML models are trained on a dataset comprised of labeled examples representing different network service types collected on various Wi-Fi network conditions. Upon evaluation, our system demonstrates a remarkable accuracy in distinguishing the network services. These results emphasize the substantial promise of integrating Artificial Intelligence in wireless technologies. Such an approach encourages more efficient energy consumption, enhances Quality of Service assurance, and optimizes the allocation of network resources, thus laying a solid groundwork for the development of advanced intelligent networks.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09609", "tags": ["Neuromorphic Computing", "Generative Adversarial Networks"]}, {"title": "Penetrative AI: Making LLMs Comprehend the Physical World", "authors": ["Huatao Xu", "Liying Han", "Mo Li", "Mani Srivastava"], "abstract": "Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term \"\\textit{Penetrative AI}\". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the knowledge they learned during training for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09605", "tags": []}, {"title": "Wafer-scale Computing: Advancements, Challenges, and Future Perspectives", "authors": ["Yang Hu", "Xinhan Lin", "Huizheng Wang", "Zhen He", "Xingmao Yu", "Jiahao Zhang", "Qize Yang", "Zheng Xu", "Sihan Guan", "Jiahao Fang", "Haoran Shang", "Xinru Tang", "Xu Dai", "Shaojun Wei", "Shouyi Yin"], "abstract": "Nowadays, artificial intelligence (AI) technology with large models plays an increasingly important role in both academia and industry. It also brings a rapidly increasing demand for the computing power of the hardware. As the computing demand for AI continues to grow, the growth of hardware computing power has failed to keep up. This has become a significant factor restricting the development of AI. The augmentation of hardware computing power is mainly propelled by the escalation of transistor density and chip area. However, the former is impeded by the termination of the Moore's Law and Dennard scaling, and the latter is significantly restricted by the challenge of disrupting the legacy fabrication equipment and process.\n  In recent years, advanced packaging technologies that have gradually matured are increasingly used to implement bigger chips that integrate multiple chiplets, while still providing interconnections with chip-level density and bandwidth. Compared to conventional high-performance computing paradigms such as multi-accelerator and datacenter-scale computing, Wafer-scale Computing shows remarkable advantages in communication bandwidth, integration density, and programmability potential. Not surprisingly, disruptive Wafer-scale Computing also brings unprecedented design challenges for hardware architecture, design-system-technology co-optimization, power and cooling systems, and compiler tool chain. At present, there are no comprehensive surveys summarizing the current state and design insights of Wafer-scale Computing. This paper aims to take the first step to help academia and industry review existing wafer-scale chips and essential technologies in a one-stop manner. So that people can conveniently grasp the basic knowledge and key points, understand the achievements and shortcomings of existing research, and contribute to this promising research direction.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09568", "tags": []}, {"title": "Scene Text Recognition Models Explainability Using Local Features", "authors": ["Mark Vincent Ty", "Rowel Atienza"], "abstract": "Explainable AI (XAI) is the study on how humans can be able to understand the cause of a model's prediction. In this work, the problem of interest is Scene Text Recognition (STR) Explainability, using XAI to understand the cause of an STR model's prediction. Recent XAI literatures on STR only provide a simple analysis and do not fully explore other XAI methods. In this study, we specifically work on data explainability frameworks, called attribution-based methods, that explain the important parts of an input data in deep learning models. However, integrating them into STR produces inconsistent and ineffective explanations, because they only explain the model in the global context. To solve this problem, we propose a new method, STRExp, to take into consideration the local explanations, i.e. the individual character prediction explanations. This is then benchmarked across different attribution-based methods on different STR datasets and evaluated across different STR models.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09549", "tags": ["Explainable AI (XAI):"]}, {"title": "Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction", "authors": ["Jai Pal", "Bryan Hong"], "abstract": "Artificial intelligence (AI) is a powerful tool for reshaping healthcare systems. In healthcare, AI is invaluable for its capacity to manage vast amounts of data, which can lead to more accurate and speedy diagnoses, ultimately easing the workload on healthcare professionals. As a result, AI has proven itself to be a power tool across various industries, simplifying complex tasks and pattern recognition that would otherwise be overwhelming for humans or traditional computer algorithms. In this paper, we review the strengths and weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring cutting edge virus analysis to healthcare professionals around the world. The model's accuracy assessment revealed promising results, with room for improvement primarily related to data organization. In addition, the severity index serves as a valuable tool to gain a broad overview of patient care needs, aligning with healthcare professionals' preference for broader categorizations.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.09485", "tags": ["AI in Healthcare", "AI and Privacy"]}, {"title": "Unified High-binding Watermark for Unconditional Image Generation Models", "authors": ["Ruinan Ma", "Yu-an Tan", "Shangbo Wu", "Tian Chen", "Yajie Wang", "Yuanzhang Li"], "abstract": "Deep learning techniques have implemented many unconditional image generation (UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images (also known as AI-Generated Content, AIGC for short) produced by these models bring urgent needs for intellectual property protection such as data traceability and copyright certification. An attacker can steal the output images of the target model and use them as part of the training data to train a private surrogate UIG model. The implementation mechanisms of UIG models are diverse and complex, and there is no unified and effective protection and verification method at present. To address these issues, we propose a two-stage unified watermark verification mechanism with high-binding effects for such models. In the first stage, we use an encoder to invisibly write the watermark image into the output images of the original AIGC tool, and reversely extract the watermark image through the corresponding decoder. In the second stage, we design the decoder fine-tuning process, and the fine-tuned decoder can make correct judgments on whether the suspicious model steals the original AIGC tool data. Experiments demonstrate our method can complete the verification work with almost zero false positive rate under the condition of only using the model output images. Moreover, the proposed method can achieve data steal verification across different types of UIG models, which further increases the practicality of the method.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09479", "tags": ["Computer Vision"]}, {"title": "Can CNNs Accurately Classify Human Emotions? A Deep-Learning Facial Expression Recognition Study", "authors": ["Ashley Jisue Hong", "David DiStefano", "Sejal Dua"], "abstract": "Emotional Artificial Intelligences are currently one of the most anticipated developments of AI. If successful, these AIs will be classified as one of the most complex, intelligent nonhuman entities as they will possess sentience, the primary factor that distinguishes living humans and mechanical machines. For AIs to be classified as \"emotional,\" they should be able to empathize with others and classify their emotions because without such abilities they cannot normally interact with humans. This study investigates the CNN model's ability to recognize and classify human facial expressions (positive, neutral, negative). The CNN model made for this study is programmed in Python and trained with preprocessed data from the Chicago Face Database. The model is intentionally designed with less complexity to further investigate its ability. We hypothesized that the model will perform better than chance (33.3%) in classifying each emotion class of input data. The model accuracy was tested with novel images. Accuracy was summarized in a percentage report, comparative plot, and confusion matrix. Results of this study supported the hypothesis as the model had 75% accuracy over 10,000 images (data), highlighting the possibility of AIs that accurately analyze human emotions and the prospect of viable Emotional AIs.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09473", "tags": ["AI and Privacy", "AI in Healthcare", "Ethical AI and Bias Mitigation"]}, {"title": "Target Variable Engineering", "authors": ["Jessica Clark"], "abstract": "How does the formulation of a target variable affect performance within the ML pipeline? The experiments in this study examine numeric targets that have been binarized by comparing against a threshold. We compare the predictive performance of regression models trained to predict the numeric targets vs. classifiers trained to predict their binarized counterparts. Specifically, we make this comparison at every point of a randomized hyperparameter optimization search to understand the effect of computational resource budget on the tradeoff between the two. We find that regression requires significantly more computational effort to converge upon the optimal performance, and is more sensitive to both randomness and heuristic choices in the training process. Although classification can and does benefit from systematic hyperparameter tuning and model selection, the improvements are much less than for regression. This work comprises the first systematic comparison of regression and classification within the framework of computational resource requirements. Our findings contribute to calls for greater replicability and efficiency within the ML pipeline for the sake of building more sustainable and robust AI systems.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09440", "tags": []}, {"title": "A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks", "authors": ["Qiming Bao", "Gael Gendron", "Alex Yuxuan Peng", "Wanjun Zhong", "Neset Tan", "Yang Chen", "Michael Witbrock", "Jiamou Liu"], "abstract": "Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named \"ReClor-plus\", \"LogiQA-plus\" and \"LogiQAv2-plus\", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by \"none of the other options are correct\", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturbing a sizable training set can markedly improve the model's generalisation and robustness in logical reasoning tasks. Moreover, applying logic-driven data augmentation for fine-tuning, combined with prompting can enhance the generalisation performance of both discriminative large language models and generative large language models. These results offer insights into assessing and improving the generalisation and robustness of large language models for logical reasoning tasks. We make our source code and data publicly available \\url{https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning}.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09430", "tags": ["Natural Language Processing"]}, {"title": "Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms", "authors": ["Xiangyu Zeng", "Jie Lin", "Piao Hu", "Ruizheng Huang", "Zhicheng Zhang"], "abstract": "How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretation of current perceptions. We exploratively apply our PMI to improve prevailing Transformers and CNN models on question-answering tasks like bAbI-20k and Sort-of-CLEVR datasets, as well as relation calculation and image classification tasks, and in each case, our PMI enhancements consistently outshine their original counterparts significantly. Visualization analyses reveal that memory consolidation, along with the interaction and integration of information from diverse memory sources, substantially contributes to the model effectiveness on inference tasks.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.09297", "tags": []}, {"title": "Augmented Computational Design: Methodical Application of Artificial Intelligence in Generative Design", "authors": ["Pirouz Nourian", "Shervin Azadi", "Roy Uijtendaal", "Nan Bai"], "abstract": "This chapter presents methodological reflections on the necessity and utility of artificial intelligence in generative design. Specifically, the chapter discusses how generative design processes can be augmented by AI to deliver in terms of a few outcomes of interest or performance indicators while dealing with hundreds or thousands of small decisions. The core of the performance-based generative design paradigm is about making statistical or simulation-driven associations between these choices and consequences for mapping and navigating such a complex decision space. This chapter will discuss promising directions in Artificial Intelligence for augmenting decision-making processes in architectural design for mapping and navigating complex design spaces.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09243", "tags": []}, {"title": "Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration", "authors": ["Yiquan Wu", "Siying Zhou", "Yifei Liu", "Weiming Lu", "Xiaozhong Liu", "Yating Zhang", "Changlong Sun", "Fei Wu", "Kun Kuang"], "abstract": "Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP), a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09241", "tags": []}, {"title": "Multinational AGI Consortium (MAGIC): A Proposal for International Coordination on AI", "authors": ["Jason Hausenloy", "Andrea Miotti", "Claire Dennis"], "abstract": "This paper proposes a Multinational Artificial General Intelligence Consortium (MAGIC) to mitigate existential risks from advanced artificial intelligence (AI). MAGIC would be the only institution in the world permitted to develop advanced AI, enforced through a global moratorium by its signatory members on all other advanced AI development. MAGIC would be exclusive, safety-focused, highly secure, and collectively supported by member states, with benefits distributed equitably among signatories. MAGIC would allow narrow AI models to flourish while significantly reducing the possibility of misaligned, rogue, breakout, or runaway outcomes of general-purpose systems. We do not address the political feasibility of implementing a moratorium or address the specific legislative strategies and rules needed to enforce a ban on high-capacity AGI training runs. Instead, we propose one positive vision of the future, where MAGIC, as a global governance regime, can lay the groundwork for long-term, safe regulation of advanced AI.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09217", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy"]}, {"title": "Lincoln AI Computing Survey (LAICS) Update", "authors": ["Albert Reuther", "Peter Michaleas", "Michael Jones", "Vijay Gadepally", "Siddharth Samsi", "Jeremy Kepner"], "abstract": "This paper is an update of the survey of AI accelerators and processors from past four years, which is now called the Lincoln AI Computing Survey - LAICS (pronounced \"lace\"). As in past years, this paper collects and summarizes the current commercial accelerators that have been publicly announced with peak performance and peak power consumption numbers. The performance and power values are plotted on a scatter graph, and a number of dimensions and observations from the trends on this plot are again discussed and analyzed. Market segments are highlighted on the scatter plot, and zoomed plots of each segment are also included. Finally, a brief description of each of the new accelerators that have been added in the survey this year is included.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09145", "tags": []}, {"title": "Insightful analysis of historical sources at scales beyond human capabilities using unsupervised Machine Learning and XAI", "authors": ["Oliver Eberle", "Jochen B\u00fcttner", "Hassan El-Hajj", "Gr\u00e9goire Montavon", "Klaus-Robert M\u00fcller", "Matteo Valleriani"], "abstract": "Historical materials are abundant. Yet, piecing together how human knowledge has evolved and spread both diachronically and synchronically remains a challenge that can so far only be very selectively addressed. The vast volume of materials precludes comprehensive studies, given the restricted number of human specialists. However, as large amounts of historical materials are now available in digital form there is a promising opportunity for AI-assisted historical analysis. In this work, we take a pivotal step towards analyzing vast historical corpora by employing innovative machine learning (ML) techniques, enabling in-depth historical insights on a grand scale. Our study centers on the evolution of knowledge within the `Sacrobosco Collection' -- a digitized collection of 359 early modern printed editions of textbooks on astronomy used at European universities between 1472 and 1650 -- roughly 76,000 pages, many of which contain astronomic, computational tables. An ML based analysis of these tables helps to unveil important facets of the spatio-temporal evolution of knowledge and innovation in the field of mathematical astronomy in the period, as taught at European universities.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09091", "tags": []}, {"title": "SAI: Solving AI Tasks with Systematic Artificial Intelligence in Communication Network", "authors": ["Lei Yao", "Yong Zhang", "Zilong Yan", "Jialu Tian"], "abstract": "In the rapid development of artificial intelligence, solving complex AI tasks is a crucial technology in intelligent mobile networks. Despite the good performance of specialized AI models in intelligent mobile networks, they are unable to handle complicated AI tasks. To address this challenge, we propose Systematic Artificial Intelligence (SAI), which is a framework designed to solve AI tasks by leveraging Large Language Models (LLMs) and JSON-format intent-based input to connect self-designed model library and database. Specifically, we first design a multi-input component, which simultaneously integrates Large Language Models (LLMs) and JSON-format intent-based inputs to fulfill the diverse intent requirements of different users. In addition, we introduce a model library module based on model cards which employ model cards to pairwise match between different modules for model composition. Model cards contain the corresponding model's name and the required performance metrics. Then when receiving user network requirements, we execute each subtask for multiple selected model combinations and provide output based on the execution results and LLM feedback. By leveraging the language capabilities of LLMs and the abundant AI models in the model library, SAI can complete numerous complex AI tasks in the communication network, achieving impressive results in network optimization, resource allocation, and other challenging tasks.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09049", "tags": ["AI in Healthcare"]}, {"title": "Optimal Scheduling of Electric Vehicle Charging with Deep Reinforcement Learning considering End Users Flexibility", "authors": ["Christoforos Menos-Aikateriniadis", "Stavros Sykiotis", "Pavlos S. Georgilakis"], "abstract": "The rapid growth of decentralized energy resources and especially Electric Vehicles (EV), that are expected to increase sharply over the next decade, will put further stress on existing power distribution networks, increasing the need for higher system reliability and flexibility. In an attempt to avoid unnecessary network investments and to increase the controllability over distribution networks, network operators develop demand response (DR) programs that incentivize end users to shift their consumption in return for financial or other benefits. Artificial intelligence (AI) methods are in the research forefront for residential load scheduling applications, mainly due to their high accuracy, high computational speed and lower dependence on the physical characteristics of the models under development. The aim of this work is to identify households' EV cost-reducing charging policy under a Time-of-Use tariff scheme, with the use of Deep Reinforcement Learning, and more specifically Deep Q-Networks (DQN). A novel end users flexibility potential reward is inferred from historical data analysis, where households with solar power generation have been used to train and test the designed algorithm. The suggested DQN EV charging policy can lead to more than 20% of savings in end users electricity bills.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09040", "tags": ["Neuromorphic Computing"]}, {"title": "Generative AI-driven Semantic Communication Framework for NextG Wireless Network", "authors": ["Avi Deb Raha", "Md. Shirajum Munir", "Apurba Adhikary", "Yu Qiao", "Choong Seon Hong"], "abstract": "This work designs a novel semantic communication (SemCom) framework for the next-generation wireless network to tackle the challenges of unnecessary transmission of vast amounts that cause high bandwidth consumption, more latency, and experience with bad quality of services (QoS). In particular, these challenges hinder applications like intelligent transportation systems (ITS), metaverse, mixed reality, and the Internet of Everything, where real-time and efficient data transmission is paramount. Therefore, to reduce communication overhead and maintain the QoS of emerging applications such as metaverse, ITS, and digital twin creation, this work proposes a novel semantic communication framework. First, an intelligent semantic transmitter is designed to capture the meaningful information (e.g., the rode-side image in ITS) by designing a domain-specific Mobile Segment Anything Model (MSAM)-based mechanism to reduce the potential communication traffic while QoS remains intact. Second, the concept of generative AI is introduced for building the SemCom to reconstruct and denoise the received semantic data frame at the receiver end. In particular, the Generative Adversarial Network (GAN) mechanism is designed to maintain a superior quality reconstruction under different signal-to-noise (SNR) channel conditions. Finally, we have tested and evaluated the proposed semantic communication (SemCom) framework with the real-world 6G scenario of ITS; in particular, the base station equipped with an RGB camera and a mmWave phased array. Experimental results demonstrate the efficacy of the proposed SemCom framework by achieving high-quality reconstruction across various SNR channel conditions, resulting in 93.45% data reduction in communication.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.09021", "tags": ["Generative Adversarial Networks"]}, {"title": "SeqXGPT: Sentence-Level AI-Generated Text Detection", "authors": ["Pengyu Wang", "Linyang Li", "Ke Ren", "Botian Jiang", "Dong Zhang", "Xipeng Qiu"], "abstract": "Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \\textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.08903", "tags": []}, {"title": "Welfare Diplomacy: Benchmarking Language Model Cooperation", "authors": ["Gabriel Mukobi", "Hannah Erlebach", "Niklas Lauffer", "Lewis Hammond", "Alan Chan", "Jesse Clifton"], "abstract": "The growing capabilities and increasingly widespread deployment of AI systems necessitate robust benchmarks for measuring their cooperative capabilities. Unfortunately, most multi-agent benchmarks are either zero-sum or purely cooperative, providing limited opportunities for such measurements. We introduce a general-sum variant of the zero-sum board game Diplomacy -- called Welfare Diplomacy -- in which players must balance investing in military conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules and implementing them via an open-source Diplomacy engine; (2) constructing baseline agents using zero-shot prompted language models; and (3) conducting experiments where we find that baselines using state-of-the-art models attain high social welfare but are exploitable. Our work aims to promote societal safety by aiding researchers in developing and assessing multi-agent AI systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is available at https://github.com/mukobi/welfare-diplomacy.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.08901", "tags": []}, {"title": "Exploration with Principles for Diverse AI Supervision", "authors": ["Hao Liu", "Matei Zaharia", "Pieter Abbeel"], "abstract": "Training large transformers using next-token prediction has given rise to groundbreaking advancements in AI. While this generative AI approach has produced impressive results, it heavily leans on human supervision. Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation. To address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. Drawing inspiration from unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content following exploration principles and a critic that evaluates the generated content, offering critiques to guide the actor. Empirical evaluations demonstrate that EAI significantly boosts model performance on complex reasoning tasks, addressing the limitations of human-intensive supervision.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.08899", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "Path To Gain Functional Transparency In Artificial Intelligence With Meaningful Explainability", "authors": ["Md. Tanzib Hosain", "Mehedi Hasan Anik", "Sadman Rafi", "Rana Tabassum", "Khaleque Insia", "Md. Mehrab Siddiky"], "abstract": "Artificial Intelligence (AI) is rapidly integrating into various aspects of our daily lives, influencing decision-making processes in areas such as targeted advertising and matchmaking algorithms. As AI systems become increasingly sophisticated, ensuring their transparency and explainability becomes crucial. Functional transparency is a fundamental aspect of algorithmic decision-making systems, allowing stakeholders to comprehend the inner workings of these systems and enabling them to evaluate their fairness and accuracy. However, achieving functional transparency poses significant challenges that need to be addressed. In this paper, we propose a design for user-centered compliant-by-design transparency in transparent systems. We emphasize that the development of transparent and explainable AI systems is a complex and multidisciplinary endeavor, necessitating collaboration among researchers from diverse fields such as computer science, artificial intelligence, ethics, law, and social science. By providing a comprehensive understanding of the challenges associated with transparency in AI systems and proposing a user-centered design framework, we aim to facilitate the development of AI systems that are accountable, trustworthy, and aligned with societal values.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.08849", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Healthcare"]}, {"title": "A Case-Based Persistent Memory for a Large Language Model", "authors": ["Ian Watson"], "abstract": "Case-based reasoning (CBR) as a methodology for problem-solving can use any appropriate computational technique. This position paper argues that CBR researchers have somewhat overlooked recent developments in deep learning and large language models (LLMs). The underlying technical developments that have enabled the recent breakthroughs in AI have strong synergies with CBR and could be used to provide a persistent memory for LLMs to make progress towards Artificial General Intelligence.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08842", "tags": []}, {"title": "Static Code Analysis in the AI Era: An In-depth Exploration of the Concept, Function, and Potential of Intelligent Code Analysis Agents", "authors": ["Gang Fan", "Xiaoheng Xie", "Xunjin Zheng", "Yinan Liang", "Peng Di"], "abstract": "The escalating complexity of software systems and accelerating development cycles pose a significant challenge in managing code errors and implementing business logic. Traditional techniques, while cornerstone for software quality assurance, exhibit limitations in handling intricate business logic and extensive codebases. To address these challenges, we introduce the Intelligent Code Analysis Agent (ICAA), a novel concept combining AI models, engineering process designs, and traditional non-AI components. The ICAA employs the capabilities of large language models (LLMs) such as GPT-3 or GPT-4 to automatically detect and diagnose code errors and business logic inconsistencies. In our exploration of this concept, we observed a substantial improvement in bug detection accuracy, reducing the false-positive rate to 66\\% from the baseline's 85\\%, and a promising recall rate of 60.8\\%. However, the token consumption cost associated with LLMs, particularly the average cost for analyzing each line of code, remains a significant consideration for widespread adoption. Despite this challenge, our findings suggest that the ICAA holds considerable potential to revolutionize software quality assurance, significantly enhancing the efficiency and accuracy of bug detection in the software development process. We hope this pioneering work will inspire further research and innovation in this field, focusing on refining the ICAA concept and exploring ways to mitigate the associated costs.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08837", "tags": []}, {"title": "Confounding-Robust Policy Improvement with Human-AI Teams", "authors": ["Ruijiang Gao", "Mingzhang Yin"], "abstract": "Human-AI collaboration has the potential to transform various domains by leveraging the complementary strengths of human experts and Artificial Intelligence (AI) systems. However, unobserved confounding can undermine the effectiveness of this collaboration, leading to biased and unreliable outcomes. In this paper, we propose a novel solution to address unobserved confounding in human-AI collaboration by employing the marginal sensitivity model (MSM). Our approach combines domain expertise with AI-driven statistical modeling to account for potential confounders that may otherwise remain hidden. We present a deferral collaboration framework for incorporating the MSM into policy learning from observational data, enabling the system to control for the influence of unobserved confounding factors. In addition, we propose a personalized deferral collaboration system to leverage the diverse expertise of different human decision-makers. By adjusting for potential biases, our proposed solution enhances the robustness and reliability of collaborative outcomes. The empirical and theoretical analyses demonstrate the efficacy of our approach in mitigating unobserved confounding and improving the overall performance of human-AI collaborations.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08824", "tags": ["AI and Privacy", "Ethical AI and Bias Mitigation", "AI in Healthcare"]}, {"title": "Spiking Semantic Communication for Feature Transmission with HARQ", "authors": ["Mengyang Wang", "Jiahui Li", "Mengyao Ma", "Xiaopeng Fan"], "abstract": "In Collaborative Intelligence (CI), the Artificial Intelligence (AI) model is divided between the edge and the cloud, with intermediate features being sent from the edge to the cloud for inference. Several deep learning-based Semantic Communication (SC) models have been proposed to reduce feature transmission overhead and mitigate channel noise interference. Previous research has demonstrated that Spiking Neural Network (SNN)-based SC models exhibit greater robustness on digital channels compared to Deep Neural Network (DNN)-based SC models. However, the existing SNN-based SC models require fixed time steps, resulting in fixed transmission bandwidths that cannot be adaptively adjusted based on channel conditions. To address this issue, this paper introduces a novel SC model called SNN-SC-HARQ, which combines the SNN-based SC model with the Hybrid Automatic Repeat Request (HARQ) mechanism. SNN-SC-HARQ comprises an SNN-based SC model that supports the transmission of features at varying bandwidths, along with a policy model that determines the appropriate bandwidth. Experimental results show that SNN-SC-HARQ can dynamically adjust the bandwidth according to the channel conditions without performance loss.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08804", "tags": ["Deep Learning"]}, {"title": "Advancing Perception in Artificial Intelligence through Principles of Cognitive Science", "authors": ["Palaash Agrawal", "Cheston Tan", "Heena Rathore"], "abstract": "Although artificial intelligence (AI) has achieved many feats at a rapid pace, there still exist open problems and fundamental shortcomings related to performance and resource efficiency. Since AI researchers benchmark a significant proportion of performance standards through human intelligence, cognitive sciences-inspired AI is a promising domain of research. Studying cognitive science can provide a fresh perspective to building fundamental blocks in AI research, which can lead to improved performance and efficiency. In this review paper, we focus on the cognitive functions of perception, which is the process of taking signals from one's surroundings as input, and processing them to understand the environment. Particularly, we study and compare its various processes through the lens of both cognitive sciences and AI. Through this study, we review all current major theories from various sub-disciplines of cognitive science (specifically neuroscience, psychology and linguistics), and draw parallels with theories and techniques from current practices in AI. We, hence, present a detailed collection of methods in AI for researchers to build AI systems inspired by cognitive science. Further, through the process of reviewing the state of cognitive-inspired AI, we point out many gaps in the current state of AI (with respect to the performance of the human brain), and hence present potential directions for researchers to develop better perception systems in AI.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08803", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving", "authors": ["Karen D. Wang", "Eric Burkholder", "Carl Wieman", "Shima Salehi", "Nick Haber"], "abstract": "The study explores the capabilities of OpenAI's ChatGPT in solving different types of physics problems. ChatGPT (with GPT-4) was queried to solve a total of 40 problems from a college-level engineering physics course. These problems ranged from well-specified problems, where all data required for solving the problem was provided, to under-specified, real-world problems where not all necessary data were given. Our findings show that ChatGPT could successfully solve 62.5\\% of the well-specified problems, but its accuracy drops to 8.3\\% for under-specified problems. Analysis of the model's incorrect solutions revealed three distinct failure modes: 1) failure to construct accurate models of the physical world, 2) failure to make reasonable assumptions about missing data, and 3) calculation errors. The study offers implications for how to leverage LLM-augmented instructional materials to enhance STEM education. The insights also contribute to the broader discourse on AI's strengths and limitations, serving both educators aiming to leverage the technology and researchers investigating human-AI collaboration frameworks for problem-solving and decision-making.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08773", "tags": []}, {"title": "Development and Validation of a Deep Learning-Based Microsatellite Instability Predictor from Prostate Cancer Whole-Slide Images", "authors": ["Qiyuan Hu", "Abbas A. Rizvi", "Geoffery Schau", "Kshitij Ingale", "Yoni Muller", "Rachel Baits", "Sebastian Pretzer", "A\u00efcha BenTaieb", "Abigail Gordhamer", "Roberto Nussenzveig", "Adam Cole", "Matthew O. Leavitt", "Rohan P. Joshi", "Nike Beaubier", "Martin C. Stumpe", "Kunal Nagpal"], "abstract": "Microsatellite instability-high (MSI-H) is a tumor agnostic biomarker for immune checkpoint inhibitor therapy. However, MSI status is not routinely tested in prostate cancer, in part due to low prevalence and assay cost. As such, prediction of MSI status from hematoxylin and eosin (H&E) stained whole-slide images (WSIs) could identify prostate cancer patients most likely to benefit from confirmatory testing and becoming eligible for immunotherapy. Prostate biopsies and surgical resections from de-identified records of consecutive prostate cancer patients referred to our institution were analyzed. Their MSI status was determined by next generation sequencing. Patients before a cutoff date were split into an algorithm development set (n=4015, MSI-H 1.8%) and a paired validation set (n=173, MSI-H 19.7%) that consisted of two serial sections from each sample, one stained and scanned internally and the other at an external site. Patients after the cutoff date formed the temporal validation set (n=1350, MSI-H 2.3%). Attention-based multiple instance learning models were trained to predict MSI-H from H&E WSIs. The MSI-H predictor achieved area under the receiver operating characteristic curve values of 0.78 (95% CI [0.69-0.86]), 0.72 (95% CI [0.63-0.81]), and 0.72 (95% CI [0.62-0.82]) on the internally prepared, externally prepared, and temporal validation sets, respectively. While MSI-H status is significantly correlated with Gleason score, the model remained predictive within each Gleason score subgroup. In summary, we developed and validated an AI-based MSI-H diagnostic model on a large real-world cohort of routine H&E slides, which effectively generalized to externally stained and scanned samples and a temporally independent validation cohort. This algorithm has the potential to direct prostate cancer patients toward immunotherapy and to identify MSI-H cases secondary to Lynch syndrome.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08743", "tags": []}, {"title": "Real-Time Event Detection with Random Forests and Temporal Convolutional Networks for More Sustainable Petroleum Industry", "authors": ["Yuanwei Qu", "Baifan Zhou", "Arild Waaler", "David Cameron"], "abstract": "The petroleum industry is crucial for modern society, but the production process is complex and risky. During the production, accidents or failures, resulting from undesired production events, can cause severe environmental and economic damage. Previous studies have investigated machine learning (ML) methods for undesired event detection. However, the prediction of event probability in real-time was insufficiently addressed, which is essential since it is important to undertake early intervention when an event is expected to happen. This paper proposes two ML approaches, random forests and temporal convolutional networks, to detect undesired events in real-time. Results show that our approaches can effectively classify event types and predict the probability of their appearance, addressing the challenges uncovered in previous studies and providing a more effective solution for failure event management during the production.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08737", "tags": []}, {"title": "How Does Artificial Intelligence Improve Human Decision-Making? Evidence from the AI-Powered Go Program", "authors": ["Sukwoong Choi", "Hyo Kang", "Namil Kim", "Junsik Kim"], "abstract": "We study how humans learn from AI, exploiting an introduction of an AI-powered Go program (APG) that unexpectedly outperformed the best professional player. We compare the move quality of professional players to that of APG's superior solutions around its public release. Our analysis of 749,190 moves demonstrates significant improvements in players' move quality, accompanied by decreased number and magnitude of errors. The effect is pronounced in the early stages of the game where uncertainty is highest. In addition, younger players and those in AI-exposed countries experience greater improvement, suggesting potential inequality in learning from AI. Further, while players of all levels learn, less skilled players derive higher marginal benefits. These findings have implications for managers seeking to adopt and utilize AI effectively within their organizations.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08704", "tags": []}, {"title": "The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features", "authors": ["Navita Goyal", "Connor Baumler", "Tin Nguyen", "Hal Daum\u00e9 III"], "abstract": "AI systems have been known to amplify biases in real world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments -- explanations, model bias disclosure and proxy correlation disclosure -- affect fairness perception and parity. We find that explanations help people detect direct biases but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and the decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08617", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback", "authors": ["Jingkang Yang", "Yuhao Dong", "Shuai Liu", "Bo Li", "Ziyue Wang", "Chencheng Jiang", "Haoran Tan", "Jiamu Kang", "Yuanhan Zhang", "Kaiyang Zhou", "Ziwei Liu"], "abstract": "Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08588", "tags": ["Reinforcement Learning"]}, {"title": "Jigsaw: Supporting Designers in Prototyping Multimodal Applications by Assembling AI Foundation Models", "authors": ["David Chuan-En Lin", "Nikolas Martelaro"], "abstract": "Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08574", "tags": ["Natural Language Processing"]}, {"title": "Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities", "authors": ["Subash Neupane", "Shaswata Mitra", "Ivan A. Fernandez", "Swayamjit Saha", "Sudip Mittal", "Jingdao Chen", "Nisha Pillai", "Shahram Rahimi"], "abstract": "Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakeholders with a holistic understanding of these areas to enhance the overall AI-Robotics system security. We begin by surveying potential attack surfaces and provide mitigating defensive strategies. We then delve into ethical issues, such as dependency and psychological impact, as well as the legal concerns regarding accountability for these systems. Besides, emerging trends such as HRI are discussed, considering privacy, integrity, safety, trustworthiness, and explainability concerns. Finally, we present our vision for future research directions in this dynamic and promising field.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08565", "tags": ["Cyber Security"]}, {"title": "MemGPT: Towards LLMs as Operating Systems", "authors": ["Charles Packer", "Vivian Fang", "Shishir G. Patil", "Kevin Lin", "Sarah Wooders", "Joseph E. Gonzalez"], "abstract": "Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08560", "tags": []}, {"title": "XAI Benchmark for Visual Explanation", "authors": ["Yifei Zhang", "Siyi Gu", "James Song", "Bo Pan", "Liang Zhao"], "abstract": "The rise of deep learning algorithms has led to significant advancements in computer vision tasks, but their \"black box\" nature has raised concerns regarding interpretability. Explainable AI (XAI) has emerged as a critical area of research aiming to open this \"black box\", and shed light on the decision-making process of AI models. Visual explanations, as a subset of Explainable Artificial Intelligence (XAI), provide intuitive insights into the decision-making processes of AI models handling visual data by highlighting influential areas in an input image. Despite extensive research conducted on visual explanations, most evaluations are model-centered since the availability of corresponding real-world datasets with ground truth explanations is scarce in the context of image data. To bridge this gap, we introduce an XAI Benchmark comprising a dataset collection from diverse topics that provide both class labels and corresponding explanation annotations for images. We have processed data from diverse domains to align with our unified visual explanation framework. We introduce a comprehensive Visual Explanation pipeline, which integrates data loading, preprocessing, experimental setup, and model evaluation processes. This structure enables researchers to conduct fair comparisons of various visual explanation techniques. In addition, we provide a comprehensive review of over 10 evaluation methods for visual explanation to assist researchers in effectively utilizing our dataset collection. To further assess the performance of existing visual explanation methods, we conduct experiments on selected datasets using various model-centered and ground truth-centered evaluation metrics. We envision this benchmark could facilitate the advancement of visual explanation models. The XAI dataset collection and easy-to-use code for evaluation are publicly accessible at https://xaidataset.github.io.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08537", "tags": ["Explainable AI (XAI):"]}, {"title": "Towards Robust Multi-Modal Reasoning via Model Selection", "authors": ["Xiangyan Liu", "Rongxue Li", "Wei Ji", "Tao Lin"], "abstract": "The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the \"brain\" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.\n  To this end, we identify the key challenges therein and propose the $\\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08446", "tags": ["Natural Language Processing"]}, {"title": "TensorMD: Scalable Tensor-Diagram based Machine Learning Interatomic Potential on Heterogeneous Many-Core Processors", "authors": ["Xin Chen", "Yucheng Ouyang", "Xin Chen", "Zhenchuan Chen", "Rongfen Lin", "Xingyu Gao", "Lifang Wang", "Fang Li", "Yin Liu", "Honghui Shang", "Haifeng Song"], "abstract": "Molecular dynamics simulations have emerged as a potent tool for investigating the physical properties and kinetic behaviors of materials at the atomic scale, particularly in extreme conditions. Ab initio accuracy is now achievable with machine learning based interatomic potentials. With recent advancements in high-performance computing, highly accurate and large-scale simulations become feasible. This study introduces TensorMD, a new machine learning interatomic potential (MLIP) model that integrates physical principles and tensor diagrams. The tensor formalism provides a more efficient computation and greater flexibility for use with other scientific codes. Additionally, we proposed several portable optimization strategies and developed a highly optimized version for the new Sunway supercomputer. Our optimized TensorMD can achieve unprecedented performance on the new Sunway, enabling simulations of up to 52 billion atoms with a time-to-solution of 31 ps/step/atom, setting new records for HPC + AI + MD.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08439", "tags": []}, {"title": "Cold Start Latency in Serverless Computing: A Systematic Review, Taxonomy, and Future Directions", "authors": ["Muhammed Golec", "Guneet Kaur Walia", "Mohit Kumar", "Felix Cuadrado", "Sukhpal Singh Gill", "Steve Uhlig"], "abstract": "Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model. In serverless computing, users pay only for the time they actually spend using the resources. Although zero scaling optimises cost and resource utilisation, it is the fundamental reason for the serverless cold start problem. Various academic and corporate sector studies are being conducted to tackle the cold start problem, which has large research challenges. To study the \"cold start\" problem in serverless computing, this article provides a comprehensive literature overview of recent research. In addition, we present a detailed taxonomy of several approaches to addressing the issue of cold start latency in serverless computing. Several academic and industrial organisations have proposed methods for cutting down the cold start time and cold start frequency, and this taxonomy is being used to explore these methods. There are several categories in which a current study on cold start latency is organised: caching and application-level optimization-based solutions, as well as AI/ML-based solutions. We have analysed the current methods and grouped them into categories based on their commonalities and features. Finally, we conclude with a review of current challenges and possible future research directions.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08437", "tags": []}, {"title": "Assessing of Soil Erosion Risk Through Geoinformation Sciences and Remote Sensing -- A Review", "authors": ["Lachezar Filchev", "Vasil Kolev"], "abstract": "During past decades a marked manifestation of widespread erosion phenomena was studied worldwide. Global conservation community has launched campaigns at local, regional and continental level in developing countries for preservation of soil resources in order not only to stop or mitigate human impact on nature but also to improve life in rural areas introducing new approaches for soil cultivation. After the adoption of Sustainable Development Goals of UNs and launching several world initiatives such as the Land Degradation Neutrality (LDN) the world came to realize the very importance of the soil resources on which the biosphere relies for its existence. The main goal of the chapter is to review different types and structures erosion models as well as their applications. Several methods using spatial analysis capabilities of geographic information systems (GIS) are in operation for soil erosion risk assessment, such as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss Equation (RUSLE) in operation worldwide and in the USA and MESALES model. These and more models are being discussed in the present work alongside more experimental models and methods for assessing soil erosion risk such as Artificial Intelligence (AI), Machine and Deep Learning, etc. At the end of this work, a prospectus for the future development of soil erosion risk assessment is drawn.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08430", "tags": []}, {"title": "Performance/power assessment of CNN packages on embedded automotive platforms", "authors": ["Paolo Burgio", "Gianluca Brilli"], "abstract": "The rise of power-efficient embedded computers based on highly-parallel accelerators opens a number of opportunities and challenges for researchers and engineers, and paved the way to the era of edge computing. At the same time, advances in embedded AI for object detection and categorization such as YOLO, GoogleNet and AlexNet reached an unprecedented level of accuracy (mean-Average Precision - mAP) and performance (Frames-Per-Second - FPS). Today, edge computers based on heterogeneous many-core systems are a predominant choice to deploy such systems in industry 4.0, wearable devices, and - our focus - autonomous driving systems. In these latter systems, engineers struggle to make reduced automotive power and size budgets co-exist with the accuracy and performance targets requested by autonomous driving. We aim at validating the effectiveness and efficiency of most recent networks on state-of-the-art platforms with embedded commercial-off-the-shelf System-on-Chips, such as Xavier AGX, Tegra X2 and Nano for NVIDIA and XCZU9EG and XCZU3EG of the Zynq UltraScale+ family, for the Xilinx counterpart. Our work aims at supporting engineers in choosing the most appropriate CNN package and computing system for their designs, and deriving guidelines for adequately sizing their systems.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08401", "tags": ["AI in Autonomous Vehicles"]}, {"title": "From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer", "authors": ["Md. Rezaul Karim", "Lina Molinas Comet", "Md Shajalal", "Oya Beyan", "Dietrich Rebholz-Schuhmann", "Stefan Decker"], "abstract": "Domain experts often rely on up-to-date knowledge for apprehending and disseminating specific biological processes that help them design strategies to develop prevention and therapeutic decision-making. A challenging scenario for artificial intelligence (AI) is using biomedical data (e.g., texts, imaging, omics, and clinical) to provide diagnosis and treatment recommendations for cancerous conditions. Data and knowledge about cancer, drugs, genes, proteins, and their mechanism is spread across structured (knowledge bases (KBs)) and unstructured (e.g., scientific articles) sources. A large-scale knowledge graph (KG) can be constructed by integrating these data, followed by extracting facts about semantically interrelated entities and relations. Such KGs not only allow exploration and question answering (QA) but also allow domain experts to deduce new knowledge. However, exploring and querying large-scale KGs is tedious for non-domain users due to a lack of understanding of the underlying data assets and semantic technologies. In this paper, we develop a domain KG to leverage cancer-specific biomarker discovery and interactive QA. For this, a domain ontology called OncoNet Ontology (ONO) is developed to enable semantic reasoning for validating gene-disease relations. The KG is then enriched by harmonizing the ONO, controlled vocabularies, and additional biomedical concepts from scientific articles by employing BioBERT- and SciBERT-based information extraction (IE) methods. Further, since the biomedical domain is evolving, where new findings often replace old ones, without employing up-to-date findings, there is a high chance an AI system exhibits concept drift while providing diagnosis and treatment. Therefore, we finetuned the KG using large language models (LLMs) based on more recent articles and KBs that might not have been seen by the named entity recognition models.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08365", "tags": []}, {"title": "A cry for help: Early detection of brain injury in newborns", "authors": ["Charles C. Onu", "Samantha Latremouille", "Arsenii Gorin", "Junhao Wang", "Uchenna Ekwochi", "Peter O. Ubuane", "Omolara A. Kehinde", "Muhammad A. Salisu", "Datonye Briggs", "Yoshua Bengio", "Doina Precup"], "abstract": "Since the 1960s, neonatal clinicians have known that newborns suffering from certain neurological conditions exhibit altered crying patterns such as the high-pitched cry in birth asphyxia. Despite an annual burden of over 1.5 million infant deaths and disabilities, early detection of neonatal brain injuries due to asphyxia remains a challenge, particularly in developing countries where the majority of births are not attended by a trained physician. Here we report on the first inter-continental clinical study to demonstrate that neonatal brain injury can be reliably determined from recorded infant cries using an AI algorithm we call Roseline. Previous and recent work has been limited by the lack of a large, high-quality clinical database of cry recordings, constraining the application of state-of-the-art machine learning. We develop a new training methodology for audio-based pathology detection models and evaluate this system on a large database of newborn cry sounds acquired from geographically diverse settings -- 5 hospitals across 3 continents. Our system extracts interpretable acoustic biomarkers that support clinical decisions and is able to accurately detect neurological injury from newborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity). Cry-based neurological monitoring opens the door for low-cost, easy-to-use, non-invasive and contact-free screening of at-risk babies, especially when integrated into simple devices like smartphones or neonatal ICU monitors. This would provide a reliable tool where there are no alternatives, but also curtail the need to regularly exert newborns to physically-exhausting or radiation-exposing assessments such as brain CT scans. This work sets the stage for embracing the infant cry as a vital sign and indicates the potential of AI-driven sound monitoring for the future of affordable healthcare.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.08338", "tags": []}, {"title": "Defending Our Privacy With Backdoors", "authors": ["Dominik Hintersdorf", "Lukas Struppek", "Daniel Neider", "Kristian Kersting"], "abstract": "The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-\"a person\" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new \"dual-use\" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08320", "tags": ["AI and Privacy"]}, {"title": "If our aim is to build morality into an artificial agent, how might we begin to go about doing so?", "authors": ["Reneira Seeamber", "Cosmin Badea"], "abstract": "As Artificial Intelligence (AI) becomes pervasive in most fields, from healthcare to autonomous driving, it is essential that we find successful ways of building morality into our machines, especially for decision-making. However, the question of what it means to be moral is still debated, particularly in the context of AI. In this paper, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI Ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08295", "tags": ["Ethical AI and Bias Mitigation", "AI in Healthcare", "AI and Privacy"]}, {"title": "Concealed Electronic Countermeasures of Radar Signal with Adversarial Examples", "authors": ["Ruinan Ma", "Canjie Zhu", "Mingfeng Lu", "Yunjie Li", "Yu-an Tan", "Ruibin Zhang", "Ran Tao"], "abstract": "Electronic countermeasures involving radar signals are an important aspect of modern warfare. Traditional electronic countermeasures techniques typically add large-scale interference signals to ensure interference effects, which can lead to attacks being too obvious. In recent years, AI-based attack methods have emerged that can effectively solve this problem, but the attack scenarios are currently limited to time domain radar signal classification. In this paper, we focus on the time-frequency images classification scenario of radar signals. We first propose an attack pipeline under the time-frequency images scenario and DITIMI-FGSM attack algorithm with high transferability. Then, we propose STFT-based time domain signal attack(STDS) algorithm to solve the problem of non-invertibility in time-frequency analysis, thus obtaining the time-domain representation of the interference signal. A large number of experiments show that our attack pipeline is feasible and the proposed attack method has a high success rate.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08292", "tags": ["Cyber Security"]}, {"title": "From QFT to Boltzmann: Freeze-in in the presence of oscillating condensates", "authors": ["Wen-Yuan Ai", "Ankit Beniwal", "Angelo Maggi", "David J. E. Marsh"], "abstract": "Scalar dark matter (DM), and axions in particular, have an irreducible abundance of particles produced by freeze-in due to portal interactions with the Standard Model plasma in the early Universe. In addition, vacuum misalignment and other mechanisms can lead to the presence of a cold, oscillating condensate. Therefore, generically, the evolution of the DM in both forms, condensate and particles, needs to be studied simultaneously. In non-equilibrium quantum field theory, the condensate and particles are described by one- and two-point functions, respectively. The fundamental coupled equations of motion (EoMs) of these objects are non-local. To simplify the EoMs and bring them into a familiar form for relic abundance calculations, we perform a Markovianization process for a quasi-harmonically oscillating homogeneous condensate, leading to local EoMs for the particle distribution function and the envelope function of condensate oscillation. This reduces the dynamics to a pair of coupled Boltzmann equations, and we derive explicitly the form of the collision operators for all particle and condensate interactions.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08272", "tags": []}, {"title": "Who Said That? Benchmarking Social Media AI Detection", "authors": ["Wanyun Cui", "Linqiu Zhang", "Qianle Wang", "Shuyang Cai"], "abstract": "AI-generated text has proliferated across various online platforms, offering both transformative prospects and posing significant risks related to misinformation and manipulation. Addressing these challenges, this paper introduces SAID (Social media AI Detection), a novel benchmark developed to assess AI-text detection models' capabilities in real social media platforms. It incorporates real AI-generate text from popular social media platforms like Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that reflects the sophisticated strategies employed by real AI users on the Internet which may evade detection or gain visibility, providing a more realistic and challenging evaluation landscape. A notable finding of our study, based on the Zhihu dataset, reveals that annotators can distinguish between AI-generated and human-generated texts with an average accuracy rate of 96.5%. This finding necessitates a re-evaluation of human capability in recognizing AI-generated text in today's widely AI-influenced environment. Furthermore, we present a new user-oriented AI-text detection challenge focusing on the practicality and effectiveness of identifying AI-generated text based on user information and multiple responses. The experimental results demonstrate that conducting detection tasks on actual social media platforms proves to be more challenging compared to traditional simulated AI-text detection, resulting in a decreased accuracy. On the other hand, user-oriented AI-generated text detection significantly improve the accuracy of detection.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08240", "tags": []}, {"title": "The Impact of Time Step Frequency on the Realism of Robotic Manipulation Simulation for Objects of Different Scales", "authors": ["Minh Q. Ta", "Holly Dinkel", "Hameed Abdul-Rashid", "Yangfei Dai", "Jessica Myers", "Tan Chen", "Junyi Geng", "Timothy Bretl"], "abstract": "This work evaluates the impact of time step frequency and component scale on robotic manipulation simulation accuracy. Increasing the time step frequency for small-scale objects is shown to improve simulation accuracy. This simulation, demonstrating pre-assembly part picking for two object geometries, serves as a starting point for discussing how to improve Sim2Real transfer in robotic assembly processes.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08233", "tags": []}, {"title": "(Re)conceptualizations: Intentional concept development in the social sciences", "authors": ["Piotr Tomasz Makowski"], "abstract": "Can intentional concept development in the social sciences be understood in terms of conceptual engineering (CE)? To answer this question, I analyze various types of conceptual changes in the social changes-with a special attention to organizational research and the so-called (re)conceptualizations-and distinguish between CE as a theoretical practice and CE as a research program. I show that social scientists, from the point of view of their scientific practice, exercise CE in two versions: CE de novo is employed as new conceptualizations and moderately progressive CE-as reconceptualizations. Importantly, the second type of CE-rather neglected in philosophy of the social sciences-appears to be highly important for the incremental progress of inquiry. Still, both types appear to be equally significant also for CE understood as a research program and for its prospects in the social sciences. Here, I point to three possible paths that help bridging the gap between actual practices of concept development in the social sciences and normative, programmatic approaches to CE: best practice recommendations, institutional actions and uses of AI-agents.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08207", "tags": []}, {"title": "Terrestrial Very-Long-Baseline Atom Interferometry: Workshop Summary", "authors": ["Sven Abend", "Baptiste Allard", "Iv\u00e1n Alonso", "John Antoniadis", "Henrique Araujo", "Gianluigi Arduini", "Aidan Arnold", "Tobias A\u00dfmann", "Nadja Augst", "Leonardo Badurina", "Antun Balaz", "Hannah Banks", "Michele Barone", "Michele Barsanti", "Angelo Bassi", "Baptiste Battelier", "Charles Baynham", "Beaufils Quentin", "Aleksandar Belic", "Ankit Beniwal", "Jose Bernabeu", "Francesco Bertinelli", "Andrea Bertoldi", "Ikbal Ahamed Biswas", "Diego Blas", "et al. (228 additional authors not shown)"], "abstract": "This document presents a summary of the 2023 Terrestrial Very-Long-Baseline Atom Interferometry Workshop hosted by CERN. The workshop brought together experts from around the world to discuss the exciting developments in large-scale atom interferometer (AI) prototypes and their potential for detecting ultralight dark matter and gravitational waves. The primary objective of the workshop was to lay the groundwork for an international TVLBAI proto-collaboration. This collaboration aims to unite researchers from different institutions to strategize and secure funding for terrestrial large-scale AI projects. The ultimate goal is to create a roadmap detailing the design and technology choices for one or more km-scale detectors, which will be operational in the mid-2030s. The key sections of this report present the physics case and technical challenges, together with a comprehensive overview of the discussions at the workshop together with the main conclusions.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08183", "tags": []}, {"title": "XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation", "authors": ["Qiang Li", "Dan Zhang", "Shengzhao Lei", "Xun Zhao", "Shuyan Li", "Porawit Kamnoedboon", "WeiWei Li"], "abstract": "The lack of standardized robustness metrics and the widespread reliance on numerous unrelated benchmark datasets for testing have created a gap between academically validated robust models and their often problematic practical adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc., we further propose a novel robustness criterion that extends beyond model generation ability assessment. This benchmark dataset, along with related code, is available at https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners can leverage this resource to evaluate the robustness of their visual models under challenging conditions and ultimately benefit from the demands of practical computer vision systems.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08182", "tags": ["Computer Vision", "Generative Adversarial Networks", "Natural Language Processing"]}, {"title": "The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms", "authors": ["Xiaotie Deng", "Dongchen Li", "Hanyu Li"], "abstract": "AI in Math deals with mathematics in a constructive manner so that reasoning becomes automated, less laborious, and less error-prone. For algorithms, the question becomes how to automate analyses for specific problems. For the first time, this work provides an automatic method for approximation analysis on a well-studied problem in theoretical computer science: computing approximate Nash equilibria in two-player games. We observe that such algorithms can be reformulated into a search-and-mix paradigm, which involves a search phase followed by a mixing phase. By doing so, we are able to fully automate the procedure of designing and analyzing the mixing phase. For example, we illustrate how to perform our method with a program to analyze the approximation bounds of all the algorithms in the literature. Same approximation bounds are computed without any hand-written proof. Our automatic method heavily relies on the LP-relaxation structure in approximate Nash equilibria. Since many approximation algorithms and online algorithms adopt the LP relaxation, our approach may be extended to automate the analysis of other algorithms.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08066", "tags": []}, {"title": "Understanding and Controlling a Maze-Solving Policy Network", "authors": ["Ulisse Mini", "Peli Grietzer", "Mrinank Sharma", "Austin Meek", "Monte MacDiarmid", "Alexander Matt Turner"], "abstract": "To understand the goals and goal representations of AI systems, we carefully study a pretrained reinforcement learning policy that solves mazes by navigating to a range of target squares. We find this network pursues multiple context-dependent goals, and we further identify circuits within the network that correspond to one of these goals. In particular, we identified eleven channels that track the location of the goal. By modifying these channels, either with hand-designed interventions or by combining forward passes, we can partially control the policy. We show that this network contains redundant, distributed, and retargetable goal representations, shedding light on the nature of goal-direction in trained policy networks.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08043", "tags": []}, {"title": "Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles", "authors": ["Can Cui", "Yunsheng Ma", "Xu Cao", "Wenqian Ye", "Ziran Wang"], "abstract": "The fusion of human-centric design and artificial intelligence (AI) capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This paper proposes a novel framework that leverages Large Language Models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs' linguistic and contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs' interpretation, interaction, and reasoning in various scenarios. We also examine real-time personalization, demonstrating how LLMs can influence driving behaviors based on verbal commands. Our empirical results highlight the substantial advantages of utilizing chain-of-thought prompting, leading to improved driving decisions, and showing the potential for LLMs to enhance personalized driving experiences through ongoing verbal feedback. The proposed framework aims to transform autonomous vehicle operations, offering personalized support, transparent decision-making, and continuous learning to enhance safety and effectiveness. We achieve user-centric, transparent, and adaptive autonomous driving ecosystems supported by the integration of LLMs into autonomous vehicles.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.08034", "tags": ["AI in Autonomous Vehicles"]}, {"title": "A Novel Statistical Measure for Out-of-Distribution Detection in Data Quality Assurance", "authors": ["Tinghui Ouyang", "Isao Echizen", "Yoshiki Seo"], "abstract": "Data outside the problem domain poses significant threats to the security of AI-based intelligent systems. Aiming to investigate the data domain and out-of-distribution (OOD) data in AI quality management (AIQM) study, this paper proposes to use deep learning techniques for feature representation and develop a novel statistical measure for OOD detection. First, to extract low-dimensional representative features distinguishing normal and OOD data, the proposed research combines the deep auto-encoder (AE) architecture and neuron activation status for feature engineering. Then, using local conditional probability (LCP) in data reconstruction, a novel and superior statistical measure is developed to calculate the score of OOD detection. Experiments and evaluations are conducted on image benchmark datasets and an industrial dataset. Through comparative analysis with other common statistical measures in OOD detection, the proposed research is validated as feasible and effective in OOD and AIQM studies.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07998", "tags": ["AI and Privacy"]}, {"title": "Large Language Models for Scientific Synthesis, Inference and Explanation", "authors": ["Yizhen Zheng", "Huan Yee Koh", "Jiaxin Ju", "Anh T. N. Nguyen", "Lauren T. May", "Geoffrey I. Webb", "Shirui Pan"], "abstract": "Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of \"knowledge\", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this \"knowledge\" by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07984", "tags": ["Natural Language Processing", "Large scale Machine Learning"]}, {"title": "Tag Your Fish in the Broken Net: A Responsible Web Framework for Protecting Online Privacy and Copyright", "authors": ["Dawen Zhang", "Boming Xia", "Yue Liu", "Xiwei Xu", "Thong Hoang", "Zhenchang Xing", "Mark Staples", "Qinghua Lu", "Liming Zhu"], "abstract": "The World Wide Web, a ubiquitous source of information, serves as a primary resource for countless individuals, amassing a vast amount of data from global internet users. However, this online data, when scraped, indexed, and utilized for activities like web crawling, search engine indexing, and, notably, AI model training, often diverges from the original intent of its contributors. The ascent of Generative AI has accentuated concerns surrounding data privacy and copyright infringement. Regrettably, the web's current framework falls short in facilitating pivotal actions like consent withdrawal or data copyright claims. While some companies offer voluntary measures, such as crawler access restrictions, these often remain inaccessible to individual users. To empower online users to exercise their rights and enable companies to adhere to regulations, this paper introduces a user-controlled consent tagging framework for online data. It leverages the extensibility of HTTP and HTML in conjunction with the decentralized nature of distributed ledger technology. With this framework, users have the ability to tag their online data at the time of transmission, and subsequently, they can track and request the withdrawal of consent for their data from the data holders. A proof-of-concept system is implemented, demonstrating the feasibility of the framework. This work holds significant potential for contributing to the reinforcement of user consent, privacy, and copyright on the modern internet and lays the groundwork for future insights into creating a more responsible and user-centric web ecosystem.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07915", "tags": ["AI and Privacy"]}, {"title": "The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research", "authors": ["Thomas Decker", "Ralf Gross", "Alexander Koebler", "Michael Lebacher", "Ronald Schnitzer", "Stefan H. Weber"], "abstract": "In this paper, we investigate the practical relevance of explainable artificial intelligence (XAI) with a special focus on the producing industries and relate them to the current state of academic XAI research. Our findings are based on an extensive series of interviews regarding the role and applicability of XAI along the Machine Learning (ML) lifecycle in current industrial practice and its expected relevance in the future. The interviews were conducted among a great variety of roles and key stakeholders from different industry sectors. On top of that, we outline the state of XAI research by providing a concise review of the relevant literature. This enables us to provide an encompassing overview covering the opinions of the surveyed persons as well as the current state of academic research. By comparing our interview results with the current research approaches we reveal several discrepancies. While a multitude of different XAI approaches exists, most of them are centered around the model evaluation phase and data scientists. Their versatile capabilities for other stages are currently either not sufficiently explored or not popular among practitioners. In line with existing work, our findings also confirm that more efforts are needed to enable also non-expert users' interpretation and understanding of opaque AI models with existing methods and frameworks.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07882", "tags": ["Explainable AI (XAI):"]}, {"title": "Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks", "authors": ["Hao-Ping Lee", "Yu-Ju Yang", "Thomas Serban von Davier", "Jodi Forlizzi", "Sauvik Das"], "abstract": "Privacy is a key principle for developing ethical AI technologies, but how does including AI technologies in products and services change privacy risks? We constructed a taxonomy of AI privacy risks by analyzing 321 documented AI privacy incidents. We codified how the unique capabilities and requirements of AI technologies described in those incidents generated new privacy risks, exacerbated known ones, or otherwise did not meaningfully alter the risk. We present 12 high-level privacy risks that AI technologies either newly created (e.g., exposure risks from deepfake pornography) or exacerbated (e.g., surveillance risks from collecting training data). One upshot of our work is that incorporating AI technologies into a product can alter the privacy risks it entails. Yet, current privacy-preserving AI/ML methods (e.g., federated learning, differential privacy) only address a subset of the privacy risks arising from the capabilities and data requirements of AI.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07879", "tags": ["AI and Privacy", "Ethical AI and Bias Mitigation"]}, {"title": "TabLib: A Dataset of 627M Tables with Context", "authors": ["Gus Eggert", "Kevin Huo", "Mike Biven", "Justin Waugh"], "abstract": "It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present \"TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07875", "tags": []}, {"title": "QArchSearch: A Scalable Quantum Architecture Search Package", "authors": ["Ankit Kulshrestha", "Danylo Lykov", "Ilya Safro", "Yuri Alexeev"], "abstract": "The current era of quantum computing has yielded several algorithms that promise high computational efficiency. While the algorithms are sound in theory and can provide potentially exponential speedup, there is little guidance on how to design proper quantum circuits to realize the appropriate unitary transformation to be applied to the input quantum state. In this paper, we present \\texttt{QArchSearch}, an AI based quantum architecture search package with the \\texttt{QTensor} library as a backend that provides a principled and automated approach to finding the best model given a task and input quantum state. We show that the search package is able to efficiently scale the search to large quantum circuits and enables the exploration of more complex models for different quantum applications. \\texttt{QArchSearch} runs at scale and high efficiency on high-performance computing systems using a two-level parallelization scheme on both CPUs and GPUs, which has been demonstrated on the Polaris supercomputer.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07858", "tags": []}, {"title": "An Information Bottleneck Characterization of the Understanding-Workload Tradeoff", "authors": ["Lindsay Sanneman", "Mycal Tucker", "Julie Shah"], "abstract": "Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject experiments. This empirical link between human factors and information-theoretic concepts provides an important mathematical characterization of the workload-understanding tradeoff which enables user-tailored XAI design.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07802", "tags": ["Explainable AI (XAI):"]}, {"title": "Explainable Attention for Few-shot Learning and Beyond", "authors": ["Bahareh Nikpour", "Narges Armanfard"], "abstract": "Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically tailored for few-shot learning scenarios, called FewXAT. Our approach employs deep reinforcement learning to implement the concept of hard attention, directly impacting raw input data and thus rendering the process interpretable for human understanding. Through extensive experimentation across various benchmark datasets, we demonstrate the efficacy of our proposed method.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07800", "tags": ["Computer Vision", "AI and Privacy"]}, {"title": "AI Algorithm for the Generation of Three-Dimensional Accessibility Ramps in Grasshopper / Rhinoceros 7", "authors": ["Antonio Li", "Leila Yi", "Brandon Yeo Pei Hui"], "abstract": "Often overlooked as a component of urban development, accessibility infrastructure is undeniably crucial in daily life. Accessibility ramps are one of the most common types of accessibility infrastructure, and serve to benefit not only people with mobile impairments but also able-bodied third parties. While the necessity of accessibility ramps is acknowledged, actual implementation fails in light of the limits of manpower required for the design stage. In response, we present an algorithm capable of the automatic generation of a feasible accessibility ramp based on a 3D model of the relevant environment. Through the manual specification of initial and terminal points within a 3D model, the algorithm uses AI search algorithms to determine the optimal pathway connecting these points. Essential components in devising a wheelchair-accessible ramp are encoded within the process, as evaluated by the algorithm, including but not limited to elevation differentials, spatial constraints, and gradient specifications. From this, the algorithm then generates the pathway to be expanded into a full-scale, usable model of a ramp, which then can be easily exported and transformed through inter-software exchanges. Though some human input is still required following the generation stage, the minimising of human resources provides significant boosts of efficiency in the design process thus lowering the threshold for the incorporation of accessibility features in future urban design.", "submitted": "2023-09-29", "link": "https://arxiv.org/pdf/2310.07728", "tags": []}, {"title": "SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation", "authors": ["Bo Pan", "Zhenke Liu", "Yifei Zhang", "Liang Zhao"], "abstract": "Explainable AI seeks to bring light to the decision-making processes of black-box models. Traditional saliency-based methods, while highlighting influential data segments, often lack semantic understanding. Recent advancements, such as Concept Activation Vectors (CAVs) and Concept Bottleneck Models (CBMs), offer concept-based explanations but necessitate human-defined concepts. However, human-annotated concepts are expensive to attain. This paper introduces the Concept Bottleneck Surrogate Models (SurroCBM), a novel framework that aims to explain the black-box models with automatically discovered concepts. SurroCBM identifies shared and unique concepts across various black-box models and employs an explainable surrogate model for post-hoc explanations. An effective training strategy using self-generated data is proposed to enhance explanation quality continuously. Through extensive experiments, we demonstrate the efficacy of SurroCBM in concept discovery and explanation, underscoring its potential in advancing the field of explainable AI.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07698", "tags": ["Explainable AI (XAI):"]}, {"title": "Discovery of Novel Reticular Materials for Carbon Dioxide Capture using GFlowNets", "authors": ["Flaviu Cipcigan", "Jonathan Booth", "Rodrigo Neumann Barros Ferreira", "Carine Ribeiro dos Santos", "Mathias Steiner"], "abstract": "Artificial intelligence holds promise to improve materials discovery. GFlowNets are an emerging deep learning algorithm with many applications in AI-assisted discovery. By using GFlowNets, we generate porous reticular materials, such as metal organic frameworks and covalent organic frameworks, for applications in carbon dioxide capture. We introduce a new Python package (matgfn) to train and sample GFlowNets. We use matgfn to generate the matgfn-rm dataset of novel and diverse reticular materials with gravimetric surface area above 5000 m$^2$/g. We calculate single- and two-component gas adsorption isotherms for the top-100 candidates in matgfn-rm. These candidates are novel compared to the state-of-art ARC-MOF dataset and rank in the 90th percentile in terms of working capacity compared to the CoRE2019 dataset. We discover 15 materials outperforming all materials in CoRE2019.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.07671", "tags": []}, {"title": "Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models", "authors": ["Luiza Pozzobon", "Beyza Ermis", "Patrick Lewis", "Sara Hooker"], "abstract": "Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language's evolving nature over time. In this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. We introduce Goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43% relative latency reduction during inference and being more computationally efficient. By incorporating a retrieval-based approach at decoding time, Goodtriever enables toxicity-controlled text generation. Our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild. Code and data are available at https://github.com/for-ai/goodtriever.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07589", "tags": ["Large scale Machine Learning", "Natural Language Processing"]}, {"title": "Qlarify: Bridging Scholarly Abstracts and Papers with Recursively Expandable Summaries", "authors": ["Raymond Fok", "Joseph Chee Chang", "Tal August", "Amy X. Zhang", "Daniel S. Weld"], "abstract": "As scientific literature has grown exponentially, researchers often rely on paper triaging strategies such as browsing abstracts before deciding to delve into a paper's full text. However, when an abstract is insufficient, researchers are required to navigate an informational chasm between 150-word abstracts and 10,000-word papers. To bridge that gap, we introduce the idea of recursively expandable summaries and present Qlarify, an interactive system that allows users to recursively expand an abstract by progressively incorporating additional information from a paper's full text. Starting from an abstract, users can brush over summary text to specify targeted information needs or select AI-suggested entities in the text. Responses are then generated on-demand by an LLM and appear in the form of a fluid, threaded expansion of the existing text. Each generated summary can be efficiently verified through attribution to a relevant source-passage in the paper. Through an interview study (n=9) and a field deployment (n=275) at a research conference, we use Qlarify as a technology probe to elaborate upon the expandable summaries design space, highlight how scholars benefit from Qlarify's expandable abstracts, and identify future opportunities to support low-effort and just-in-time exploration of scientific documents $\\unicode{x2013}$ and other information spaces $\\unicode{x2013}$ through LLM-powered interactions.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07581", "tags": []}, {"title": "ChatGPT for Computational Topology", "authors": ["Jian Liu", "Li Shen", "Guo-Wei Wei"], "abstract": "ChatGPT represents a significant milestone in the field of artificial intelligence (AI), finding widespread applications across diverse domains. However, its effectiveness in mathematical contexts has been somewhat constrained by its susceptibility to conceptual errors. Concurrently, topological data analysis (TDA), a relatively new discipline, has garnered substantial interest in recent years. Nonetheless, the advancement of TDA is impeded by the limited understanding of computational algorithms and coding proficiency among theoreticians. This work endeavors to bridge the gap between theoretical topological concepts and their practical implementation in computational topology through the utilization of ChatGPT. We showcase how a pure theoretician, devoid of computational experience and coding skills, can effectively transform mathematical formulations and concepts into functional code for computational topology with the assistance of ChatGPT. Our strategy outlines a productive process wherein a mathematician trains ChatGPT on pure mathematical concepts, steers ChatGPT towards generating computational topology code, and subsequently validates the generated code using established examples. Our specific case studies encompass the computation of Betti numbers, Laplacian matrices, and Dirac matrices for simplicial complexes, as well as the persistence of various homologies and Laplacians. Furthermore, we explore the application of ChatGPT in computing recently developed topological theories for hypergraphs and digraphs. This work serves as an initial step towards effectively transforming pure mathematical theories into practical computational tools, with the ultimate goal of enabling real applications across diverse fields.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07570", "tags": []}, {"title": "Human-Centered Evaluation of XAI Methods", "authors": ["Karam Dawoud", "Wojciech Samek", "Peter Eisert", "Sebastian Lapuschkin", "Sebastian Bosse"], "abstract": "In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called \"black boxes\" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offer humans a nearly equivalent depth of understanding. This enables users to discern and categorize images efficiently, reinforcing the value of these methods in enhancing AI transparency.", "submitted": "2023-10-16", "link": "https://arxiv.org/pdf/2310.07534", "tags": []}, {"title": "Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining", "authors": ["Sadasivan Shankar"], "abstract": "Estimates of energy usage in layers of computing from devices to algorithms have been determined and analyzed. Building on the previous analysis [3], energy needed from single devices and systems including three large-scale computing applications such as Artificial Intelligence (AI)/Machine Learning for Natural Language Processing, Scientific Simulations, and Cryptocurrency Mining have been estimated. In contrast to the bit-level switching, in which transistors achieved energy efficiency due to geometrical scaling, higher energy is expended both at the at the instructions and simulations levels of an application. Additionally, the analysis based on AI/ML Accelerators indicate that changes in architectures using an older semiconductor technology node have comparable energy efficiency with a different architecture using a newer technology. Further comparisons of the energy in computing systems with the thermodynamic and biological limits, indicate that there is a 27-36 orders of magnitude higher energy requirements for total simulation of an application. These energy estimates underscore the need for serious considerations of energy efficiency in computing by including energy as a design parameter, enabling growing needs of compute-intensive applications in a digital world.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07516", "tags": []}, {"title": "Model-based Clustering of Individuals' Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance", "authors": ["Mandani Ntekouli", "Gerasimos Spanakis", "Lourens Waldorp", "Anne Roefs"], "abstract": "Through Ecological Momentary Assessment (EMA) studies, a number of time-series data is collected across multiple individuals, continuously monitoring various items of emotional behavior. Such complex data is commonly analyzed in an individual level, using personalized models. However, it is believed that additional information of similar individuals is likely to enhance these models leading to better individuals' description. Thus, clustering is investigated with an aim to group together the most similar individuals, and subsequently use this information in group-based models in order to improve individuals' predictive performance. More specifically, two model-based clustering approaches are examined, where the first is using model-extracted parameters of personalized models, whereas the second is optimized on the model-based forecasting performance. Both methods are then analyzed using intrinsic clustering evaluation measures (e.g. Silhouette coefficients) as well as the performance of a downstream forecasting scheme, where each forecasting group-model is devoted to describe all individuals belonging to one cluster. Among these, clustering based on performance shows the best results, in terms of all examined evaluation measures. As another level of evaluation, those group-models' performance is compared to three baseline scenarios, the personalized, the all-in-one group and the random group-based concept. According to this comparison, the superiority of clustering-based methods is again confirmed, indicating that the utilization of group-based information could be effectively enhance the overall performance of all individuals' data.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07491", "tags": []}, {"title": "An Ontology of Co-Creative AI Systems", "authors": ["Zhiyu Lin", "Mark Riedl"], "abstract": "The term co-creativity has been used to describe a wide variety of human-AI assemblages in which human and AI are both involved in a creative endeavor. In order to assist with disambiguating research efforts, we present an ontology of co-creative systems, focusing on how responsibilities are divided between human and AI system and the information exchanged between them. We extend Lubart's original ontology of creativity support tools with three new categories emphasizing artificial intelligence: computer-as-subcontractor, computer-as-critic, and computer-as-teammate, some of which have sub-categorizations.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07472", "tags": ["Ethical AI and Bias Mitigation", "AI in Education"]}, {"title": "AI/ML-based Load Prediction in IEEE 802.11 Enterprise Networks", "authors": ["Francesc Wilhelmi", "Dariush Salami", "Gianluca Fontanesi", "Lorenzo Galati-Giordano", "Mika Kasslin"], "abstract": "Enterprise Wi-Fi networks can greatly benefit from Artificial Intelligence and Machine Learning (AI/ML) thanks to their well-developed management and operation capabilities. At the same time, AI/ML-based traffic/load prediction is one of the most appealing data-driven solutions to improve the Wi-Fi experience, either through the enablement of autonomous operation or by boosting troubleshooting with forecasted network utilization. In this paper, we study the suitability and feasibility of adopting AI/ML-based load prediction in practical enterprise Wi-Fi networks. While leveraging AI/ML solutions can potentially contribute to optimizing Wi-Fi networks in terms of energy efficiency, performance, and reliability, their effective adoption is constrained to aspects like data availability and quality, computational capabilities, and energy consumption. Our results show that hardware-constrained AI/ML models can potentially predict network load with less than 20% average error and 3% 85th-percentile error, which constitutes a suitable input for proactively driving Wi-Fi network optimization.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07467", "tags": ["Neuromorphic Computing"]}, {"title": "Uncovering ECG Changes during Healthy Aging using Explainable AI", "authors": ["Gabriel Ott", "Yannik Schaubelt", "Juan Miguel Lopez Alcaraz", "Wilhelm Haverkamp", "Nils Strodthoff"], "abstract": "Cardiovascular diseases remain the leading global cause of mortality. This necessitates a profound understanding of heart aging processes to diagnose constraints in cardiovascular fitness. Traditionally, most of such insights have been drawn from the analysis of electrocardiogram (ECG) feature changes of individuals as they age. However, these features, while informative, may potentially obscure underlying data relationships. In this paper, we employ a deep-learning model and a tree-based model to analyze ECG data from a robust dataset of healthy individuals across varying ages in both raw signals and ECG feature format. Explainable AI techniques are then used to identify ECG features or raw signal characteristics are most discriminative for distinguishing between age groups. Our analysis with tree-based classifiers reveal age-related declines in inferred breathing rates and identifies notably high SDANN values as indicative of elderly individuals, distinguishing them from younger adults. Furthermore, the deep-learning model underscores the pivotal role of the P-wave in age predictions across all age groups, suggesting potential changes in the distribution of different P-wave types with age. These findings shed new light on age-related ECG changes, offering insights that transcend traditional feature-based approaches.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07463", "tags": []}, {"title": "Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation", "authors": ["Jian Wang", "Yi Cheng", "Dongding Lin", "Chak Tou Leong", "Wenjie Li"], "abstract": "Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.", "submitted": "2023-10-13", "link": "https://arxiv.org/pdf/2310.07397", "tags": []}, {"title": "WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models", "authors": ["Mehdi Letafati", "Samad Ali", "Matti Latva-aho"], "abstract": "Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose denoising diffusion probabilistic models (DDPM) for a wireless communication scheme with non-ideal transceivers, where 30% improvement is achieved in terms of bit error rate. As the second application, DDPMs are employed at the transmitter to shape the constellation symbols, highlighting a robust out-of-distribution performance. Finally, future directions and open issues for the development of generative AI-based wireless systems are discussed to promote future research endeavors towards wireless generative AI (WiGenAI).", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.07312", "tags": []}, {"title": "Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine Learning in Epigraphy", "authors": ["Andrei C. Aioanei", "Regine Hunziker-Rodewald", "Konstantin Klein", "Dominik L. Michels"], "abstract": "Epigraphy increasingly turns to modern artificial intelligence (AI) technologies such as machine learning (ML) for extracting insights from ancient inscriptions. However, scarce labeled data for training ML algorithms severely limits current techniques, especially for ancient scripts like Old Aramaic. Our research pioneers an innovative methodology for generating synthetic training data tailored to Old Aramaic letters. Our pipeline synthesizes photo-realistic Aramaic letter datasets, incorporating textural features, lighting, damage, and augmentations to mimic real-world inscription diversity. Despite minimal real examples, we engineer a dataset of 250,000 training and 25,000 validation images covering the 22 letter classes in the Aramaic alphabet. This comprehensive corpus provides a robust volume of data for training a residual neural network (ResNet) to classify highly degraded Aramaic letters. The ResNet model demonstrates high accuracy in classifying real images from the 8th century BCE Hadad statue inscription. Additional experiments validate performance on varying materials and styles, proving effective generalization. Our results validate the model's capabilities in handling diverse real-world scenarios, proving the viability of our synthetic data approach and avoiding the dependence on scarce training data that has constrained epigraphic analysis. Our innovative framework elevates interpretation accuracy on damaged inscriptions, thus enhancing knowledge extraction from these historical resources.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07310", "tags": ["AI and Privacy"]}, {"title": "Search for $J/\u03c8$ weak decays containing $D$ meson", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "M. R. An", "Q. An", "Y. Bai", "O. Bakina", "I. Balossino", "Y. Ban", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "et al. (600 additional authors not shown)"], "abstract": "Using a sample of about 10 billion $J/\u03c8$ events with the BESIII detector, we search for the weak decays of $J/\u03c8\\to \\bar{D}^0\u03c0^0 + c.c.$, $J/\u03c8\\to \\bar{D}^0\u03b7+ c.c.$, $J/\u03c8\\to \\bar{D}^0\u03c1^0 + c.c.$, $J/\u03c8\\to D^-\u03c0^+ + c.c.$, and $J/\u03c8\\to D^-\u03c1^+ + c.c.$. Since no significant signal is observed, we set the upper limits of the branching fractions of these decays to be $\\mathcal{B}(J/\u03c8\\to \\bar{D}^0\u03c0^0 + c.c.) < 4.7 \\times 10^{-7}$, $\\mathcal{B}(J/\u03c8\\to \\bar{D}^0\u03b7+ c.c.) < 6.8 \\times 10^{-7}$, $\\mathcal{B}(J/\u03c8\\to \\bar{D}^0\u03c1^0 + c.c.) < 5.2 \\times 10^{-7}$, $\\mathcal{B}(J/\u03c8\\to D^-\u03c0^+ + c.c.) < 7.0 \\times 10^{-8}$, and $\\mathcal{B}(J/\u03c8\\to D^-\u03c1^+ + c.c.) < 6.0 \\times 10^{-7}$ at the 90\\% confidence level.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07277", "tags": []}, {"title": "Classification of Dysarthria based on the Levels of Severity. A Systematic Review", "authors": ["Afnan Al-Ali", "Somaya Al-Maadeed", "Moutaz Saleh", "Rani Chinnappa Naidu", "Zachariah C Alex", "Prakash Ramachandran", "Rajeev Khoodeeram", "Rajesh Kumar M"], "abstract": "Dysarthria is a neurological speech disorder that can significantly impact affected individuals' communication abilities and overall quality of life. The accurate and objective classification of dysarthria and the determination of its severity are crucial for effective therapeutic intervention. While traditional assessments by speech-language pathologists (SLPs) are common, they are often subjective, time-consuming, and can vary between practitioners. Emerging machine learning-based models have shown the potential to provide a more objective dysarthria assessment, enhancing diagnostic accuracy and reliability. This systematic review aims to comprehensively analyze current methodologies for classifying dysarthria based on severity levels. Specifically, this review will focus on determining the most effective set and type of features that can be used for automatic patient classification and evaluating the best AI techniques for this purpose. We will systematically review the literature on the automatic classification of dysarthria severity levels. Sources of information will include electronic databases and grey literature. Selection criteria will be established based on relevance to the research questions. Data extraction will include methodologies used, the type of features extracted for classification, and AI techniques employed. The findings of this systematic review will contribute to the current understanding of dysarthria classification, inform future research, and support the development of improved diagnostic tools. The implications of these findings could be significant in advancing patient care and improving therapeutic outcomes for individuals affected by dysarthria.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07264", "tags": []}, {"title": "Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset", "authors": ["Ibrahim Ethem Hamamci"], "abstract": "Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing high-quality and realistic MRI sequences, enabling clinicians to improve their diagnostic capabilities and support the application of AI methods to brain tumor MRI quantification.", "submitted": "2023-10-14", "link": null, "tags": []}, {"title": "Improved Membership Inference Attacks Against Language Classification Models", "authors": ["Shlomit Shachor", "Natalia Razinkov", "Abigail Goldsteen"], "abstract": "Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07219", "tags": ["AI and Privacy"]}, {"title": "State of the Art on Diffusion Models for Visual Computing", "authors": ["Ryan Po", "Wang Yifan", "Vladislav Golyanik", "Kfir Aberman", "Jonathan T. Barron", "Amit H. Bermano", "Eric Ryan Chan", "Tali Dekel", "Aleksander Holynski", "Angjoo Kanazawa", "C. Karen Liu", "Lingjie Liu", "Ben Mildenhall", "Matthias Nie\u00dfner", "Bj\u00f6rn Ommer", "Christian Theobalt", "Peter Wonka", "Gordon Wetzstein"], "abstract": "The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07204", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "MatChat: A Large Language Model and Application Service Platform for Materials Science", "authors": ["Ziyi Chen", "Fankai Xie", "Meng Wan", "Yang Yuan", "Miao Liu", "Zongguo Wang", "Sheng Meng", "Yangang Wang"], "abstract": "The prediction of chemical synthesis pathways plays a pivotal role in materials science research. Challenges, such as the complexity of synthesis pathways and the lack of comprehensive datasets, currently hinder our ability to predict these chemical processes accurately. However, recent advancements in generative artificial intelligence (GAI), including automated text generation and question-answering systems, coupled with fine-tuning techniques, have facilitated the deployment of large-scale AI models tailored to specific domains. In this study, we harness the power of the LLaMA2-7B model and enhance it through a learning process that incorporates 13,878 pieces of structured material knowledge data. This specialized AI model, named MatChat, focuses on predicting inorganic material synthesis pathways. MatChat exhibits remarkable proficiency in generating and reasoning with knowledge in materials science. Although MatChat requires further refinement to meet the diverse material design needs, this research undeniably highlights its impressive reasoning capabilities and innovative potential in the field of materials science. MatChat is now accessible online and open for use, with both the model and its application framework available as open source. This study establishes a robust foundation for collaborative innovation in the integration of generative AI in materials science.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.07197", "tags": ["AI in Healthcare", "AI in Education", "AI and Privacy"]}, {"title": "LLark: A Multimodal Foundation Model for Music", "authors": ["Josh Gardner", "Simon Durand", "Daniel Stoller", "Rachel M. Bittner"], "abstract": "Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https://github.com/spotify-research/llark .", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.07160", "tags": ["Natural Language Processing"]}, {"title": "Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting", "authors": ["Zhiyu Chen", "Yujie Lu", "William Yang Wang"], "abstract": "Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient's speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cognitive distortion detection, while generating high-quality rationales approved by human experts.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.07146", "tags": []}, {"title": "What constraints can one pose on the maximum mass of neutron stars from multi-messenger observations?", "authors": ["Shunke Ai", "He Gao", "Yong Yuan", "Bing Zhang", "Lin Lan"], "abstract": "The maximum mass of neutron stars ($M_{\\rm TOV}$) plays a crucial role in understanding their equation of state (EoS). Previous studies have used the measurements for the compactness of massive pulsars and the tidal deformability of neutron stars in binary neutron star (BNS) mergers to constrain the EoS and thus the $M_{\\rm TOV}$. The discovery of the most massive pulsar, PSR J0952-0607, with a mass $\\sim 2.35M_{\\odot}$, has provided a valuable lower limit for $M_{\\rm TOV}$. Another efficient method to constrain $M_{\\rm TOV}$ is by examining the type of central remnant formed after a BNS merger. Gravitational wave (GW) data can provide the total mass of the system, while accompanying electromagnetic signals can help infer the remnant type. In this study, we combine all the previous constraints and utilize the observational facts that about $24\\%$ of the short gamma-ray bursts are followed by an X-ray internal plateau, which indicate that roughly this fraction of BNS mergers yield supermassive neutron stars, to perform (Markov Chain) Monte Carlo simulations. These simulations allow us to explore the probability density distribution of $M_{\\rm TOV}$ and other parameters related to BNS mergers. Our findings suggest that $M_{\\rm TOV}$ is likely around $2.49M_{\\odot} - 2.52M_{\\odot}$, with an uncertainty range of approximately [$-0.16M_{\\odot}$, $0.15M_{\\odot}$] ([$-0.28M_{\\odot}$, $0.26M_{\\odot}$]) at $1\u03c3$ ($2\u03c3$) confidence level. Furthermore, we examine the type of merger remnants in specific events like GW170817 and GW190425 to further constrain $M_{\\rm TOV}$ and other relevant parameters, which can help to understand the physical processes involved in BNS mergers.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.07133", "tags": []}, {"title": "An HCI-Centric Survey and Taxonomy of Human-Generative-AI Interactions", "authors": ["Jingyu Shi", "Rahul Jain", "Hyungjun Doh", "Ryo Suzuki", "Karthik Ramani"], "abstract": "Generative AI (GenAI) has shown remarkable capabilities in generating diverse and realistic content across different formats like images, videos, and text. In Generative AI, human involvement is essential, thus HCI literature has investigated how to effectively create collaborations between humans and GenAI systems. However, the current literature lacks a comprehensive framework to better understand Human-GenAI Interactions, as the holistic aspects of human-centered GenAI systems are rarely analyzed systematically. In this paper, we present a survey of 154 papers, providing a novel taxonomy and analysis of Human-GenAI Interactions from both human and Gen-AI perspectives. The dimension of design space includes 1) Purposes of Using Generative AI, 2) Feedback from Models to Users , 3) Control from Users to Models, 4) Levels of Engagement, 5) Application Domains, and 6) Evaluation Strategies. Our work is also timely at the current development stage of GenAI, where the Human-GenAI interaction design is of paramount importance. We also highlight challenges and opportunities to guide the design of Gen-AI systems and interactions towards the future design of human-centered Generative AI applications.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.07127", "tags": ["AI in Education", "AI and Privacy"]}, {"title": "ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations", "authors": ["Benjamin Kereopa-Yorke"], "abstract": "In a digital epoch where cyberspace is the emerging nexus of geopolitical contention, the melding of information operations and Large Language Models (LLMs) heralds a paradigm shift, replete with immense opportunities and intricate challenges. As tools like the Mistral 7B LLM (Mistral, 2023) democratise access to LLM capabilities (Jin et al., 2023), a vast spectrum of actors, from sovereign nations to rogue entities (Howard et al., 2023), find themselves equipped with potent narrative-shaping instruments (Goldstein et al., 2023). This paper puts forth a framework for navigating this brave new world in the \"ClausewitzGPT\" equation. This novel formulation not only seeks to quantify the risks inherent in machine-speed LLM-augmented operations but also underscores the vital role of autonomous AI agents (Wang, Xie, et al., 2023). These agents, embodying ethical considerations (Hendrycks et al., 2021), emerge as indispensable components (Wang, Ma, et al., 2023), ensuring that as we race forward, we do not lose sight of moral compasses and societal imperatives.\n  Mathematically underpinned and inspired by the timeless tenets of Clausewitz's military strategy (Clausewitz, 1832), this thesis delves into the intricate dynamics of AI-augmented information operations. With references to recent findings and research (Department of State, 2023), it highlights the staggering year-on-year growth of AI information campaigns (Evgeny Pashentsev, 2023), stressing the urgency of our current juncture. The synthesis of Enlightenment thinking, and Clausewitz's principles provides a foundational lens, emphasising the imperative of clear strategic vision, ethical considerations, and holistic understanding in the face of rapid technological advancement.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.07099", "tags": []}, {"title": "Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling", "authors": ["Clay H. Yoo", "Ashiqur R. KhudaBukhsh"], "abstract": "This paper makes two key contributions. First, it argues that highly specialized rare content classifiers trained on small data typically have limited exposure to the richness and topical diversity of the negative class (dubbed anticontent) as observed in the wild. As a result, these classifiers' strong performance observed on the test set may not translate into real-world settings. In the context of COVID-19 misinformation detection, we conduct an in-the-wild audit of multiple datasets and demonstrate that models trained with several prominently cited recent datasets are vulnerable to anticontent when evaluated in the wild. Second, we present a novel active learning pipeline that requires zero manual annotation and iteratively augments the training data with challenging anticontent, robustifying these classifiers.", "submitted": "2023-08-05", "link": "https://arxiv.org/pdf/2310.07078", "tags": []}, {"title": "Case Law Grounding: Aligning Judgments of Humans and AI on Socially-Constructed Concepts", "authors": ["Quan Ze Chen", "Amy X. Zhang"], "abstract": "Systems for making determinations on socially-constructed and complex concepts at scale are increasingly being deployed. To make such fuzzy concepts tractable for training and evaluating AI, aligning model outputs, or human-in-the-loop workflows, the prevailing strategy involves developing `constitutions' in the form of rules, policies, or principles. However, high-level rules often fail to capture situational nuances or have differing interpretations, resulting in inconsistent decisions. In this work, we introduce case law grounding (CLG), a hybrid workflow inspired by case law in the legal realm where past judgments on specific cases inform new decisions. Evaluating on two task domains, we find that CLG can improve alignment of decisions (+9.6% and +10.9% accuracy) and consistency ($\u0394\\bar\u03ba$ of +0.263 and +0.433) of human decision-makers, while also providing auditable rationales. We also find similarly substantial alignment improvements for an LLM decision-maker (+25% and +23% accuracy).", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.07019", "tags": []}, {"title": "Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention", "authors": ["Rodolfo Valentim", "Idilio Drago", "Marco Mellia", "Federico Cerutti"], "abstract": "Sound-squatting is a phishing attack that tricks users into malicious resources by exploiting similarities in the pronunciation of words. Proactive defense against sound-squatting candidates is complex, and existing solutions rely on manually curated lists of homophones. We here introduce Sound-skwatter, a multi-language AI-based system that generates sound-squatting candidates for proactive defense. Sound-skwatter relies on an innovative multi-modal combination of Transformers Networks and acoustic models to learn sound similarities. We show that Sound-skwatter can automatically list known homophones and thousands of high-quality candidates. In addition, it covers cross-language sound-squatting, i.e., when the reader and the listener speak different languages, supporting any combination of languages. We apply Sound-skwatter to network-centric phishing via squatted domain names. We find ~ 10% of the generated domains exist in the wild, the vast majority unknown to protection solutions. Next, we show attacks on the PyPI package manager, where ~ 17% of the popular packages have at least one existing candidate. We believe Sound-skwatter is a crucial asset to mitigate the sound-squatting phenomenon proactively on the Internet. To increase its impact, we publish an online demo and release our models and code as open source.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.07005", "tags": []}, {"title": "CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a Multiplatform Artificial Intelligence Toolkit for Portable and Wearable Device Electrocardiograms", "authors": ["Sumukh Vasisht Shankar", "Evangelos K Oikonomou", "Rohan Khera"], "abstract": "In the rapidly evolving landscape of modern healthcare, the integration of wearable & portable technology provides a unique opportunity for personalized health monitoring in the community. Devices like the Apple Watch, FitBit, and AliveCor KardiaMobile have revolutionized the acquisition and processing of intricate health data streams. Amidst the variety of data collected by these gadgets, single-lead electrocardiogram (ECG) recordings have emerged as a crucial source of information for monitoring cardiovascular health. There has been significant advances in artificial intelligence capable of interpreting these 1-lead ECGs, facilitating clinical diagnosis as well as the detection of rare cardiac disorders. This design study describes the development of an innovative multiplatform system aimed at the rapid deployment of AI-based ECG solutions for clinical investigation & care delivery. The study examines design considerations, aligning them with specific applications, develops data flows to maximize efficiency for research & clinical use. This process encompasses the reception of single-lead ECGs from diverse wearable devices, channeling this data into a centralized data lake & facilitating real-time inference through AI models for ECG interpretation. An evaluation of the platform demonstrates a mean duration from acquisition to reporting of results of 33.0 to 35.7 seconds, after a standard 30 second acquisition. There were no substantial differences in acquisition to reporting across two commercially available devices (Apple Watch and KardiaMobile). These results demonstrate the succcessful translation of design principles into a fully integrated & efficient strategy for leveraging 1-lead ECGs across platforms & interpretation by AI-ECG algorithms. Such a platform is critical to translating AI discoveries for wearable and portable ECG devices to clinical impact through rapid deployment.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.07000", "tags": ["AI and Privacy"]}, {"title": "A Systematic Review of Machine Learning Enabled Phishing", "authors": ["Krystal A. Jackson"], "abstract": "Developments in artificial intelligence (AI) are likely to affect social engineering and change cyber defense operations. The broad and sweeping nature of AI impact means that many aspects of social engineering could be automated, potentially giving adversaries an advantage. In this review, we assess the ways phishing and spear-phishing might be affected by machine learning techniques. By performing a systematic review of demonstrated ML-enabled phishing campaigns, we take a broad survey the space for current developments. We develop a detailed approach for evaluation by creating a risk framework for analyzing and contextualizing these developments. The object of this review is to answer the research questions: (1) Are there high-risk ML-enabled phishing use cases? (2) Is there a meaningful difference between traditional targeted phishing campaigns and ML-enabled phishing campaigns? Practitioners may use this review to inform standards, future research directions, and cyber defense strategies.", "submitted": "2023-05-24", "link": "https://arxiv.org/pdf/2310.06998", "tags": []}, {"title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation", "authors": ["Yangsibo Huang", "Samyak Gupta", "Mengzhou Xia", "Kai Li", "Danqi Chen"], "abstract": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as \"jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. Our code is available at https://github.com/Princeton-SysML/Jailbreak_LLM.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06987", "tags": ["Natural Language Processing"]}, {"title": "Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models", "authors": ["Courtland Leer", "Vincent Trost", "Vineeth Voruganti"], "abstract": "Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \\textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mitigate risk along with possible directions for future inquiry.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06983", "tags": ["AI in Education", "AI and Privacy"]}, {"title": "LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing", "authors": ["Stephen Moskal", "Sam Laney", "Erik Hemberg", "Una-May O'Reilly"], "abstract": "In this paper, we explore the potential of Large Language Models (LLMs) to reason about threats, generate information about tools, and automate cyber campaigns. We begin with a manual exploration of LLMs in supporting specific threat-related actions and decisions. We proceed by automating the decision process in a cyber campaign. We present prompt engineering approaches for a plan-act-report loop for one action of a threat campaign and and a prompt chaining design that directs the sequential decision process of a multi-action campaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the short campaign we demonstrate and provide insights into prompt design for eliciting actionable responses. We discuss the potential impact of LLMs on the threat landscape and the ethical considerations of using LLMs for accelerating threat actor capabilities. We report a promising, yet concerning, application of generative AI to cyber threats. However, the LLM's capabilities to deal with more complex networks, sophisticated vulnerabilities, and the sensitivity of prompts are open questions. This research should spur deliberations over the inevitable advancements in LLM-supported cyber adversarial landscape.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06936", "tags": ["Cyber Security"]}, {"title": "Brave new world: Artificial Intelligence in teaching and learning", "authors": ["Adrian Groza", "Anca Marginean"], "abstract": "We exemplify how Large Language Models are used in both teaching and learning. We also discuss the AI incidents that have already occurred in the education domain, and we argue for the urgent need to introduce AI policies in universities and for the ongoing strategies to regulate AI. Regarding policy for AI, our view is that each institution should have a policy for AI in teaching and learning. This is important from at least twofolds: (i) to raise awareness on the numerous educational tools that can both positively and negatively affect education; (ii) to minimise the risk of AI incidents in education.", "submitted": "2023-09-27", "link": "https://arxiv.org/pdf/2310.06856", "tags": ["Ethical AI and Bias Mitigation", "AI in Education"]}, {"title": "How Knowledge Workers Think Generative AI Will (Not) Transform Their Industries", "authors": ["Allison Woodruff", "Renee Shelby", "Patrick Gage Kelley", "Steven Rousso-Schindler", "Jamila Smith-Loud", "Lauren Wilcox"], "abstract": "Generative AI is expected to have transformative effects in multiple knowledge industries. To better understand how knowledge workers expect generative AI may affect their industries in the future, we conducted participatory research workshops for seven different industries, with a total of 54 participants across three US cities. We describe participants' expectations of generative AI's impact, including a dominant narrative that cut across the groups' discourse: participants largely envision generative AI as a tool to perform menial work, under human review. Participants do not generally anticipate the disruptive changes to knowledge industries currently projected in common media and academic narratives. Participants do however envision generative AI may amplify four social forces currently shaping their industries: deskilling, dehumanization, disconnection, and disinformation. We describe these forces, and then we provide additional detail regarding attitudes in specific knowledge industries. We conclude with a discussion of implications and research challenges for the HCI community.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06778", "tags": ["AI in Education", "Ethical AI and Bias Mitigation"]}, {"title": "Conceptual Framework for Autonomous Cognitive Entities", "authors": ["David Shapiro", "Wangfan Li", "Manuel Delaflor", "Carlos Toxtli"], "abstract": "The rapid development and adoption of Generative AI (GAI) technology in the form of chatbots such as ChatGPT and Claude has greatly increased interest in agentic machines. This paper introduces the Autonomous Cognitive Entity (ACE) model, a novel framework for a cognitive architecture, enabling machines and software agents to operate more independently. Drawing inspiration from the OSI model, the ACE framework presents layers of abstraction to conceptualize artificial cognitive architectures. The model is designed to harness the capabilities of the latest generative AI technologies, including large language models (LLMs) and multimodal generative models (MMMs), to build autonomous, agentic systems. The ACE framework comprises six layers: the Aspirational Layer, Global Strategy, Agent Model, Executive Function, Cognitive Control, and Task Prosecution. Each layer plays a distinct role, ranging from setting the moral compass and strategic thinking to task selection and execution. The ACE framework also incorporates mechanisms for handling failures and adapting actions, thereby enhancing the robustness and flexibility of autonomous agents. This paper introduces the conceptual framework and proposes implementation strategies that have been tested and observed in industry. The goal of this paper is to formalize this framework so as to be more accessible.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.06775", "tags": []}, {"title": "Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography Parameters in Third-Party E-Commerce Integrations: A Pre-Quantum Era Analysis", "authors": ["Felipe Tellez", "Jorge Ortiz"], "abstract": "This paper presents a comparative analysis between the Genetic Algorithm (GA) and Particle Swarm Optimization (PSO), two vital artificial intelligence algorithms, focusing on optimizing Elliptic Curve Cryptography (ECC) parameters. These encompass the elliptic curve coefficients, prime number, generator point, group order, and cofactor. The study provides insights into which of the bio-inspired algorithms yields better optimization results for ECC configurations, examining performances under the same fitness function. This function incorporates methods to ensure robust ECC parameters, including assessing for singular or anomalous curves and applying Pollard's rho attack and Hasse's theorem for optimization precision. The optimized parameters generated by GA and PSO are tested in a simulated e-commerce environment, contrasting with well-known curves like secp256k1 during the transmission of order messages using Elliptic Curve-Diffie Hellman (ECDH) and Hash-based Message Authentication Code (HMAC). Focusing on traditional computing in the pre-quantum era, this research highlights the efficacy of GA and PSO in ECC optimization, with implications for enhancing cybersecurity in third-party e-commerce integrations. We recommend the immediate consideration of these findings before quantum computing's widespread adoption.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06752", "tags": []}, {"title": "Temporally Aligning Long Audio Interviews with Questions: A Case Study in Multimodal Data Integration", "authors": ["Piyush Singh Pasi", "Karthikeya Battepati", "Preethi Jyothi", "Ganesh Ramakrishnan", "Tanmay Mahapatra", "Manoj Singh"], "abstract": "The problem of audio-to-text alignment has seen significant amount of research using complete supervision during training. However, this is typically not in the context of long audio recordings wherein the text being queried does not appear verbatim within the audio file. This work is a collaboration with a non-governmental organization called CARE India that collects long audio health surveys from young mothers residing in rural parts of Bihar, India. Given a question drawn from a questionnaire that is used to guide these surveys, we aim to locate where the question is asked within a long audio recording. This is of great value to African and Asian organizations that would otherwise have to painstakingly go through long and noisy audio recordings to locate questions (and answers) of interest. Our proposed framework, INDENT, uses a cross-attention-based model and prior information on the temporal ordering of sentences to learn speech embeddings that capture the semantics of the underlying spoken text. These learnt embeddings are used to retrieve the corresponding audio segment based on text queries at inference time. We empirically demonstrate the significant effectiveness (improvement in R-avg of about 3%) of our model over those obtained using text-based heuristics. We also show how noisy ASR, generated using state-of-the-art ASR models for Indian languages, yields better results when used in place of speech. INDENT, trained only on Hindi data is able to cater to all languages supported by the (semantically) shared text space. We illustrate this empirically on 11 Indic languages.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06702", "tags": []}, {"title": "Machine Learning Quantum Systems with Magnetic p-bits", "authors": ["Shuvro Chowdhury", "Kerem Y. Camsari"], "abstract": "The slowing down of Moore's Law has led to a crisis as the computing workloads of Artificial Intelligence (AI) algorithms continue skyrocketing. There is an urgent need for scalable and energy-efficient hardware catering to the unique requirements of AI algorithms and applications. In this environment, probabilistic computing with p-bits emerged as a scalable, domain-specific, and energy-efficient computing paradigm, particularly useful for probabilistic applications and algorithms. In particular, spintronic devices such as stochastic magnetic tunnel junctions (sMTJ) show great promise in designing integrated p-computers. Here, we examine how a scalable probabilistic computer with such magnetic p-bits can be useful for an emerging field combining machine learning and quantum physics.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06679", "tags": []}, {"title": "Evaluating Explanation Methods for Vision-and-Language Navigation", "authors": ["Guanqi Chen", "Lei Yang", "Guanhua Chen", "Jia Pan"], "abstract": "The ability to navigate robots with natural language instructions in an unknown environment is a crucial step for achieving embodied artificial intelligence (AI). With the improving performance of deep neural models proposed in the field of vision-and-language navigation (VLN), it is equally interesting to know what information the models utilize for their decision-making in the navigation tasks. To understand the inner workings of deep neural models, various explanation methods have been developed for promoting explainable AI (XAI). But they are mostly applied to deep neural models for image or text classification tasks and little work has been done in explaining deep neural models for VLN tasks. In this paper, we address these problems by building quantitative benchmarks to evaluate explanation methods for VLN models in terms of faithfulness. We propose a new erasure-based evaluation pipeline to measure the step-wise textual explanation in the sequential decision-making setting. We evaluate several explanation methods for two representative VLN models on two popular VLN datasets and reveal valuable findings through our experiments.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06654", "tags": ["Explainable AI (XAI):"]}, {"title": "XAI for Early Crop Classification", "authors": ["Ayshah Chan", "Maja Schneider", "Marco K\u00f6rner"], "abstract": "We propose an approach for early crop classification through identifying important timesteps with eXplainable AI (XAI) methods. Our approach consists of training a baseline crop classification model to carry out layer-wise relevance propagation (LRP) so that the salient time step can be identified. We chose a selected number of such important time indices to create the bounding region of the shortest possible classification timeframe. We identified the period 21st April 2019 to 9th August 2019 as having the best trade-off in terms of accuracy and earliness. This timeframe only suffers a 0.75% loss in accuracy as compared to using the full timeseries. We observed that the LRP-derived important timesteps also highlight small details in input values that differentiates between different classes and", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06574", "tags": []}, {"title": "21 cm foreground removal using AI and frequency-difference technique", "authors": ["Feng Shi", "Haoxiang Chang", "Le Zhang", "Huanyuan Shan", "Jiajun Zhang", "Suiping Zhou", "Ming Jiang", "Zitong Wang"], "abstract": "The deep learning technique has been employed in removing foreground contaminants from 21 cm intensity mapping, but its effectiveness is limited by the large dynamic range of the foreground amplitude. In this study, we develop a novel foreground removal technique grounded in U-Net networks. The essence of this technique lies in introducing an innovative data preprocessing step specifically, utilizing the temperature difference between neighboring frequency bands as input, which can substantially reduce the dynamic range of foreground amplitudes by approximately two orders of magnitude. This reduction proves to be highly advantageous for the U-Net foreground removal. We observe that the HI signal can be reliably recovered, as indicated by the cross-correlation power spectra showing unity agreement at the scale of $k < 0.3 h^{-1}$Mpc in the absence of instrumental effects. Moreover, accounting for the systematic beam effects, our reconstruction displays consistent auto-correlation and cross-correlation power spectrum ratios at the $1\u03c3$ level across scales $k \\lesssim 0.1 h^{-1}$Mpc, with only a 10% reduction observed in the cross-correlation power spectrum at $k\\simeq0.2 h^{-1}$Mpc. The effects of redshift-space distortion are also reconstructed successfully, as evidenced by the quadrupole power spectra matching. In comparison, our method outperforms the traditional Principal Component Analysis method, which derived cross-correlation ratios are underestimated by around 75%. We simulated various white noise levels in the map and found that the mean cross-correlation ratio $\\bar{R}_\\mathrm{cross} \\gtrsim 0.75$ when the level of the thermal noise is smaller than or equal to that of the HI signal. We conclude that the proposed frequency-difference technique can significantly enhance network performance by reducing the amplitude range of foregrounds and aiding in the prevention of HI loss.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06518", "tags": []}, {"title": "Runway Sign Classifier: A DAL C Certifiable Machine Learning System", "authors": ["Konstantin Dmitriev", "Johann Schumann", "Islam Bostanov", "Mostafa Abdelhamid", "Florian Holzapfel"], "abstract": "In recent years, the remarkable progress of Machine Learning (ML) technologies within the domain of Artificial Intelligence (AI) systems has presented unprecedented opportunities for the aviation industry, paving the way for further advancements in automation, including the potential for single pilot or fully autonomous operation of large commercial airplanes. However, ML technology faces major incompatibilities with existing airborne certification standards, such as ML model traceability and explainability issues or the inadequacy of traditional coverage metrics. Certification of ML-based airborne systems using current standards is problematic due to these challenges. This paper presents a case study of an airborne system utilizing a Deep Neural Network (DNN) for airport sign detection and classification. Building upon our previous work, which demonstrates compliance with Design Assurance Level (DAL) D, we upgrade the system to meet the more stringent requirements of Design Assurance Level C. To achieve DAL C, we employ an established architectural mitigation technique involving two redundant and dissimilar Deep Neural Networks. The application of novel ML-specific data management techniques further enhances this approach. This work is intended to illustrate how the certification challenges of ML-based systems can be addressed for medium criticality airborne applications.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06506", "tags": ["Cyber Security", "AI in Autonomous Vehicles"]}, {"title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity", "authors": ["Robert Kirk", "Ishita Mediratta", "Christoforos Nalmpantis", "Jelena Luketina", "Eric Hambro", "Edward Grefenstette", "Roberta Raileanu"], "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the trade-off between generalisation and diversity.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06452", "tags": []}, {"title": "Retromorphic Testing: A New Approach to the Test Oracle Problem", "authors": ["Boxi Yu", "Qiuyang Mang", "Qingshuo Guo", "Pinjia He"], "abstract": "A test oracle serves as a criterion or mechanism to assess the correspondence between software output and the anticipated behavior for a given input set. In automated testing, black-box techniques, known for their non-intrusive nature in test oracle construction, are widely used, including notable methodologies like differential testing and metamorphic testing. Inspired by the mathematical concept of inverse function, we present Retromorphic Testing, a novel black-box testing methodology. It leverages an auxiliary program in conjunction with the program under test, which establishes a dual-program structure consisting of a forward program and a backward program. The input data is first processed by the forward program and then its program output is reversed to its original input format using the backward program. In particular, the auxiliary program can operate as either the forward or backward program, leading to different testing modes. The process concludes by examining the relationship between the initial input and the transformed output within the input domain. For example, to test the implementation of the sine function $\\sin(x)$, we can employ its inverse function, $\\arcsin(x)$, and validate the equation $x = \\sin(\\arcsin(x)+2k\u03c0), \\forall k \\in \\mathbb{Z}$. In addition to the high-level concept of Retromorphic Testing, this paper presents its three testing modes with illustrative use cases across diverse programs, including algorithms, traditional software, and AI applications.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06433", "tags": []}, {"title": "Proceedings of The first international workshop on eXplainable AI for the Arts (XAIxArts)", "authors": ["Nick Bryan-Kinns", "Corey Ford", "Alan Chamberlain", "Steven David Benford", "Helen Kennedy", "Zijin Li", "Wu Qiong", "Gus G. Xia", "Jeba Rezwana"], "abstract": "This first international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\n  Workshop held at the 15th ACM Conference on Creativity and Cognition (C&C 2023).", "submitted": "2023-10-10", "link": null, "tags": ["Explainable AI (XAI):", "Ethical AI and Bias Mitigation"]}, {"title": "DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening", "authors": ["Bowen Gao", "Bo Qiang", "Haichuan Tan", "Minsi Ren", "Yinjun Jia", "Minsi Lu", "Jingjing Liu", "Weiying Ma", "Yanyan Lan"], "abstract": "Virtual screening, which identifies potential drugs from vast compound databases to bind with a particular protein pocket, is a critical step in AI-assisted drug discovery. Traditional docking methods are highly time-consuming, and can only work with a restricted search library in real-life applications. Recent supervised learning approaches using scoring functions for binding-affinity prediction, although promising, have not yet surpassed docking methods due to their strong dependency on limited data with reliable binding-affinity labels. In this paper, we propose a novel contrastive learning framework, DrugCLIP, by reformulating virtual screening as a dense retrieval task and employing contrastive learning to align representations of binding protein pockets and molecules from a large quantity of pairwise data without explicit binding-affinity scores. We also introduce a biological-knowledge inspired data augmentation strategy to learn better protein-molecule representations. Extensive experiments show that DrugCLIP significantly outperforms traditional docking and supervised learning methods on diverse virtual screening benchmarks with highly reduced computation time, especially in zero-shot setting.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06367", "tags": ["Reinforcement Learning", "Large scale Machine Learning"]}, {"title": "Anticipating Impacts: Using Large-Scale Scenario Writing to Explore Diverse Implications of Generative AI in the News Environment", "authors": ["Kimon Kieslich", "Nicholas Diakopoulos", "Natali Helberger"], "abstract": "The tremendous rise of generative AI has reached every part of society - including the news environment. There are many concerns about the individual and societal impact of the increasing use of generative AI, including issues such as disinformation and misinformation, discrimination, and the promotion of social tensions. However, research on anticipating the impact of generative AI is still in its infancy and mostly limited to the views of technology developers and/or researchers. In this paper, we aim to broaden the perspective and capture the expectations of three stakeholder groups (news consumers; technology developers; content creators) about the potential negative impacts of generative AI, as well as mitigation strategies to address these. Methodologically, we apply scenario writing and use participatory foresight in the context of a survey (n=119) to delve into cognitively diverse imaginations of the future. We qualitatively analyze the scenarios using thematic analysis to systematically map potential impacts of generative AI on the news environment, potential mitigation strategies, and the role of stakeholders in causing and mitigating these impacts. In addition, we measure respondents' opinions on a specific mitigation strategy, namely transparency obligations as suggested in Article 52 of the draft EU AI Act. We compare the results across different stakeholder groups and elaborate on the (non-) presence of different expected impacts across these groups. We conclude by discussing the usefulness of scenario-writing and participatory foresight as a toolbox for generative AI impact assessment.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06361", "tags": ["Ethical AI and Bias Mitigation", "AI in Education", "AI and Privacy"]}, {"title": "Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing", "authors": ["Yingyan Zeng", "Xiaoyu Chen", "Ran Jin"], "abstract": "It is challenging but important to save annotation efforts in streaming data acquisition to maintain data quality for supervised learning base learners. We propose an ensemble active learning method to actively acquire samples for annotation by contextual bandits, which is will enforce the exploration-exploitation balance and leading to improved AI modeling performance.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06306", "tags": []}, {"title": "Dobby: A Conversational Service Robot Driven by GPT-4", "authors": ["Carson Stark", "Bohkyung Chun", "Casey Charleston", "Varsha Ravi", "Luis Pabon", "Surya Sunkari", "Tarun Mohan", "Peter Stone", "Justin Hart"], "abstract": "This work introduces a robotics platform which embeds a conversational AI agent in an embodied system for natural language understanding and intelligent decision-making for service tasks; integrating task planning and human-like conversation. The agent is derived from a large language model, which has learned from a vast corpus of general knowledge. In addition to generating dialogue, this agent can interface with the physical world by invoking commands on the robot; seamlessly merging communication and behavior. This system is demonstrated in a free-form tour-guide scenario, in an HRI study combining robots with and without conversational AI capabilities. Performance is measured along five dimensions: overall effectiveness, exploration abilities, scrutinization abilities, receptiveness to personification, and adaptability.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.06303", "tags": ["Natural Language Processing"]}, {"title": "BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models", "authors": ["Haoxiang Luo", "Jian Luo", "Athanasios V. Vasilakos"], "abstract": "In recent years, artificial intelligence (AI) and machine learning (ML) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. Among them, the AI language model represented by ChatGPT has made great progress. Such large language models (LLMs) serve people in the form of AI-generated content (AIGC) and are widely used in consulting, healthcare, and education. However, it is difficult to guarantee the authenticity and reliability of AIGC learning data. In addition, there are also hidden dangers of privacy disclosure in distributed AI training. Moreover, the content generated by LLMs is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. The above information security issues in the coming era of AI powered by LLMs will be infinitely amplified and affect everyone's life. Therefore, we consider empowering LLMs using blockchain technology with superior security features to propose a vision for trusted AI. This paper mainly introduces the motivation and technical route of blockchain for LLM (BC4LLM), including reliable learning corpus, secure training process, and identifiable generated content. Meanwhile, this paper also reviews the potential applications and future challenges, especially in the frontier communication networks field, including network resource allocation, dynamic spectrum sharing, and semantic communication. Based on the above work combined and the prospect of blockchain and LLMs, it is expected to help the early realization of trusted AI and provide guidance for the academic community.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06278", "tags": ["AI in Education", "Ethical AI and Bias Mitigation", "AI and Privacy"]}, {"title": "The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, & Future Improvements", "authors": ["Michael Feffer", "Nikolas Martelaro", "Hoda Heidari"], "abstract": "Prior work has established the importance of integrating AI ethics topics into computer and data sciences curricula. We provide evidence suggesting that one of the critical objectives of AI Ethics education must be to raise awareness of AI harms. While there are various sources to learn about such harms, The AI Incident Database (AIID) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of AI technologies in the real world. This study assesses the effectiveness of AIID as an educational tool to raise awareness regarding the prevalence and severity of AI harms in socially high-stakes domains. We present findings obtained through a classroom study conducted at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our qualitative findings characterize students' initial perceptions of core topics in AI ethics and their desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. We find that interacting with the database helps students better understand the magnitude and severity of AI harms and instills in them a sense of urgency around (a) designing functional and safe AI and (b) strengthening governance and accountability mechanisms. Finally, we compile students' feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of AI harms in AI ethics education.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06269", "tags": ["Ethical AI and Bias Mitigation", "AI in Education", "AI and Privacy"]}, {"title": "Rate Compatible LDPC Neural Decoding Network: A Multi-Task Learning Approach", "authors": ["Yukun Cheng", "Wei Chen", "Lun Li", "Bo Ai"], "abstract": "Deep learning based decoding networks have shown significant improvement in decoding LDPC codes, but the neural decoders are limited by rate-matching operations such as puncturing or extending, thus needing to train multiple decoders with different code rates for a variety of channel conditions. In this correspondence, we propose a Multi-Task Learning based rate-compatible LDPC ecoding network, which utilizes the structure of raptor-like LDPC codes and can deal with multiple code rates. In the proposed network, different portions of parameters are activated to deal with distinct code rates, which leads to parameter sharing among tasks. Numerical experiments demonstrate the effectiveness of the proposed method. Training the specially designed network under multiple code rates makes the decoder compatible with multiple code rates without sacrificing frame error rate performance.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06256", "tags": []}, {"title": "Differentially Private Multi-Site Treatment Effect Estimation", "authors": ["Tatsuki Koga", "Kamalika Chaudhuri", "David Page"], "abstract": "Patient privacy is a major barrier to healthcare AI. For confidentiality reasons, most patient data remains in silo in separate hospitals, preventing the design of data-driven healthcare AI systems that need large volumes of patient data to make effective decisions. A solution to this is collective learning across multiple sites through federated learning with differential privacy. However, literature in this space typically focuses on differentially private statistical estimation and machine learning, which is different from the causal inference-related problems that arise in healthcare. In this work, we take a fresh look at federated learning with a focus on causal inference; specifically, we look at estimating the average treatment effect (ATE), an important task in causal inference for healthcare applications, and provide a federated analytics approach to enable ATE estimation across multiple sites along with differential privacy (DP) guarantees at each site. The main challenge comes from site heterogeneity -- different sites have different sample sizes and privacy budgets. We address this through a class of per-site estimation algorithms that reports the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate. Our experiments on real and synthetic data show that our method reliably aggregates private statistics across sites and provides better privacy-utility tradeoff under site heterogeneity than baselines.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06237", "tags": ["AI and Privacy", "AI in Healthcare"]}, {"title": "Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI", "authors": ["Masahiro Yamamoto"], "abstract": "Since the invention of computers, communication through natural language (actual human language) has been a dream technology. However, natural language is extremely difficult to mathematically formulate, making it difficult to realize as an algorithm without considering programming. While there have been numerous technological developments, one cannot say that any results allowing free utilization have been achieved thus far. In the case of language learning in humans, for instance when learning one's mother tongue or foreign language, one must admit that this process is similar to the adage \"practice makes perfect\" in principle, even though the learning method is significant up to a point. Deep learning has played a central role in contemporary AI technology in recent years. When applied to natural language processing (NLP), this produced unprecedented results. Achievements exceeding the initial predictions have been reported from the results of learning vast amounts of textual data using deep learning. For instance, four arithmetic operations could be performed without explicit learning, thereby enabling the explanation of complex images and the generation of images from corresponding explanatory texts. It is an accurate example of the learner embodying the concept of \"practice makes perfect\" by using vast amounts of textual data. This report provides a technological explanation of how cutting-edge NLP has made it possible to realize the \"practice makes perfect\" principle. Additionally, examples of how this can be applied to business are provided. We reported in June 2022 in Japanese on the NLP movement from late 2021 to early 2022. We would like to summarize this as a memorandum since this is just the initial movement leading to the current large language models (LLMs).", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06228", "tags": ["Natural Language Processing"]}, {"title": "Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM", "authors": ["Saeed Maleki"], "abstract": "AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06178", "tags": ["AI in Healthcare"]}, {"title": "Factual and Personalized Recommendations using Language Models and Reinforcement Learning", "authors": ["Jihwan Jeong", "Yinlam Chow", "Guy Tennenholtz", "Chih-Wei Hsu", "Azamat Tulepbergenov", "Mohammad Ghavamzadeh", "Craig Boutilier"], "abstract": "Recommender systems (RSs) play a central role in connecting users to content, products, and services, matching candidate items to users based on their preferences. While traditional RSs rely on implicit user feedback signals, conversational RSs interact with users in natural language. In this work, we develop a comPelling, Precise, Personalized, Preference-relevant language model (P4LM) that recommends items to users while putting emphasis on explaining item characteristics and their relevance. P4LM uses the embedding space representation of a user's preferences to generate compelling responses that are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we develop a joint reward function that measures precision, appeal, and personalization, which we use as AI-based feedback in a reinforcement learning-based language model framework. Using the MovieLens 25M dataset, we demonstrate that P4LM delivers compelling, personalized movie narratives to users.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06176", "tags": []}, {"title": "Predictable Artificial Intelligence", "authors": ["Lexin Zhou", "Pablo A. Moreno-Casares", "Fernando Mart\u00ednez-Plumed", "John Burden", "Ryan Burnell", "Lucy Cheke", "C\u00e8sar Ferri", "Alexandru Marcoci", "Behzad Mehrbakhsh", "Yael Moros-Daval", "Se\u00e1n \u00d3 h\u00c9igeartaigh", "Danaja Rutar", "Wout Schellaert", "Konstantinos Voudouris", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "abstract": "We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06167", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent", "authors": ["Yiren Liu", "Si Chen", "Haocong Cheng", "Mengxia Yu", "Xiao Ran", "Andrew Mo", "Yiliu Tang", "Yun Huang"], "abstract": "Developing novel research questions (RQs) often requires extensive literature reviews, especially for interdisciplinary fields. Leveraging Large Language Models (LLMs), we built an LLM-based agent system, called CoQuest, supporting RQ development through human-AI co-creation. We conducted an experimental design with 20 participants to examine the effect of two interaction designs: breadth-first and depth-first RQ generation. The results showed that participants found the breadth-first approach more creative and trustworthy upon task completion. However, during the task, they rated the RQs generated through the depth-first approach as more creative. We also discovered that AI processing delays allowed users to contemplate multiple RQs simultaneously, resulting in more generated RQs and an increased sense of perceived control. Our work makes both theoretical and practical contributions by proposing and assessing a mental model for human-AI co-creation RQs.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.06155", "tags": []}, {"title": "Auditing Gender Analyzers on Text Data", "authors": ["Siddharth D Jaiswal", "Ankit Kumar Verma", "Animesh Mukherjee"], "abstract": "AI models have become extremely popular and accessible to the general public. However, they are continuously under the scanner due to their demonstrable biases toward various sections of the society like people of color and non-binary people. In this study, we audit three existing gender analyzers -- uClassify, Readable and HackerFactor, for biases against non-binary individuals. These tools are designed to predict only the cisgender binary labels, which leads to discrimination against non-binary members of the society. We curate two datasets -- Reddit comments (660k) and, Tumblr posts (2.05M) and our experimental evaluation shows that the tools are highly inaccurate with the overall accuracy being ~50% on all platforms. Predictions for non-binary comments on all platforms are mostly female, thus propagating the societal bias that non-binary individuals are effeminate. To address this, we fine-tune a BERT multi-label classifier on the two datasets in multiple combinations, observe an overall performance of ~77% on the most realistically deployable setting and a surprisingly higher performance of 90% for the non-binary class. We also audit ChatGPT using zero-shot prompts on a small dataset (due to high pricing) and observe an average accuracy of 58% for Reddit and Tumblr combined (with overall better results for Reddit).\n  Thus, we show that existing systems, including highly advanced ones like ChatGPT are biased, and need better audits and moderation and, that such societal biases can be addressed and alleviated through simple off-the-shelf models like BERT trained on more gender inclusive datasets.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06061", "tags": []}, {"title": "Divide-and-Conquer Dynamics in AI-Driven Disempowerment", "authors": ["Peter S. Park", "Max Tegmark"], "abstract": "AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.\n  Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Second, the movement against AI-driven disempowerment can become more united, and thereby more likely to prevail, if members believe that their efforts will be successful as opposed to futile. Finally, the movement can better unite and prevail if its members are less myopic. Myopic members prioritize their future well-being less than their present well-being, and are thus disinclined to solidarily support current victims today at personal cost, even if this is necessary to counter the shared threat of AI-driven disempowerment.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.06009", "tags": []}, {"title": "What Skills Do You Need When Developing Software Using ChatGPT? (Discussion Paper)", "authors": ["Johan Jeuring", "Roel Groot", "Hieke Keuning"], "abstract": "Since the release of LLM-based tools such as GitHub Copilot and ChatGPT the media and popular scientific literature, but also journals such as the Communications of the ACM, have been flooded with opinions how these tools will change programming. The opinions range from ``machines will program themselves'', to ``AI does not help programmers''. Of course, these statements are meant to to stir up a discussion, and should be taken with a grain of salt, but we argue that such unfounded statements are potentially harmful. Instead, we propose to investigate which skills are required to develop software using LLM-based tools.\n  In this paper we report on an experiment in which we explore if Computational Thinking (CT) skills predict the ability to develop software using LLM-based tools. Our results show that the ability to develop software using LLM-based tools can indeed be predicted by the score on a CT assessment. There are many limitations to our experiment, and this paper is also a call to discuss how to approach, preferably experimentally, the question of which skills are required to develop software using LLM-based tools. We propose to rephrase this question to include by what kind of people/programmers, to develop what kind of software using what kind of LLM-based tools.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05998", "tags": []}, {"title": "A novel Network Science Algorithm for Improving Triage of Patients", "authors": ["Pietro Hiram Guzzi", "Annamaria De Filippo", "Pierangelo Veltri"], "abstract": "Patient triage plays a crucial role in healthcare, ensuring timely and appropriate care based on the urgency of patient conditions. Traditional triage methods heavily rely on human judgment, which can be subjective and prone to errors. Recently, a growing interest has been in leveraging artificial intelligence (AI) to develop algorithms for triaging patients. This paper presents the development of a novel algorithm for triaging patients. It is based on the analysis of patient data to produce decisions regarding their prioritization. The algorithm was trained on a comprehensive data set containing relevant patient information, such as vital signs, symptoms, and medical history. The algorithm was designed to accurately classify patients into triage categories through rigorous preprocessing and feature engineering. Experimental results demonstrate that our algorithm achieved high accuracy and performance, outperforming traditional triage methods. By incorporating computer science into the triage process, healthcare professionals can benefit from improved efficiency, accuracy, and consistency, prioritizing patients effectively and optimizing resource allocation. Although further research is needed to address challenges such as biases in training data and model interpretability, the development of AI-based algorithms for triaging patients shows great promise in enhancing healthcare delivery and patient outcomes.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05996", "tags": ["AI in Healthcare"]}, {"title": "Does Artificial Intelligence benefit UK businesses? An empirical study of the impact of AI on productivity", "authors": ["Sam Hainsworth"], "abstract": "Media hype and technological breakthroughs are fuelling the race to adopt Artificial Intelligence amongst the business community, but is there evidence to suggest this will increase productivity? This paper uses 2015-2019 microdata from the UK Office for National Statistics to identify if the adoption of Artificial Intelligence techniques increases labour productivity in UK businesses. Using fixed effects estimation (Within Group) with a log-linear regression specification the paper concludes that there is no statistically significant impact of AI adoption on labour productivity.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.05985", "tags": []}, {"title": "Vulnerability Clustering and other Machine Learning Applications of Semantic Vulnerability Embeddings", "authors": ["Mark-Oliver Stehr", "Minyoung Kim"], "abstract": "Cyber-security vulnerabilities are usually published in form of short natural language descriptions (e.g., in form of MITRE's CVE list) that over time are further manually enriched with labels such as those defined by the Common Vulnerability Scoring System (CVSS). In the Vulnerability AI (Analytics and Intelligence) project, we investigated different types of semantic vulnerability embeddings based on natural language processing (NLP) techniques to obtain a concise representation of the vulnerability space. We also evaluated their use as a foundation for machine learning applications that can support cyber-security researchers and analysts in risk assessment and other related activities. The particular applications we explored and briefly summarize in this report are clustering, classification, and visualization, as well as a new logic-based approach to evaluate theories about the vulnerability space.", "submitted": "2023-08-23", "link": "https://arxiv.org/pdf/2310.05935", "tags": []}, {"title": "Deep Learning based Tomato Disease Detection and Remedy Suggestions using Mobile Application", "authors": ["Yagya Raj Pandeya", "Samin Karki", "Ishan Dangol", "Nitesh Rajbanshi"], "abstract": "We have developed a comprehensive computer system to assist farmers who practice traditional farming methods and have limited access to agricultural experts for addressing crop diseases. Our system utilizes artificial intelligence (AI) to identify and provide remedies for vegetable diseases. To ensure ease of use, we have created a mobile application that offers a user-friendly interface, allowing farmers to inquire about vegetable diseases and receive suitable solutions in their local language. The developed system can be utilized by any farmer with a basic understanding of a smartphone. Specifically, we have designed an AI-enabled mobile application for identifying and suggesting remedies for vegetable diseases, focusing on tomato diseases to benefit the local farming community in Nepal. Our system employs state-of-the-art object detection methodology, namely You Only Look Once (YOLO), to detect tomato diseases. The detected information is then relayed to the mobile application, which provides remedy suggestions guided by domain experts. In order to train our system effectively, we curated a dataset consisting of ten classes of tomato diseases. We utilized various data augmentation methods to address overfitting and trained a YOLOv5 object detector. The proposed method achieved a mean average precision of 0.76 and offers an efficient mobile interface for interacting with the AI system. While our system is currently in the development phase, we are actively working towards enhancing its robustness and real-time usability by accumulating more training samples.", "submitted": "2023-08-16", "link": "https://arxiv.org/pdf/2310.05929", "tags": ["Cyber Security", "AI in Healthcare"]}, {"title": "SALMON: Self-Alignment with Principle-Following Reward Models", "authors": ["Zhiqing Sun", "Yikang Shen", "Hongxin Zhang", "Qinhong Zhou", "Zhenfang Chen", "David Cox", "Yiming Yang", "Chuang Gan"], "abstract": "Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05910", "tags": ["Natural Language Processing"]}, {"title": "Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts", "authors": ["Lizhang Chen", "Bo Liu", "Kaizhao Liang", "Qiang Liu"], "abstract": "Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.\n  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint $\\|x\\|_\\infty \\leq 1/\u03bb$. Lion achieves this through the incorporation of decoupled weight decay, where $\u03bb$ represents the weight decay coefficient. Our analysis is made possible by the development of a new Lyapunov function for the Lion updates. It applies to a broader family of Lion-$\u03ba$ algorithms, where the $\\text{sign}(\\cdot)$ operator in Lion is replaced by the subgradient of a convex function $\u03ba$, leading to the solution of a general composite optimization problem of $\\min_x f(x) + \u03ba^*(x)$. Our findings provide valuable insights into the dynamics of Lion and pave the way for further improvements and extensions of Lion-related algorithms.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.05898", "tags": []}, {"title": "AI Systems of Concern", "authors": ["Kayla Matteucci", "Shahar Avin", "Fazl Barez", "Se\u00e1n \u00d3 h\u00c9igeartaigh"], "abstract": "Concerns around future dangers from advanced AI often centre on systems hypothesised to have intrinsic characteristics such as agent-like behaviour, strategic awareness, and long-range planning. We label this cluster of characteristics as \"Property X\". Most present AI systems are low in \"Property X\"; however, in the absence of deliberate steering, current research directions may rapidly lead to the emergence of highly capable AI systems that are also high in \"Property X\". We argue that \"Property X\" characteristics are intrinsically dangerous, and when combined with greater capabilities will result in AI systems for which safety and control is difficult to guarantee. Drawing on several scholars' alternative frameworks for possible AI research trajectories, we argue that most of the proposed benefits of advanced AI can be obtained by systems designed to minimise this property. We then propose indicators and governance interventions to identify and limit the development of systems with risky \"Property X\" characteristics.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05876", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "Dynamic value alignment through preference aggregation of multiple objectives", "authors": ["Marcin Korecki", "Damian Dailisan", "Cesare Carissimo"], "abstract": "The development of ethical AI systems is currently geared toward setting objective functions that align with human objectives. However, finding such functions remains a research challenge, while in RL, setting rewards by hand is a fairly standard approach. We present a methodology for dynamic value alignment, where the values that are to be aligned with are dynamically changing, using a multiple-objective approach. We apply this approach to extend Deep $Q$-Learning to accommodate multiple objectives and evaluate this method on a simplified two-leg intersection controlled by a switching agent.Our approach dynamically accommodates the preferences of drivers on the system and achieves better overall performance across three metrics (speeds, stops, and waits) while integrating objectives that have competing or conflicting actions.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05871", "tags": []}, {"title": "Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for Competitive Programming Problems", "authors": ["Juntae Kim", "Eunjung Cho", "Dongwoo Kim", "Dongbin Na"], "abstract": "The recent program development industries have required problem-solving abilities for engineers, especially application developers. However, AI-based education systems to help solve computer algorithm problems have not yet attracted attention, while most big tech companies require the ability to solve algorithm problems including Google, Meta, and Amazon. The most useful guide to solving algorithm problems might be guessing the category (tag) of the facing problems. Therefore, our study addresses the task of predicting the algorithm tag as a useful tool for engineers and developers. Moreover, we also consider predicting the difficulty levels of algorithm problems, which can be used as useful guidance to calculate the required time to solve that problem. In this paper, we present a real-world algorithm problem multi-task dataset, AMT, by mainly collecting problem samples from the most famous and large competitive programming website Codeforces. To the best of our knowledge, our proposed dataset is the most large-scale dataset for predicting algorithm tags compared to previous studies. Moreover, our work is the first to address predicting the difficulty levels of algorithm problems. We present a deep learning-based novel method for simultaneously predicting algorithm tags and the difficulty levels of an algorithm problem given. All datasets and source codes are available at https://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05791", "tags": ["Deep Learning"]}, {"title": "Foundation Models Meet Visualizations: Challenges and Opportunities", "authors": ["Weikai Yang", "Mengchen Liu", "Zheng Wang", "Shixia Liu"], "abstract": "Recent studies have indicated that foundation models, such as BERT and GPT, excel in adapting to a variety of downstream tasks. This adaptability has established them as the dominant force in building artificial intelligence (AI) systems. As visualization techniques intersect with these models, a new research paradigm emerges. This paper divides these intersections into two main areas: visualizations for foundation models (VIS4FM) and foundation models for visualizations (FM4VIS). In VIS4FM, we explore the primary role of visualizations in understanding, refining, and evaluating these intricate models. This addresses the pressing need for transparency, explainability, fairness, and robustness. Conversely, within FM4VIS, we highlight how foundation models can be utilized to advance the visualization field itself. The confluence of foundation models and visualizations holds great promise, but it also comes with its own set of challenges. By highlighting these challenges and the growing opportunities, this paper seeks to provide a starting point for continued exploration in this promising avenue.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05771", "tags": []}, {"title": "A Review of the Ethics of Artificial Intelligence and its Applications in the United States", "authors": ["Esther Taiwo", "Ahmed Akinsola", "Edward Tella", "Kolade Makinde", "Mayowa Akinwande"], "abstract": "This study is focused on the ethics of Artificial Intelligence and its application in the United States, the paper highlights the impact AI has in every sector of the US economy and multiple facets of the technological space and the resultant effect on entities spanning businesses, government, academia, and civil society. There is a need for ethical considerations as these entities are beginning to depend on AI for delivering various crucial tasks, which immensely influence their operations, decision-making, and interactions with each other. The adoption of ethical principles, guidelines, and standards of work is therefore required throughout the entire process of AI development, deployment, and usage to ensure responsible and ethical AI practices. Our discussion explores eleven fundamental 'ethical principles' structured as overarching themes. These encompass Transparency, Justice, Fairness, Equity, Non- Maleficence, Responsibility, Accountability, Privacy, Beneficence, Freedom, Autonomy, Trust, Dignity, Sustainability, and Solidarity. These principles collectively serve as a guiding framework, directing the ethical path for the responsible development, deployment, and utilization of artificial intelligence (AI) technologies across diverse sectors and entities within the United States. The paper also discusses the revolutionary impact of AI applications, such as Machine Learning, and explores various approaches used to implement AI ethics. This examination is crucial to address the growing concerns surrounding the inherent risks associated with the widespread use of artificial intelligence.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05751", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy"]}, {"title": "A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics", "authors": ["Kai He", "Rui Mao", "Qika Lin", "Yucheng Ruan", "Xiang Lan", "Mengling Feng", "Erik Cambria"], "abstract": "The utilization of large language models (LLMs) in the Healthcare domain has generated both excitement and concern due to their ability to effectively respond to freetext queries with certain professional knowledge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other. Then we summarize related Healthcare training data, training methods, optimization strategies, and usage. Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty. Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05694", "tags": ["AI in Healthcare"]}, {"title": "Automated Argument Generation from Legal Facts", "authors": ["Oscar Tuvey", "Procheta Sen"], "abstract": "The count of pending cases has shown an exponential rise across nations (e.g., with more than 10 million pending cases in India alone). The main issue lies in the fact that the number of cases submitted to the law system is far greater than the available number of legal professionals present in a country. Given this worldwide context, the utilization of AI technology has gained paramount importance to enhance the efficiency and speed of legal procedures. In this study we partcularly focus on helping legal professionals in the process of analyzing a legal case. Our specific investigation delves into harnessing the generative capabilities of open-sourced large language models to create arguments derived from the facts present in legal cases. Experimental results show that the generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.05680", "tags": []}, {"title": "CLAID: Closing the Loop on AI & Data Collection -- A Cross-Platform Transparent Computing Middleware Framework for Smart Edge-Cloud and Digital Biomarker Applications", "authors": ["Patrick Langer", "Elgar Fleisch", "Filipe Barata"], "abstract": "The increasing number of edge devices with enhanced sensing capabilities, such as smartphones, wearables, and IoT devices equipped with sensors, holds the potential for innovative smart-edge applications in healthcare. These devices generate vast amounts of multimodal data, enabling the implementation of digital biomarkers which can be leveraged by machine learning solutions to derive insights, predict health risks, and allow personalized interventions. Training these models requires collecting data from edge devices and aggregating it in the cloud. To validate and verify those models, it is essential to utilize them in real-world scenarios and subject them to testing using data from diverse cohorts. Since some models are too computationally expensive to be run on edge devices directly, a collaborative framework between the edge and cloud becomes necessary. In this paper, we present CLAID, an open-source cross-platform middleware framework based on transparent computing compatible with Android, iOS, WearOS, Linux, macOS, and Windows. CLAID enables logical integration of devices running different operating systems into an edge-cloud system, facilitating communication and offloading between them, with bindings available in different programming languages. We provide Modules for data collection from various sensors as well as for the deployment of machine-learning models. Furthermore, we propose a novel methodology, \"ML-Model in the Loop\" for verifying deployed machine learning models, which helps to analyze problems that may occur during the migration of models from cloud to edge devices. We verify our framework in three different experiments and achieve 100% sampling coverage for data collection across different sensors as well as an equal performance of a cough detection model deployed on both Android and iOS devices. We evaluate the memory and battery consumption of our framework.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05643", "tags": ["Large scale Machine Learning", "AI and Privacy", "AI in Healthcare"]}, {"title": "EdgeAISim: A Toolkit for Simulation and Modelling of AI Models in Edge Computing Environments", "authors": ["Aadharsh Roshan Nandhakumar", "Ayush Baranwal", "Priyanshukumar Choudhary", "Muhammed Golec", "Sukhpal Singh Gill"], "abstract": "To meet next-generation IoT application demands, edge computing moves processing power and storage closer to the network edge to minimise latency and bandwidth utilisation. Edge computing is becoming popular as a result of these benefits, but resource management is still challenging. Researchers are utilising AI models to solve the challenge of resource management in edge computing systems. However, existing simulation tools are only concerned with typical resource management policies, not the adoption and implementation of AI models for resource management, especially. Consequently, researchers continue to face significant challenges, making it hard and time-consuming to use AI models when designing novel resource management policies for edge computing with existing simulation tools. To overcome these issues, we propose a lightweight Python-based toolkit called EdgeAISim for the simulation and modelling of AI models for designing resource management policies in edge computing environments. In EdgeAISim, we extended the basic components of the EdgeSimPy framework and developed new AI-based simulation models for task scheduling, energy management, service migration, network flow scheduling, and mobility support for edge computing environments. In EdgeAISim, we have utilised advanced AI models such as Multi-Armed Bandit with Upper Confidence Bound, Deep Q-Networks, Deep Q-Networks with Graphical Neural Network, and ActorCritic Network to optimize power usage while efficiently managing task migration within the edge computing environment. The performance of these proposed models of EdgeAISim is compared with the baseline, which uses a worst-fit algorithm-based resource management policy in different settings. Experimental results indicate that EdgeAISim exhibits a substantial reduction in power consumption, highlighting the compelling success of power optimization strategies in EdgeAISim.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05605", "tags": []}, {"title": "Decoding the Threat Landscape : ChatGPT, FraudGPT, and WormGPT in Social Engineering Attacks", "authors": ["Polra Victor Falade"], "abstract": "In the ever-evolving realm of cybersecurity, the rise of generative AI models like ChatGPT, FraudGPT, and WormGPT has introduced both innovative solutions and unprecedented challenges. This research delves into the multifaceted applications of generative AI in social engineering attacks, offering insights into the evolving threat landscape using the blog mining technique. Generative AI models have revolutionized the field of cyberattacks, empowering malicious actors to craft convincing and personalized phishing lures, manipulate public opinion through deepfakes, and exploit human cognitive biases. These models, ChatGPT, FraudGPT, and WormGPT, have augmented existing threats and ushered in new dimensions of risk. From phishing campaigns that mimic trusted organizations to deepfake technology impersonating authoritative figures, we explore how generative AI amplifies the arsenal of cybercriminals. Furthermore, we shed light on the vulnerabilities that AI-driven social engineering exploits, including psychological manipulation, targeted phishing, and the crisis of authenticity. To counter these threats, we outline a range of strategies, including traditional security measures, AI-powered security solutions, and collaborative approaches in cybersecurity. We emphasize the importance of staying vigilant, fostering awareness, and strengthening regulations in the battle against AI-enhanced social engineering attacks. In an environment characterized by the rapid evolution of AI models and a lack of training data, defending against generative AI threats requires constant adaptation and the collective efforts of individuals, organizations, and governments. This research seeks to provide a comprehensive understanding of the dynamic interplay between generative AI and social engineering attacks, equipping stakeholders with the knowledge to navigate this intricate cybersecurity landscape.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05595", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "STREAM: Social data and knowledge collective intelligence platform for TRaining Ethical AI Models", "authors": ["Yuwei Wang", "Enmeng Lu", "Zizhe Ruan", "Yao Liang", "Yi Zeng"], "abstract": "This paper presents Social data and knowledge collective intelligence platform for TRaining Ethical AI Models (STREAM) to address the challenge of aligning AI models with human moral values, and to provide ethics datasets and knowledge bases to help promote AI models \"follow good advice as naturally as a stream follows its course\". By creating a comprehensive and representative platform that accurately mirrors the moral judgments of diverse groups including humans and AIs, we hope to effectively portray cultural and group variations, and capture the dynamic evolution of moral judgments over time, which in turn will facilitate the Establishment, Evaluation, Embedding, Embodiment, Ensemble, and Evolvement (6Es) of the moral capabilities of AI models. Currently, STREAM has already furnished a comprehensive collection of ethical scenarios, and amassed substantial moral judgment data annotated by volunteers and various popular Large Language Models (LLMs), collectively portraying the moral preferences and performances of both humans and AIs across a range of moral contexts. This paper will outline the current structure and construction of STREAM, explore its potential applications, and discuss its future prospects.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05563", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions", "authors": ["Jiyuan Wang", "Chunyu Lin", "Lang Nie", "Shujun Huang", "Yao Zhao", "Xing Pan", "Rui Ai"], "abstract": "Depth estimation models have shown promising performance on clear scenes but fail to generalize to adverse weather conditions due to illumination variations, weather particles, etc. In this paper, we propose WeatherDepth, a self-supervised robust depth estimation model with curriculum contrastive learning, to tackle performance degradation in complex weather conditions. Concretely, we first present a progressive curriculum learning scheme with three simple-to-complex curricula to gradually adapt the model from clear to relative adverse, and then to adverse weather scenes. It encourages the model to gradually grasp beneficial depth cues against the weather effect, yielding smoother and better domain adaption. Meanwhile, to prevent the model from forgetting previous curricula, we integrate contrastive learning into different curricula. Drawn the reference knowledge from the previous course, our strategy establishes a depth consistency constraint between different courses towards robust depth estimation in diverse weather. Besides, to reduce manual intervention and better adapt to different models, we designed an adaptive curriculum scheduler to automatically search for the best timing for course switching. In the experiment, the proposed solution is proven to be easily incorporated into various architectures and demonstrates state-of-the-art (SoTA) performance on both synthetic and real weather datasets.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05556", "tags": ["Computer Vision", "Natural Language Processing"]}, {"title": "Regulation and NLP (RegNLP): Taming Large Language Models", "authors": ["Catalina Goanta", "Nikolaos Aletras", "Ilias Chalkidis", "Sofia Ranchordas", "Gerasimos Spanakis"], "abstract": "The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying political agendas on AI regulation and governance and posing issues of regulatory capture. Capture occurs when the regulator advances the interests of the industry it is supposed to regulate, or of special interest groups rather than pursuing the general public interest. Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms. This often happens without systematic methodologies or sufficient rooting in the disciplines that inspire an extended scope of NLP research, jeopardizing the scientific integrity of these endeavors. Regulation studies are a rich source of knowledge on how to systematically deal with risk and uncertainty, as well as with scientific evidence, to evaluate and compare regulatory options. This resource has largely remained untapped so far. In this paper, we argue how NLP research on these topics can benefit from proximity to regulatory studies and adjacent fields. We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing with risk assessment. Finally, we advocate for the development of a new multidisciplinary research space on regulation and NLP (RegNLP), focused on connecting scientific knowledge to regulatory processes based on systematic methodologies.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05553", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "Ethics of Artificial Intelligence and Robotics in the Architecture, Engineering, and Construction Industry", "authors": ["Ci-Jyun Liang", "Thai-Hoa Le", "Youngjib Ham", "Bharadwaj R. K. Mantha", "Marvin H. Cheng", "Jacob J. Lin"], "abstract": "Artificial intelligence (AI) and robotics research and implementation emerged in the architecture, engineering, and construction (AEC) industry to positively impact project efficiency and effectiveness concerns such as safety, productivity, and quality. This shift, however, warrants the need for ethical considerations of AI and robotics adoption due to its potential negative impacts on aspects such as job security, safety, and privacy. Nevertheless, this did not receive sufficient attention, particularly within the academic community. This research systematically reviews AI and robotics research through the lens of ethics in the AEC community for the past five years. It identifies nine key ethical issues namely job loss, data privacy, data security, data transparency, decision-making conflict, acceptance and trust, reliability and safety, fear of surveillance, and liability, by summarizing existing literature and filtering it further based on its AEC relevance. Furthermore, thirteen research topics along the process were identified based on existing AEC studies that had direct relevance to the theme of ethics in general and their parallels are further discussed. Finally, the current challenges and knowledge gaps are discussed and seven specific future research directions are recommended. This study not only signifies more stakeholder awareness of this important topic but also provides imminent steps towards safer and more efficient realization.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05414", "tags": ["AI and Privacy", "Ethical AI and Bias Mitigation"]}, {"title": "Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training", "authors": ["Michael Benington", "Leo Phan", "Chris Pierre Paul", "Evan Shoemaker", "Priyanka Ranade", "Torstein Collett", "Grant Hodgson Perez", "Christopher Krieger"], "abstract": "AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in size, computational requirements and memory demands for AI also continue to grow. These challenges have inspired the development of distributed algorithm and circuit-based optimization techniques that enable the ability to progressively scale models in multi-node environments, efficiently minimize neural network cost functions for faster convergence, and store more parameters into a set number of available resources. In our research project, we focus on parallel and distributed machine learning algorithm development, specifically for optimizing the data processing and pre-training of a set of 5 encoder-decoder LLMs, ranging from 580 million parameters to 13 billion parameters. We performed a fine-grained study to quantify the relationships between three ML parallelism methods, specifically exploring Microsoft DeepSpeed Zero Redundancy Optimizer (ZeRO) stages.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.05350", "tags": ["Large scale Machine Learning"]}, {"title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF", "authors": ["Yi Dong", "Zhilin Wang", "Makesh Narsimhan Sreedhar", "Xianchao Wu", "Oleksii Kuchaiev"], "abstract": "Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05344", "tags": ["Reinforcement Learning", "Natural Language Processing", "Large scale Machine Learning"]}, {"title": "Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation", "authors": ["Xunxin Cai", "Meng Xiao", "Zhiyuan Ning", "Yuanchun Zhou"], "abstract": "In addressing the imbalanced issue of data within the realm of Natural Language Processing, text data augmentation methods have emerged as pivotal solutions. This data imbalance is prevalent in the research proposals submitted during the funding application process. Such imbalances, resulting from the varying popularity of disciplines or the emergence of interdisciplinary studies, significantly impede the precision of downstream topic models that deduce the affiliated disciplines of these proposals. At the data level, proposals penned by experts and scientists are inherently complex technological texts, replete with intricate terminologies, which augmenting such specialized text data poses unique challenges. At the system level, this, in turn, compromises the fairness of AI-assisted reviewer assignment systems, which raises a spotlight on solving this issue. This study leverages large language models (Llama V1) as data generators to augment research proposals categorized within intricate disciplinary hierarchies, aiming to rectify data imbalances and enhance the equity of expert assignments. We first sample within the hierarchical structure to find the under-represented class. Then we designed a prompt for keyword-based research proposal generation. Our experiments attests to the efficacy of the generated data, demonstrating that research proposals produced using the prompts can effectively address the aforementioned issues and generate high quality scientific text data, thus help the model overcome the imbalanced issue.", "submitted": "2023-10-14", "link": "https://arxiv.org/pdf/2310.05318", "tags": ["Natural Language Processing", "AI and Privacy"]}, {"title": "Accelerating Deep Neural Network guided MCTS using Adaptive Parallelism", "authors": ["Yuan Meng", "Qian Wang", "Tianxin Zu", "Viktor Prasanna"], "abstract": "Deep Neural Network guided Monte-Carlo Tree Search (DNN-MCTS) is a powerful class of AI algorithms. In DNN-MCTS, a Deep Neural Network model is trained collaboratively with a dynamic Monte-Carlo search tree to guide the agent towards actions that yields the highest returns. While the DNN operations are highly parallelizable, the search tree operations involved in MCTS are sequential and often become the system bottleneck. Existing MCTS parallel schemes on shared-memory multi-core CPU platforms either exploit data parallelism but sacrifice memory access latency, or take advantage of local cache for low-latency memory accesses but constrain the tree search to a single thread. In this work, we analyze the tradeoff of these parallel schemes and develop performance models for both parallel schemes based on the application and hardware parameters. We propose a novel implementation that addresses the tradeoff by adaptively choosing the optimal parallel scheme for the MCTS component on the CPU. Furthermore, we propose an efficient method for searching the optimal communication batch size as the MCTS component on the CPU interfaces with DNN operations offloaded to an accelerator (GPU). Using a representative DNN-MCTS algorithm - Alphazero on board game benchmarks, we show that the parallel framework is able to adaptively generate the best-performing parallel implementation, leading to a range of $1.5\\times - 3\\times$ speedup compared with the baseline methods on CPU and CPU-GPU platforms.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05313", "tags": ["Deep Learning"]}, {"title": "Quality Assurance of A GPT-based Sentiment Analysis System: Adversarial Review Data Generation and Detection", "authors": ["Tinghui Ouyang", "Hoang-Quoc Nguyen-Son", "Huy H. Nguyen", "Isao Echizen", "Yoshiki Seo"], "abstract": "Large Language Models (LLMs) have been garnering significant attention of AI researchers, especially following the widespread popularity of ChatGPT. However, due to LLMs' intricate architecture and vast parameters, several concerns and challenges regarding their quality assurance require to be addressed. In this paper, a fine-tuned GPT-based sentiment analysis model is first constructed and studied as the reference in AI quality analysis. Then, the quality analysis related to data adequacy is implemented, including employing the content-based approach to generate reasonable adversarial review comments as the wrongly-annotated data, and developing surprise adequacy (SA)-based techniques to detect these abnormal data. Experiments based on Amazon.com review data and a fine-tuned GPT model were implemented. Results were thoroughly discussed from the perspective of AI quality assurance to present the quality analysis of an LLM model on generated adversarial textual data and the effectiveness of using SA on anomaly detection in data quality assurance.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05312", "tags": ["AI and Privacy"]}, {"title": "HypoCompass: Large-Language-Model-based Tutor for Hypothesis Construction in Debugging for Novices", "authors": ["Qianou Ma", "Hua Shen", "Kenneth Koedinger", "Tongshuang Wu"], "abstract": "With the prevalence of imperfect but capable LLMs in software development, it becomes increasingly important for developers to cultivate debugging skills -- to form hypotheses about the source of error in both their own codes and codes produced by their AI pair programmers. Despite the necessity, hypothesis construction in debugging is rarely taught due to a lack of explicit instruction. In this work, we explore whether LLMs can be used to train novices on hypothesis construction, by designing a theoretically motivated, LLM-augmented tutor -- HypoCompass. HypoCompass relies on LLMs for generating rich training materials guided by learning principles and presents them in a learning-by-teaching environment, where LLMs act as students who write bugs and attempt to fix them, and human novices focus on debugging in the role of a Teaching Assistant. Evaluations show that HypoCompass consistently generates high-quality training materials, and brings significant learning gain: In a pre-to-post test setup, 10 novices improved their performances by 17%, with a reduced completion time of 13%.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05292", "tags": []}, {"title": "Generalizable Error Modeling for Search Relevance Data Annotation Tasks", "authors": ["Heinrich Peters", "Alireza Hashemi", "James Rae"], "abstract": "Human data annotation is critical in shaping the quality of machine learning (ML) and artificial intelligence (AI) systems. One significant challenge in this context is posed by annotation errors, as their effects can degrade the performance of ML models. This paper presents a predictive error model trained to detect potential errors in search relevance annotation tasks for three industry-scale ML applications (music streaming, video streaming, and mobile apps) and assesses its potential to enhance the quality and efficiency of the data annotation process. Drawing on real-world data from an extensive search relevance annotation program, we illustrate that errors can be predicted with moderate model performance (AUC=0.65-0.75) and that model performance generalizes well across applications (i.e., a global, task-agnostic model performs on par with task-specific models). We present model explainability analyses to identify which types of features are the main drivers of predictive performance. Additionally, we demonstrate the usefulness of the model in the context of auditing, where prioritizing tasks with high predicted error probabilities considerably increases the amount of corrected annotation errors (e.g., 40% efficiency gains for the music streaming application). These results underscore that automated error detection models can yield considerable improvements in the efficiency and quality of data annotation processes. Thus, our findings reveal critical insights into effective error management in the data annotation process, thereby contributing to the broader field of human-in-the-loop ML.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05286", "tags": ["Large scale Machine Learning", "AI in Healthcare"]}, {"title": "Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using a Novel Distributed Denoising CNN (DDCNN)", "authors": ["Sankar B.", "Mukil Saravanan", "Kalaivanan Kumar", "Siri Dubbaka"], "abstract": "Art restoration is crucial for preserving cultural heritage, but traditional methods have limitations in faithfully reproducing original artworks while addressing issues like fading, staining, and damage. We present an innovative approach using deep learning, specifically Convolutional Neural Networks (CNNs), and Computer Vision techniques to revolutionize art restoration. We start by creating a diverse dataset of deteriorated art images with various distortions and degradation levels. This dataset trains a Distributed Denoising CNN (DDCNN) to remove distortions while preserving intricate details. Our method is adaptable to different distortion types and levels, making it suitable for various deteriorated artworks, including paintings, sketches, and photographs. Extensive experiments demonstrate our approach's efficiency and effectiveness compared to other Denoising CNN models. We achieve a substantial reduction in distortion, transforming deteriorated artworks into masterpieces. Quantitative evaluations confirm our method's superiority over traditional techniques, reshaping the art restoration field and preserving cultural heritage. In summary, our paper introduces an AI-powered solution that combines Computer Vision and deep learning with DDCNN to restore artworks accurately, overcoming limitations and paving the way for future advancements in art restoration.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05270", "tags": []}, {"title": "ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data", "authors": ["Tianyang Zhong", "Wei Zhao", "Yutong Zhang", "Yi Pan", "Peixin Dong", "Zuowei Jiang", "Xiaoyan Kui", "Youlan Shang", "Li Yang", "Yaonai Wei", "Longtao Yang", "Hao Chen", "Huan Zhao", "Yuxiao Liu", "Ning Zhu", "Yiwei Li", "Yisong Wang", "Jiaqi Yao", "Jiaqi Wang", "Ying Zeng", "Lei He", "Chao Zheng", "Zhixue Zhang", "Ming Li", "Zhengliang Liu", "et al. (17 additional authors not shown)"], "abstract": "Radiology report generation, as a key step in medical image analysis, is critical to the quantitative analysis of clinically informed decision-making levels. However, complex and diverse radiology reports with cross-source heterogeneity pose a huge generalizability challenge to the current methods under massive data volume, mainly because the style and normativity of radiology reports are obviously distinctive among institutions, body regions inspected and radiologists. Recently, the advent of large language models (LLM) offers great potential for recognizing signs of health conditions. To resolve the above problem, we collaborate with the Second Xiangya Hospital in China and propose ChatRadio-Valuer based on the LLM, a tailored model for automatic radiology report generation that learns generalizable representations and provides a basis pattern for model adaptation in sophisticated analysts' cases. Specifically, ChatRadio-Valuer is trained based on the radiology reports from a single institution by means of supervised fine-tuning, and then adapted to disease diagnosis tasks for human multi-system evaluation (i.e., chest, abdomen, muscle-skeleton, head, and maxillofacial $\\&$ neck) from six different institutions in clinical-level events. The clinical dataset utilized in this study encompasses a remarkable total of \\textbf{332,673} observations. From the comprehensive results on engineering indicators, clinical efficacy and deployment cost metrics, it can be shown that ChatRadio-Valuer consistently outperforms state-of-the-art models, especially ChatGPT (GPT-3.5-Turbo) and GPT-4 et al., in terms of the diseases diagnosis from radiology reports. ChatRadio-Valuer provides an effective avenue to boost model generalization performance and alleviate the annotation workload of experts to enable the promotion of clinical AI applications in radiology reports.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05242", "tags": ["AI in Healthcare", "Natural Language Processing"]}, {"title": "Accelerating Machine Learning Primitives on Commodity Hardware", "authors": ["Roman Snytsar"], "abstract": "Sliding Window Sum algorithms have been successfully used for training and inference of Deep Neural Networks. We have shown before how both pooling and convolution 1-D primitives could be expressed as sliding sums and evaluated by the compute kernels with a shared structure. In this paper, we present an extensive study of the Sliding Window convolution technique as a more efficient alternative to the commonly used General Matrix Multiplication (GEMM) based convolution in Deep Neural Networks (DNNs). The Sliding Window technique addresses the memory bloating problem and demonstrates a significant speedup in 2-D convolution. We explore the performance of this technique on a range of implementations, including custom kernels for specific filter sizes. Our results suggest that the Sliding Window computation kernels can outperform GEMM-based convolution on a CPU and even on dedicated hardware accelerators. This could promote a wider adoption of AI on low-power and low-memory devices without the need for specialized hardware. We also discuss the compatibility of model compression methods and optimized network architectures with the Sliding Window technique, encouraging further research in these areas.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05218", "tags": ["Deep Learning"]}, {"title": "Factuality Challenges in the Era of Large Language Models", "authors": ["Isabelle Augenstein", "Timothy Baldwin", "Meeyoung Cha", "Tanmoy Chakraborty", "Giovanni Luca Ciampaglia", "David Corney", "Renee DiResta", "Emilio Ferrara", "Scott Hale", "Alon Halevy", "Eduard Hovy", "Heng Ji", "Filippo Menczer", "Ruben Miguez", "Preslav Nakov", "Dietram Scheufele", "Shivam Sharma", "Giovanni Zagni"], "abstract": "The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as \"hallucinations.\" Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to shed light on navigating various aspects of veracity in the era of generative AI.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.05189", "tags": []}, {"title": "Evolutionary Retrosynthetic Route Planning", "authors": ["Yan Zhang", "Hao Hao", "Xiao He", "Shuanhu Gao", "Aimin Zhou"], "abstract": "Molecular retrosynthesis is a significant and complex problem in the field of chemistry, however, traditional manual synthesis methods not only need well-trained experts but also are time-consuming. With the development of big data and machine learning, artificial intelligence (AI) based retrosynthesis is attracting more attention and is becoming a valuable tool for molecular retrosynthesis. At present, Monte Carlo tree search is a mainstream search framework employed to address this problem. Nevertheless, its search efficiency is compromised by its large search space. Therefore, we propose a novel approach for retrosynthetic route planning based on evolutionary optimization, marking the first use of Evolutionary Algorithm (EA) in the field of multi-step retrosynthesis. The proposed method involves modeling the retrosynthetic problem into an optimization problem, defining the search space and operators. Additionally, to improve the search efficiency, a parallel strategy is implemented. The new approach is applied to four case products, and is compared with Monte Carlo tree search. The experimental results show that, in comparison to the Monte Carlo tree search algorithm, EA significantly reduces the number of calling single-step model by an average of 53.9%. The time required to search three solutions decreased by an average of 83.9%, and the number of feasible search routes increases by 5 times.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05186", "tags": ["Deep Learning", "Large scale Machine Learning"]}, {"title": "Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements", "authors": ["Yushan Qian", "Wei-Nan Zhang", "Ting Liu"], "abstract": "Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs) in this field has attracted great attention. This work empirically investigates the performance of LLMs in generating empathetic responses and proposes three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base. Extensive experiments show that LLMs can significantly benefit from our proposed methods and is able to achieve state-of-the-art performance in both automatic and human evaluations. Additionally, we explore the possibility of GPT-4 simulating human evaluators.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05140", "tags": []}, {"title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature", "authors": ["Guangsheng Bao", "Yanbin Zhao", "Zhiyang Teng", "Linyi Yang", "Yue Zhang"], "abstract": "Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present Fast-DetectGPT, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only outperforms DetectGPT in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05130", "tags": []}, {"title": "How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts", "authors": ["Tharindu Kumarage", "Paras Sheth", "Raha Moraffah", "Joshua Garland", "Huan Liu"], "abstract": "In recent years, there has been a rapid proliferation of AI-generated text, primarily driven by the release of powerful pre-trained language models (PLMs). To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI detector and the Stanford DetectGPT. In our study, we ask how reliable these detectors are. We answer the question by designing a novel approach that can prompt any PLM to generate text that evades these high-performing detectors. The proposed approach suggests a universal evasive prompt, a novel type of soft prompt, which guides PLMs in producing \"human-like\" text that can mislead the detectors. The novel universal evasive prompt is achieved in two steps: First, we create an evasive soft prompt tailored to a specific PLM through prompt tuning; and then, we leverage the transferability of soft prompts to transfer the learned evasive soft prompt from one PLM to another. Employing multiple PLMs in various writing tasks, we conduct extensive experiments to evaluate the efficacy of the evasive soft prompts in their evasion of state-of-the-art detectors.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05095", "tags": []}, {"title": "FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis", "authors": ["Raman Dutt", "Ondrej Bohdal", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "abstract": "Training models with robust group fairness properties is crucial in ethically sensitive application areas such as medical diagnosis. Despite the growing body of work aiming to minimise demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalisation gap: High-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalisation performance differs across subgroups. This motivates us to take a bi-level optimisation perspective on fair learning: Optimising the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between updating more parameters, enabling a better fit to the task of interest vs. fewer parameters, potentially reducing the generalisation gap. To manage this tradeoff, we propose FairTune, a framework to optimise the choice of PEFT parameters with respect to fairness. We demonstrate empirically that FairTune leads to improved fairness on a range of medical imaging datasets.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05055", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think -- Introducing AI Detectability Index", "authors": ["Megha Chakraborty", "S. M Towhidul Islam Tonmoy", "S M Mehedi Zaman", "Krish Sharma", "Niyar R Barman", "Chandan Gupta", "Shreya Gautam", "Tanay Kumar", "Vinija Jain", "Aman Chadha", "Amit P. Sheth", "Amitava Das"], "abstract": "With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that 'If a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it'. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a higher ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.05030", "tags": ["Ethical AI and Bias Mitigation", "AI in Education"]}, {"title": "Unmasking Biases and Navigating Pitfalls in the Ophthalmic Artificial Intelligence Lifecycle: A Review", "authors": ["Luis Filipe Nakayama", "Jo\u00e3o Matos", "Justin Quion", "Frederico Novaes", "William Greig Mitchell", "Rogers Mwavu", "Ju-Yi Ji Hung", "Alvina Pauline dy Santiago", "Warachaya Phanphruk", "Jaime S. Cardoso", "Leo Anthony Celi"], "abstract": "Over the past two decades, exponential growth in data availability, computational power, and newly available modeling techniques has led to an expansion in interest, investment, and research in Artificial Intelligence (AI) applications. Ophthalmology is one of many fields that seek to benefit from AI given the advent of telemedicine screening programs and the use of ancillary imaging. However, before AI can be widely deployed, further work must be done to avoid the pitfalls within the AI lifecycle. This review article breaks down the AI lifecycle into seven steps: data collection; defining the model task; data pre-processing and labeling; model development; model evaluation and validation; deployment; and finally, post-deployment evaluation, monitoring, and system recalibration and delves into the risks for harm at each step and strategies for mitigating them.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04997", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Healthcare"]}, {"title": "VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for Generalist Ophthalmic Artificial Intelligence", "authors": ["Jianing Qiu", "Jian Wu", "Hao Wei", "Peilun Shi", "Minqing Zhang", "Yunyun Sun", "Lin Li", "Hanruo Liu", "Hongyi Liu", "Simeng Hou", "Yuyang Zhao", "Xuehui Shi", "Junfang Xian", "Xiaoxia Qu", "Sirui Zhu", "Lijie Pan", "Xiaoniao Chen", "Xiaojia Zhang", "Shuai Jiang", "Kebing Wang", "Chenlong Yang", "Mingqiang Chen", "Sujie Fan", "Jianhua Hu", "Aiguo Lv", "et al. (17 additional authors not shown)"], "abstract": "We present VisionFM, a foundation model pre-trained with 3.4 million ophthalmic images from 560,457 individuals, covering a broad range of ophthalmic diseases, modalities, imaging devices, and demography. After pre-training, VisionFM provides a foundation to foster multiple ophthalmic artificial intelligence (AI) applications, such as disease screening and diagnosis, disease prognosis, subclassification of disease phenotype, and systemic biomarker and disease prediction, with each application enhanced with expert-level intelligence and accuracy. The generalist intelligence of VisionFM outperformed ophthalmologists with basic and intermediate levels in jointly diagnosing 12 common ophthalmic diseases. Evaluated on a new large-scale ophthalmic disease diagnosis benchmark database, as well as a new large-scale segmentation and detection benchmark database, VisionFM outperformed strong baseline deep neural networks. The ophthalmic image representations learned by VisionFM exhibited noteworthy explainability, and demonstrated strong generalizability to new ophthalmic modalities, disease spectrum, and imaging devices. As a foundation model, VisionFM has a large capacity to learn from diverse ophthalmic imaging data and disparate datasets. To be commensurate with this capacity, in addition to the real data used for pre-training, we also generated and leveraged synthetic ophthalmic imaging data. Experimental results revealed that synthetic data that passed visual Turing tests, can also enhance the representation learning capability of VisionFM, leading to substantial performance gains on downstream ophthalmic AI tasks. Beyond the ophthalmic AI applications developed, validated, and demonstrated in this work, substantial further applications can be achieved in an efficient and cost-effective manner using VisionFM as the foundation.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04992", "tags": ["AI and Privacy", "Large scale Machine Learning", "AI in Healthcare"]}, {"title": "The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations", "authors": ["Vipula Rawte", "Swagata Chakraborty", "Agnibh Pathak", "Anubhav Sarkar", "S. M Towhidul Islam Tonmoy", "Aman Chadha", "Amit P. Sheth", "Amitava Das"], "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04988", "tags": []}, {"title": "Data-centric Graph Learning: A Survey", "authors": ["Cheng Yang", "Deyu Bo", "Jixi Liu", "Yufei Peng", "Boyu Chen", "Haoran Dai", "Ao Sun", "Yue Yu", "Yixin Xiao", "Qi Zhang", "Chunchen Wang", "Yuxin Guo", "Chuan Shi"], "abstract": "The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer two crucial questions: (1) when to modify graph data and (2) how to modify graph data to unlock the potential of various graph models. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the processing methods for different data structures in the graph data, i.e., topology, feature and label. Furthermore, we analyze some potential problems embedded in graph data and discuss how to solve them in a data-centric manner. Finally, we provide some promising future directions for data-centric graph learning.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04987", "tags": ["AI and Privacy"]}, {"title": "A new economic and financial theory of money", "authors": ["Michael E. Glinsky", "Sharon Sievert"], "abstract": "This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time scale models that capture the true risk. The decision-making will be approached from the perspective of true systems control based on a system response function given by the multi scale risk model and system controllers that utilize the Deep Reinforcement Learning, Generative Pretrained Transformers, and other methods of Artificial Intelligence (DRL/GPT/AI). Finally, the sub-economy will be viewed as a nonlinear complex physical system with both stable equilibriums that are associated with short-term exploitation, and unstable equilibriums that need to be stabilized with active nonlinear control based on the multi scale system response functions and DRL/GPT/AI.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.04986", "tags": []}, {"title": "MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks", "authors": ["Jingyuan Qi", "Minqian Liu", "Ying Shen", "Zhiyang Xu", "Lifu Huang"], "abstract": "Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for the subsequent step, respectively. Built from WikiHow, MultiScript covers multimodal scripts in videos and text descriptions for over 6,655 human everyday tasks across 19 diverse domains. To establish baseline performance on MultiScript, we propose two knowledge-guided multimodal generative frameworks that incorporate the task-related knowledge prompted from large language models such as Vicuna. Experimental results show that our proposed approaches significantly improve over the competitive baselines.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04965", "tags": []}, {"title": "LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation", "authors": ["Christian Munley", "Aaron Jarmusch", "Sunita Chandrasekaran"], "abstract": "Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. In this paper, we explore the capabilitity of state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and open-source alternatives like Meta AI Codellama, to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based programming paradigm, OpenACC. Our approach entails exploring various prompt engineering techniques including a code template, retrieval-augmented generation (RAG) with code template, expressive prompt using RAG with code template, one-shot example, and RAG with one-shot example. This paper focusses on (a) exploring the capabilities of the latest LLMs for code generation, (b) investigating prompt and fine tuning methods, and (c) analyzing the outcome of LLMs generated tests", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04963", "tags": []}, {"title": "Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks", "authors": ["Jiazhi Li", "Mahyar Khayatkhoei", "Jiageng Zhu", "Hanchen Xie", "Mohamed E. Hussein", "Wael AbdAlmageed"], "abstract": "Ensuring a neural network is not relying on protected attributes (e.g., race, sex, age) for predictions is crucial in advancing fair and trustworthy AI. While several promising methods for removing attribute bias in neural networks have been proposed, their limitations remain under-explored. In this work, we mathematically and empirically reveal an important limitation of attribute bias removal methods in presence of strong bias. Specifically, we derive a general non-vacuous information-theoretical upper bound on the performance of any attribute bias removal method in terms of the bias strength. We provide extensive experiments on synthetic, image, and census datasets to verify the theoretical bound and its consequences in practice. Our findings show that existing attribute bias removal methods are effective only when the inherent bias in the dataset is relatively weak, thus cautioning against the use of these methods in smaller datasets where strong attribute bias can occur, and advocating the need for methods that can overcome this limitation.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04955", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "Generative AI May Prefer to Present National-level Characteristics of Cities Based on Stereotypical Geographic Impressions at the Continental Level", "authors": ["Shan Ye"], "abstract": "A simple experiment was conducted to test the ability of the Chinese-based generative artificial intelligence (AI) platform, Wenxin Yige, to render images of urban street views of different countries. The study found that images generated by this AI platform may contain continental-level stereotypes in terms of showing the level of economic development and modernization. Street view images generated from Wenxin Yige do not adequately represent the diverse range of urban landscapes found across different nations. Using these generated images for geography education or outreach initiatives could inadvertently strengthen people's existing stereotypical views about individual countries.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04897", "tags": []}, {"title": "Commercialized Generative AI: A Critical Study of the Feasibility and Ethics of Generating Native Advertising Using Large Language Models in Conversational Web Search", "authors": ["Ines Zelch", "Matthias Hagen", "Martin Potthast"], "abstract": "How will generative AI pay for itself? Unless charging users for access, selling advertising is the only alternative. Especially in the multi-billion dollar web search market with ads as the main source of revenue, the introduction of a subscription model seems unlikely. The recent disruption of search by generative large language models could thus ultimately be accompanied by generated ads. Our concern is that the commercialization of generative AI in general and large language models in particular could lead to native advertising in the form of quite subtle brand or product placements. In web search, the evolution of search engine results pages (SERPs) from traditional lists of ``ten blue links'' (lists SERPs) to generated text with web page references (text SERPs) may further blur the line between advertising-based and organic search results, making it difficult for users to distinguish between the two, depending on how advertising is integrated and disclosed. To raise awareness of this potential development, we conduct a pilot study analyzing the capabilities of current large language models to blend ads with organic search results. Although the models still struggle to subtly frame ads in an unrelated context, their potential is evident when integrating ads into related topics which calls for further investigation.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04892", "tags": ["Natural Language Processing"]}, {"title": "Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models", "authors": ["Gabriele Tolomei", "Cesare Campagnano", "Fabrizio Silvestri", "Giovanni Trappolini"], "abstract": "In this paper, we present a groundbreaking paradigm for human-computer interaction that revolutionizes the traditional notion of an operating system.\n  Within this innovative framework, user requests issued to the machine are handled by an interconnected ecosystem of generative AI models that seamlessly integrate with or even replace traditional software applications. At the core of this paradigm shift are large generative models, such as language and diffusion models, which serve as the central interface between users and computers. This pioneering approach leverages the abilities of advanced language models, empowering users to engage in natural language conversations with their computing devices. Users can articulate their intentions, tasks, and inquiries directly to the system, eliminating the need for explicit commands or complex navigation. The language model comprehends and interprets the user's prompts, generating and displaying contextual and meaningful responses that facilitate seamless and intuitive interactions.\n  This paradigm shift not only streamlines user interactions but also opens up new possibilities for personalized experiences. Generative models can adapt to individual preferences, learning from user input and continuously improving their understanding and response generation. Furthermore, it enables enhanced accessibility, as users can interact with the system using speech or text, accommodating diverse communication preferences.\n  However, this visionary concept raises significant challenges, including privacy, security, trustability, and the ethical use of generative models. Robust safeguards must be in place to protect user data and prevent potential misuse or manipulation of the language model.\n  While the full realization of this paradigm is still far from being achieved, this paper serves as a starting point for envisioning this transformative potential.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04875", "tags": ["Natural Language Processing", "AI and Privacy"]}, {"title": "PaperCard for Reporting Machine Assistance in Academic Writing", "authors": ["Won Ik Cho", "Eunjung Cho", "Kyunghyun Cho"], "abstract": "Academic writing process has benefited from various technological developments over the years including search engines, automatic translators, and editing tools that review grammar and spelling mistakes. They have enabled human writers to become more efficient in writing academic papers, for example by helping with finding relevant literature more effectively and polishing texts. While these developments have so far played a relatively assistive role, recent advances in large-scale language models (LLMs) have enabled LLMs to play a more major role in the writing process, such as coming up with research questions and generating key contents. This raises critical questions surrounding the concept of authorship in academia. ChatGPT, a question-answering system released by OpenAI in November 2022, has demonstrated a range of capabilities that could be utilised in producing academic papers. The academic community will have to address relevant pressing questions, including whether Artificial Intelligence (AI) should be merited authorship if it made significant contributions in the writing process, or whether its use should be restricted such that human authorship would not be undermined. In this paper, we aim to address such questions, and propose a framework we name \"PaperCard\", a documentation for human authors to transparently declare the use of AI in their writing process.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04824", "tags": ["AI in Education"]}, {"title": "Chat Vector: A Simple Approach to Equip LLMs With New Language Chat Capabilities", "authors": ["Shih-Cheng Huang", "Pin-Zu Li", "Yu-Chi Hsu", "Kuang-Ming Chen", "Yu Tung Lin", "Shih-Kai Hsiao", "Richard Tzong-Han Tsai", "Hung-yi Lee"], "abstract": "With the advancements in conversational AI, such as ChatGPT, this paper focuses on exploring developing Large Language Models (LLMs) for non-English languages, especially emphasizing alignment with human preferences. We introduce a computationally efficient method, leveraging chat vector, to synergize pre-existing knowledge and behaviors in LLMs, restructuring the conventional training paradigm from continual pre-train -> SFT -> RLHF to continual pre-train + chat vector. Our empirical studies, primarily focused on Traditional Chinese, employ LLaMA2 as the base model and acquire the chat vector by subtracting the pre-trained weights, LLaMA2, from the weights of LLaMA2-chat. Evaluating from three distinct facets, which are toxicity, ability of instruction following, and multi-turn dialogue demonstrates the chat vector's superior efficacy in chatting. To confirm the adaptability of our approach, we extend our experiments to include models pre-trained in both Korean and Simplified Chinese, illustrating the versatility of our methodology. Overall, we present a significant solution in aligning LLMs with human preferences efficiently across various languages, accomplished by the chat vector.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04799", "tags": ["Natural Language Processing"]}, {"title": "Investigating the Influence of Legal Case Retrieval Systems on Users' Decision Process", "authors": ["Beining Wang", "Ruizhe Zhang", "Yueyue Wu", "Qingyao Ai", "Min Zhang", "Yiqun Liu"], "abstract": "Given a specific query case, legal case retrieval systems aim to retrieve a set of case documents relevant to the case at hand. Previous studies on user behavior analysis have shown that information retrieval (IR) systems can significantly influence users' decisions by presenting results in varying orders and formats. However, whether such influence exists in legal case retrieval remains largely unknown. This study presents the first investigation into the influence of legal case retrieval systems on the decision-making process of legal users. We conducted an online user study involving more than ninety participants, and our findings suggest that the result distribution of legal case retrieval systems indeed affect users' judgements on the sentences in cases. Notably, when users are presented with biased results that involve harsher sentences, they tend to impose harsher sentences on the current case as well. This research highlights the importance of optimizing the unbiasedness of legal case retrieval systems.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04735", "tags": []}, {"title": "Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API", "authors": ["Zhizheng Zhang", "Wenxuan Xie", "Xiaoyi Zhang", "Yan Lu"], "abstract": "Recent popularity of Large Language Models (LLMs) has opened countless possibilities in automating numerous AI tasks by connecting LLMs to various domain-specific models or APIs, where LLMs serve as dispatchers while domain-specific models or APIs are action executors. Despite the vast numbers of domain-specific models/APIs, they still struggle to comprehensively cover super diverse automation demands in the interaction between human and User Interfaces (UIs). In this work, we build a multimodal model to ground natural language instructions in given UI screenshots as a generic UI task automation executor. This metadata-free grounding model, consisting of a visual encoder and a language decoder, is first pretrained on well studied document understanding tasks and then learns to decode spatial information from UI screenshots in a promptable way. To facilitate the exploitation of image-to-text pretrained knowledge, we follow the pixel-to-sequence paradigm to predict geometric coordinates in a sequence of tokens using a language decoder. We further propose an innovative Reinforcement Learning (RL) based algorithm to supervise the tokens in such sequence jointly with visually semantic metrics, which effectively strengthens the spatial decoding capability of the pixel-to-sequence paradigm. Extensive experiments demonstrate our proposed reinforced UI instruction grounding model outperforms the state-of-the-art methods by a clear margin and shows the potential as a generic UI task automation API.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.04716", "tags": ["Natural Language Processing"]}, {"title": "EasyPhoto: Your Smart AI Photo Generator", "authors": ["Ziheng Wu", "Jiaqi Xu", "Xinyi Zou", "Kunzhe Huang", "Xing Shi", "Jun Huang"], "abstract": "Stable Diffusion web UI (SD-WebUI) is a comprehensive project that provides a browser interface based on Gradio library for Stable Diffusion models. In this paper, We propose a novel WebUI plugin called EasyPhoto, which enables the generation of AI portraits. By training a digital doppelganger of a specific user ID using 5 to 20 relevant images, the finetuned model (according to the trained LoRA model) allows for the generation of AI photos using arbitrary templates. Our current implementation supports the modification of multiple persons and different photo styles. Furthermore, we allow users to generate fantastic template image with the strong SDXL model, enhancing EasyPhoto's capabilities to deliver more diverse and satisfactory results. The source code for EasyPhoto is available at: https://github.com/aigc-apps/sd-webui-EasyPhoto. We also support a webui-free version by using diffusers: https://github.com/aigc-apps/EasyPhoto. We are continuously enhancing our efforts to expand the EasyPhoto pipeline, making it suitable for any identification (not limited to just the face), and we enthusiastically welcome any intriguing ideas or suggestions.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04672", "tags": ["AI in Healthcare"]}, {"title": "Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem Formulation and Dataset", "authors": ["Korawat Charoenpitaks", "Van-Quang Nguyen", "Masanori Suganuma", "Masahiro Takahashi", "Ryoma Niihara", "Takayuki Okatani"], "abstract": "This paper addresses the problem of predicting hazards that drivers may encounter while driving a car. We formulate it as a task of anticipating impending accidents using a single input image captured by car dashcams. Unlike existing approaches to driving hazard prediction that rely on computational simulations or anomaly detection from videos, this study focuses on high-level inference from static images. The problem needs predicting and reasoning about future events based on uncertain observations, which falls under visual abductive reasoning. To enable research in this understudied area, a new dataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is created. The dataset consists of 15K dashcam images of street scenes, and each image is associated with a tuple containing car speed, a hypothesized hazard description, and visual entities present in the scene. These are annotated by human annotators, who identify risky scenes and provide descriptions of potential accidents that could occur a few seconds later. We present several baseline methods and evaluate their performance on our dataset, identifying remaining issues and discussing future directions. This study contributes to the field by introducing a novel problem formulation and dataset, enabling researchers to explore the potential of multi-modal AI for driving hazard prediction.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.04671", "tags": []}, {"title": "DxPU: Large Scale Disaggregated GPU Pools in the Datacenter", "authors": ["Bowen He", "Xiao Zheng", "Yuan Chen", "Weinan Li", "Yajin Zhou", "Xin Long", "Pengcheng Zhang", "Xiaowei Lu", "Linquan Jiang", "Qiang Liu", "Dennis Cai", "Xiantao Zhang"], "abstract": "The rapid adoption of AI and convenience offered by cloud services have resulted in the growing demands for GPUs in the cloud. Generally, GPUs are physically attached to host servers as PCIe devices. However, the fixed assembly combination of host servers and GPUs is extremely inefficient in resource utilization, upgrade, and maintenance. Due to these issues, the GPU disaggregation technique has been proposed to decouple GPUs from host servers. It aggregates GPUs into a pool, and allocates GPU node(s) according to user demands. However, existing GPU disaggregation systems have flaws in software-hardware compatibility, disaggregation scope, and capacity. In this paper, we present a new implementation of datacenter-scale GPU disaggregation, named DxPU. DxPU efficiently solves the above problems and can flexibly allocate as many GPU node(s) as users demand. In order to understand the performance overhead incurred by DxPU, we build up a performance model for AI specific workloads. With the guidance of modeling results, we develop a prototype system, which has been deployed into the datacenter of a leading cloud provider for a test run. We also conduct detailed experiments to evaluate the performance overhead caused by our system. The results show that the overhead of DxPU is less than 10%, compared with native GPU servers, in most of user scenarios.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04648", "tags": []}, {"title": "Trust in Generative AI among students: An Exploratory Study", "authors": ["Matin Amoozadeh", "David Daniels", "Daye Nam", "Stella Chen", "Michael Hilton", "Sruti Srinivasa Ragavan", "Mohammad Amin Alipour"], "abstract": "Generative artificial systems (GenAI) have experienced exponential growth in the past couple of years. These systems offer exciting capabilities, such as generating programs, that students can well utilize for their learning. Among many dimensions that might affect the effective adoption of GenAI, in this paper, we investigate students' \\textit{trust}. Trust in GenAI influences the extent to which students adopt GenAI, in turn affecting their learning. In this study, we surveyed 253 students at two large universities to understand how much they trust \\genai tools and their feedback on how GenAI impacts their performance in CS courses. Our results show that students have different levels of trust in GenAI. We also observe different levels of confidence and motivation, highlighting the need for further understanding of factors impacting trust.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04631", "tags": []}, {"title": "Metadata-Conditioned Generative Models to Synthesize Anatomically-Plausible 3D Brain MRIs", "authors": ["Wei Peng", "Tomas Bosschieter", "Jiahong Ouyang", "Robert Paul", "Ehsan Adeli", "Qingyu Zhao", "Kilian M. Pohl"], "abstract": "Generative AI models hold great potential in creating synthetic brain MRIs that advance neuroimaging studies by, for example, enriching data diversity. However, the mainstay of AI research only focuses on optimizing the visual quality (such as signal-to-noise ratio) of the synthetic MRIs while lacking insights into their relevance to neuroscience. To gain these insights with respect to T1-weighted MRIs, we first propose a new generative model, BrainSynth, to synthesize metadata-conditioned (e.g., age- and sex-specific) MRIs that achieve state-of-the-art visual quality. We then extend our evaluation with a novel procedure to quantify anatomical plausibility, i.e., how well the synthetic MRIs capture macrostructural properties of brain regions, and how accurately they encode the effects of age and sex. Results indicate that more than half of the brain regions in our synthetic MRIs are anatomically accurate, i.e., with a small effect size between real and synthetic MRIs. Moreover, the anatomical plausibility varies across cortical regions according to their geometric complexity. As is, our synthetic MRIs can significantly improve the training of a Convolutional Neural Network to identify accelerated aging effects in an independent study. These results highlight the opportunities of using generative AI to aid neuroimaging research and point to areas for further improvement.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04630", "tags": []}, {"title": "(Re)framing Built Heritage through the Machinic Gaze", "authors": ["Vanicka Arora", "Liam Magee", "Luke Munn"], "abstract": "Built heritage has been both subject and product of a gaze that has been sustained through moments of colonial fixation on ruins and monuments, technocratic examination and representation, and fetishisation by aglobal tourist industry. We argue that the recent proliferation of machine learning and vision technologies create new scopic regimes for heritage: storing and retrieving existing images from vast digital archives, and further imparting their own distortions upon its visual representation. We introduce the term `machinic gaze' to conceptualise the reconfiguration of heritage representation via AI models. To explore how this gaze reframes heritage, we deploy an image-text-image pipeline that reads, interprets, and resynthesizes images of several UNESCO World Heritage Sites. Employing two concepts from media studies -- heteroscopia and anamorphosis -- we describe the reoriented perspective that machine vision systems introduce. We propose that the machinic gaze highlights the artifice of the human gaze and its underlying assumptions and practices that combine to form established notions of heritage.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04628", "tags": []}, {"title": "DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies", "authors": ["Shuaiwen Leon Song", "Bonnie Kruft", "Minjia Zhang", "Conglong Li", "Shiyang Chen", "Chengming Zhang", "Masahiro Tanaka", "Xiaoxia Wu", "Jeff Rasley", "Ammar Ahmad Awan", "Connor Holmes", "Martin Cai", "Adam Ghanem", "Zhongzhu Zhou", "Yuxiong He", "Pete Luferenko", "Divya Kumar", "Jonathan Weyn", "Ruixiong Zhang", "Sylwester Klocek", "Volodymyr Vragov", "Mohammed AlQuraishi", "Gustaf Ahdritz", "Christina Floristean", "Cristina Negri", "et al. (67 additional authors not shown)"], "abstract": "In the upcoming decade, deep learning may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy. To answer this call, we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. By leveraging DeepSpeed's current technology pillars (training, inference and compression) as base technology enablers, DeepSpeed4Science will create a new set of AI system technologies tailored for accelerating scientific discoveries by addressing their unique complexity beyond the common technical approaches used for accelerating generic large language models (LLMs). In this paper, we showcase the early progress we made with DeepSpeed4Science in addressing two of the critical system challenges in structural biology research.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.04610", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators", "authors": ["Murali Emani", "Sam Foreman", "Varuni Sastry", "Zhen Xie", "Siddhisanket Raskar", "William Arnold", "Rajeev Thakur", "Venkatram Vishwanath", "Michael E. Papka"], "abstract": "Artificial intelligence (AI) methods have become critical in scientific applications to help accelerate scientific discovery. Large language models (LLMs) are being considered as a promising approach to address some of the challenging problems because of their superior generalization capabilities across domains. The effectiveness of the models and the accuracy of the applications is contingent upon their efficient execution on the underlying hardware infrastructure. Specialized AI accelerator hardware systems have recently become available for accelerating AI applications. However, the comparative performance of these AI accelerators on large language models has not been previously studied. In this paper, we systematically study LLMs on multiple AI accelerators and GPUs and evaluate their performance characteristics for these models. We evaluate these systems with (i) a micro-benchmark using a core transformer block, (ii) a GPT- 2 model, and (iii) an LLM-driven science use case, GenSLM. We present our findings and analyses of the models' performance to better understand the intrinsic capabilities of AI accelerators. Furthermore, our analysis takes into account key factors such as sequence lengths, scaling behavior, sparsity, and sensitivity to gradient accumulation steps.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04607", "tags": ["Ethical AI and Bias Mitigation", "AI in Healthcare"]}, {"title": "TrialView: An AI-powered Visual Analytics System for Temporal Event Data in Clinical Trials", "authors": ["Zuotian Li", "Xiang Liu", "Zelei Cheng", "Yingjie Chen", "Wanzhu Tu", "Jing Su"], "abstract": "Randomized controlled trials (RCT) are the gold standards for evaluating the efficacy and safety of therapeutic interventions in human subjects. In addition to the pre-specified endpoints, trial participants' experience reveals the time course of the intervention. Few analytical tools exist to summarize and visualize the individual experience of trial participants. Visual analytics allows integrative examination of temporal event patterns of patient experience, thus generating insights for better care decisions. Towards this end, we introduce TrialView, an information system that combines graph artificial intelligence (AI) and visual analytics to enhance the dissemination of trial data. TrialView offers four distinct yet interconnected views: Individual, Cohort, Progression, and Statistics, enabling an interactive exploration of individual and group-level data. The TrialView system is a general-purpose analytical tool for a broad class of clinical trials. The system is powered by graph AI, knowledge-guided clustering, explanatory modeling, and graph-based agglomeration algorithms. We demonstrate the system's effectiveness in analyzing temporal event data through a case study.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04586", "tags": ["AI and Privacy"]}, {"title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models", "authors": ["Iman Mirzadeh", "Keivan Alizadeh", "Sachin Mehta", "Carlo C Del Mundo", "Oncel Tuzel", "Golnoosh Samei", "Mohammad Rastegari", "Mehrdad Farajtabar"], "abstract": "Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04564", "tags": []}, {"title": "Measuring Information in Text Explanations", "authors": ["Zining Zhu", "Frank Rudzicz"], "abstract": "Text-based explanation is a particularly promising approach in explainable AI, but the evaluation of text explanations is method-dependent. We argue that placing the explanations on an information-theoretic framework could unify the evaluations of two popular text explanation methods: rationale and natural language explanations (NLE). This framework considers the post-hoc text pipeline as a series of communication channels, which we refer to as ``explanation channels''. We quantify the information flow through these channels, thereby facilitating the assessment of explanation characteristics. We set up tools for quantifying two information scores: relevance and informativeness. We illustrate what our proposed information scores measure by comparing them against some traditional evaluation metrics. Our information-theoretic scores reveal some unique observations about the underlying mechanisms of two representative text explanations. For example, the NLEs trade-off slightly between transmitting the input-related information and the target-related information, whereas the rationales do not exhibit such a trade-off mechanism. Our work contributes to the ongoing efforts in establishing rigorous and standardized evaluation criteria in the rapidly evolving field of explainable AI.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04557", "tags": ["Explainable AI (XAI):"]}, {"title": "Auto-survey Challenge", "authors": ["Thanh Gia Hieu Khuong", "Benedictus Kent Rachmat"], "abstract": "We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.04480", "tags": ["AI in Education"]}, {"title": "Taming Binarized Neural Networks and Mixed-Integer Programs", "authors": ["Johannes Aspman", "Georgios Korpas", "Jakub Marecek"], "abstract": "There has been a great deal of recent interest in binarized neural networks, especially because of their explainability. At the same time, automatic differentiation algorithms such as backpropagation fail for binarized neural networks, which limits their applicability. By reformulating the problem of training binarized neural networks as a subadditive dual of a mixed-integer program, we show that binarized neural networks admit a tame representation. This, in turn, makes it possible to use the framework of Bolte et al. for implicit differentiation, which offers the possibility for practical implementation of backpropagation in the context of binarized neural networks. This approach could also be used for a broader class of mixed-integer programs, beyond the training of binarized neural networks, as encountered in symbolic approaches to AI and beyond.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.04469", "tags": []}, {"title": "Design Principles for Lifelong Learning AI Accelerators", "authors": ["Dhireesha Kudithipudi", "Anurag Daram", "Abdullah M. Zyarah", "Fatima Tuz Zohora", "James B. Aimone", "Angel Yanguas-Gil", "Nicholas Soures", "Emre Neftci", "Matthew Mattina", "Vincenzo Lomonaco", "Clare D. Thiem", "Benjamin Epstein"], "abstract": "Lifelong learning - an agent's ability to learn throughout its lifetime - is a hallmark of biological learning systems and a central challenge for artificial intelligence (AI). The development of lifelong learning algorithms could lead to a range of novel AI applications, but this will also require the development of appropriate hardware accelerators, particularly if the models are to be deployed on edge platforms, which have strict size, weight, and power constraints. Here, we explore the design of lifelong learning AI accelerators that are intended for deployment in untethered environments. We identify key desirable capabilities for lifelong learning accelerators and highlight metrics to evaluate such accelerators. We then discuss current edge AI accelerators and explore the future design of lifelong learning accelerators, considering the role that different emerging technologies could play.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.04467", "tags": ["AI and Privacy", "AI in Education"]}, {"title": "AI-based automated active learning for discovery of hidden dynamic processes: A use case in light microscopy", "authors": ["Nils Friederich", "Angelo Yamachui Sitcheu", "Oliver Neumann", "S\u00fcheyla Ero\u011flu-Kay\u0131k\u00e7\u0131", "Roshan Prizak", "Lennart Hilbert", "Ralf Mikut"], "abstract": "In the biomedical environment, experiments assessing dynamic processes are primarily performed by a human acquisition supervisor. Contemporary implementations of such experiments frequently aim to acquire a maximum number of relevant events from sometimes several hundred parallel, non-synchronous processes. Since in some high-throughput experiments, only one or a few instances of a given process can be observed simultaneously, a strategy for planning and executing an efficient acquisition paradigm is essential. To address this problem, we present two new methods in this paper. The first method, Encoded Dynamic Process (EDP), is Artificial Intelligence (AI)-based and represents dynamic processes so as to allow prediction of pseudo-time values from single still images. Second, with Experiment Automation Pipeline for Dynamic Processes (EAPDP), we present a Machine Learning Operations (MLOps)-based pipeline that uses the extracted knowledge from EDP to efficiently schedule acquisition in biomedical experiments for dynamic processes in practice. In a first experiment, we show that the pre-trained State-Of-The- Art (SOTA) object segmentation method Contour Proposal Networks (CPN) works reliably as a module of EAPDP to extract the relevant object for EDP from the acquired three-dimensional image stack.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.04461", "tags": []}, {"title": "A Brief History of Prompt: Leveraging Language Models", "authors": ["Golam Md Muktadir"], "abstract": "This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the emergence of advanced techniques like unsupervised pre-training and novel reward shaping. Throughout the paper, we reference specific research studies that exemplify the impact of various developments on prompt engineering. The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems.", "submitted": "2023-09-30", "link": "https://arxiv.org/pdf/2310.04438", "tags": ["Natural Language Processing"]}, {"title": "Generative AI in the Construction Industry: Opportunities & Challenges", "authors": ["Prashnna Ghimire", "Kyungki Kim", "Manoj Acharya"], "abstract": "In the last decade, despite rapid advancements in artificial intelligence (AI) transforming many industry practices, construction largely lags in adoption. Recently, the emergence and rapid adoption of advanced large language models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown great potential and sparked considerable global interest. However, the current surge lacks a study investigating the opportunities and challenges of implementing Generative AI (GenAI) in the construction sector, creating a critical knowledge gap for researchers and practitioners. This underlines the necessity to explore the prospects and complexities of GenAI integration. Bridging this gap is fundamental to optimizing GenAI's early-stage adoption within the construction sector. Given GenAI's unprecedented capabilities to generate human-like content based on learning from existing content, we reflect on two guiding questions: What will the future bring for GenAI in the construction industry? What are the potential opportunities and challenges in implementing GenAI in the construction industry? This study delves into reflected perception in literature, analyzes the industry perception using programming-based word cloud and frequency analysis, and integrates authors' opinions to answer these questions. This paper recommends a conceptual GenAI implementation framework, provides practical recommendations, summarizes future research questions, and builds foundational literature to foster subsequent research expansion in GenAI within the construction and its allied architecture & engineering domains.", "submitted": "2023-09-19", "link": "https://arxiv.org/pdf/2310.04427", "tags": ["AI in Education"]}, {"title": "Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol and the NIST-approved Quantum-Resistant Cryptographic Algorithms", "authors": ["Petar Radanliev", "David De Roure", "Omar Santos"], "abstract": "In the contemporary digital age, Quantum Computing and Artificial Intelligence (AI) convergence is reshaping the cyber landscape, introducing unprecedented opportunities and potential vulnerabilities.This research, conducted over five years, delves into the cybersecurity implications of this convergence, with a particular focus on AI/Natural Language Processing (NLP) models and quantum cryptographic protocols, notably the BB84 method and specific NIST-approved algorithms. Utilising Python and C++ as primary computational tools, the study employs a \"red teaming\" approach, simulating potential cyber-attacks to assess the robustness of quantum security measures. Preliminary research over 12 months laid the groundwork, which this study seeks to expand upon, aiming to translate theoretical insights into actionable, real-world cybersecurity solutions. Located at the University of Oxford's technology precinct, the research benefits from state-of-the-art infrastructure and a rich collaborative environment. The study's overarching goal is to ensure that as the digital world transitions to quantum-enhanced operations, it remains resilient against AI-driven cyber threats. The research aims to foster a safer, quantum-ready digital future through iterative testing, feedback integration, and continuous improvement. The findings are intended for broad dissemination, ensuring that the knowledge benefits academia and the global community, emphasising the responsible and secure harnessing of quantum technology.", "submitted": "2023-09-16", "link": "https://arxiv.org/pdf/2310.04425", "tags": ["Cyber Security"]}, {"title": "Stability Analysis of Non-Linear Classifiers using Gene Regulatory Neural Network for Biological AI", "authors": ["Adrian Ratwatte", "Samitha Somathilaka", "Sasitharan Balasubramaniam", "Assaf A. Gilad"], "abstract": "The Gene Regulatory Network (GRN) of biological cells governs a number of key functionalities that enables them to adapt and survive through different environmental conditions. Close observation of the GRN shows that the structure and operational principles resembles an Artificial Neural Network (ANN), which can pave the way for the development of Biological Artificial Intelligence. In particular, a gene's transcription and translation process resembles a sigmoidal-like property based on transcription factor inputs. In this paper, we develop a mathematical model of gene-perceptron using a dual-layered transcription-translation chemical reaction model, enabling us to transform a GRN into a Gene Regulatory Neural Network (GRNN). We perform stability analysis for each gene-perceptron within the fully-connected GRNN sub network to determine temporal as well as stable concentration outputs that will result in reliable computing performance. We focus on a non-linear classifier application for the GRNN, where we analyzed generic multi-layer GRNNs as well as E.Coli GRNN that is derived from trans-omic experimental data. Our analysis found that varying the parameters of the chemical reactions can allow us shift the boundaries of the classification region, laying the platform for programmable GRNNs that suit diverse application requirements.", "submitted": "2023-09-14", "link": "https://arxiv.org/pdf/2310.04424", "tags": ["Neuromorphic Computing"]}, {"title": "Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets", "authors": ["Zhang-Wei Hong", "Aviral Kumar", "Sathwik Karnik", "Abhishek Bhandwaldar", "Akash Srivastava", "Joni Pajarinen", "Romain Laroche", "Abhishek Gupta", "Pulkit Agrawal"], "abstract": "Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data\" rather than all actions in the dataset (i.e., uniform sampling). We present a realization of the sampling strategy and an algorithm that can be used as a plug-and-play module in standard offline RL algorithms. Our evaluation demonstrates significant performance gains in 72 imbalanced datasets, D4RL dataset, and across three different offline RL algorithms. Code is available at https://github.com/Improbable-AI/dw-offline-rl.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.04413", "tags": []}, {"title": "Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates", "authors": ["Camille Olivia Little", "Debolina Halder Lina", "Genevera I. Allen"], "abstract": "Across various sectors such as healthcare, criminal justice, national security, finance, and technology, large-scale machine learning (ML) and artificial intelligence (AI) systems are being deployed to make critical data-driven decisions. Many have asked if we can and should trust these ML systems to be making these decisions. Two critical components are prerequisites for trust in ML systems: interpretability, or the ability to understand why the ML system makes the decisions it does, and fairness, which ensures that ML systems do not exhibit bias against certain individuals or groups. Both interpretability and fairness are important and have separately received abundant attention in the ML literature, but so far, there have been very few methods developed to directly interpret models with regard to their fairness. In this paper, we focus on arguably the most popular type of ML interpretation: feature importance scores. Inspired by the use of decision trees in knowledge distillation, we propose to leverage trees as interpretable surrogates for complex black-box ML models. Specifically, we develop a novel fair feature importance score for trees that can be used to interpret how each feature contributes to fairness or bias in trees, tree-based ensembles, or tree-based surrogates of any complex ML system. Like the popular mean decrease in impurity for trees, our Fair Feature Importance Score is defined based on the mean decrease (or increase) in group bias. Through simulations as well as real examples on benchmark fairness datasets, we demonstrate that our Fair Feature Importance Score offers valid interpretations for both tree-based ensembles and tree-based surrogates of other ML systems.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04352", "tags": ["Ethical AI and Bias Mitigation", "AI in Finance and Fintech", "AI in Healthcare"]}, {"title": "Saliency-Guided Hidden Associative Replay for Continual Learning", "authors": ["Guangji Bai", "Qilong Zhao", "Xiaoyang Jiang", "Yifei Zhang", "Liang Zhao"], "abstract": "Continual Learning is a burgeoning domain in next-generation AI, focusing on training neural networks over a sequence of tasks akin to human learning. While CL provides an edge over traditional supervised learning, its central challenge remains to counteract catastrophic forgetting and ensure the retention of prior tasks during subsequent learning. Amongst various strategies to tackle this, replay based methods have emerged as preeminent, echoing biological memory mechanisms. However, these methods are memory intensive, often preserving entire data samples, an approach inconsistent with humans selective memory retention of salient experiences. While some recent works have explored the storage of only significant portions of data in episodic memory, the inherent nature of partial data necessitates innovative retrieval mechanisms. Current solutions, like inpainting, approximate full data reconstruction from partial cues, a method that diverges from genuine human memory processes. Addressing these nuances, this paper presents the Saliency Guided Hidden Associative Replay for Continual Learning. This novel framework synergizes associative memory with replay-based strategies. SHARC primarily archives salient data segments via sparse memory encoding. Importantly, by harnessing associative memory paradigms, it introduces a content focused memory retrieval mechanism, promising swift and near-perfect recall, bringing CL a step closer to authentic human memory processes. Extensive experimental results demonstrate the effectiveness of our proposed method for various continual learning tasks.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04334", "tags": []}, {"title": "From task structures to world models: What do LLMs know?", "authors": ["Ilker Yildirim", "L. A. Paul"], "abstract": "In what sense does a large language model have knowledge? The answer to this question extends beyond the capabilities of a particular AI system, and challenges our assumptions about the nature of knowledge and intelligence. We answer by granting LLMs \"instrumental knowledge\"; knowledge defined by a certain set of abilities. We then ask how such knowledge is related to the more ordinary, \"worldly\" knowledge exhibited by human agents, and explore this in terms of the degree to which instrumental knowledge can be said to incorporate the structured world models of cognitive science. We discuss ways LLMs could recover degrees of worldly knowledge, and suggest such recovery will be governed by an implicit, resource-rational tradeoff between world models and task demands.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04276", "tags": ["Natural Language Processing"]}, {"title": "Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions", "authors": ["Udo Schlegel", "Daniel A. Keim"], "abstract": "Given the increasing amount and general complexity of time series data in domains such as finance, weather forecasting, and healthcare, there is a growing need for state-of-the-art performance models that can provide interpretable insights into underlying patterns and relationships. Attribution techniques enable the extraction of explanations from time series models to gain insights but are hard to evaluate for their robustness and trustworthiness. We propose the Attribution Stability Indicator (ASI), a measure to incorporate robustness and trustworthiness as properties of attribution techniques for time series into account. We extend a perturbation analysis with correlations of the original time series to the perturbed instance and the attributions to include wanted properties in the measure. We demonstrate the wanted properties based on an analysis of the attributions in a dimension-reduced space and the ASI scores distribution over three whole time series classification datasets.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04178", "tags": []}, {"title": "AI Regulation in Europe: From the AI Act to Future Regulatory Challenges", "authors": ["Philipp Hacker"], "abstract": "This chapter provides a comprehensive discussion on AI regulation in the European Union, contrasting it with the more sectoral and self-regulatory approach in the UK. It argues for a hybrid regulatory strategy that combines elements from both philosophies, emphasizing the need for agility and safe harbors to ease compliance. The paper examines the AI Act as a pioneering legislative effort to address the multifaceted challenges posed by AI, asserting that, while the Act is a step in the right direction, it has shortcomings that could hinder the advancement of AI technologies. The paper also anticipates upcoming regulatory challenges, such as the management of toxic content, environmental concerns, and hybrid threats. It advocates for immediate action to create protocols for regulated access to high-performance, potentially open-source AI systems. Although the AI Act is a significant legislative milestone, it needs additional refinement and global collaboration for the effective governance of rapidly evolving AI technologies.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04072", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models", "authors": ["Boyu Zhang", "Hongyang Yang", "Tianyu Zhou", "Ali Babar", "Xiao-Yang Liu"], "abstract": "Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15\\% to 48\\% performance gain in accuracy and F1 score.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04027", "tags": []}, {"title": "The Role of Federated Learning in a Wireless World with Foundation Models", "authors": ["Zihan Chen", "Howard H. Yang", "Y. C. Tay", "Kai Fong Ernest Chong", "Tony Q. S. Quek"], "abstract": "Foundation models (FMs) are general-purpose artificial intelligence (AI) models that have recently enabled multiple brand-new generative AI applications. The rapid advances in FMs serve as an important contextual backdrop for the vision of next-generation wireless networks, where federated learning (FL) is a key enabler of distributed network intelligence. Currently, the exploration of the interplay between FMs and FL is still in its nascent stage. Naturally, FMs are capable of boosting the performance of FL, and FL could also leverage decentralized data and computing resources to assist in the training of FMs. However, the exceptionally high requirements that FMs have for computing resources, storage, and communication overhead would pose critical challenges to FL-enabled wireless networks. In this article, we explore the extent to which FMs are suitable for FL over wireless networks, including a broad overview of research challenges and opportunities. In particular, we discuss multiple new paradigms for realizing future intelligent networks that integrate FMs and FL. We also consolidate several broad research directions associated with these paradigms.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.04003", "tags": []}, {"title": "From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self", "authors": ["Yue Fu", "Sami Foell", "Xuhai Xu", "Alexis Hiniker"], "abstract": "In the rapidly evolving landscape of AI-mediated communication (AIMC), tools powered by Large Language Models (LLMs) are becoming integral to interpersonal communication. Employing a mixed-methods approach, we conducted a one-week diary and interview study to explore users' perceptions of these tools' ability to: 1) support interpersonal communication in the short-term, and 2) lead to potential long-term effects. Our findings indicate that participants view AIMC support favorably, citing benefits such as increased communication confidence, and finding precise language to express their thoughts, navigating linguistic and cultural barriers. However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology. Furthermore, we identified four key communication spaces delineated by communication stakes (high or low) and relationship dynamics (formal or informal) that differentially predict users' attitudes toward AIMC tools. Specifically, participants found the tool is more suitable for communicating in formal relationships than informal ones and more beneficial in high-stakes than low-stakes communication.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03976", "tags": []}, {"title": "TRAIL Team Description Paper for RoboCup@Home 2023", "authors": ["Chikaha Tsuji", "Dai Komukai", "Mimo Shirasaka", "Hikaru Wada", "Tsunekazu Omija", "Aoi Horo", "Daiki Furuta", "Saki Yamaguchi", "So Ikoma", "Soshi Tsunashima", "Masato Kobayashi", "Koki Ishimoto", "Yuya Ikeda", "Tatsuya Matsushima", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Our team, TRAIL, consists of AI/ML laboratory members from The University of Tokyo. We leverage our extensive research experience in state-of-the-art machine learning to build general-purpose in-home service robots. We previously participated in two competitions using Human Support Robot (HSR): RoboCup@Home Japan Open 2020 (DSPL) and World Robot Summit 2020, equivalent to RoboCup World Tournament. Throughout the competitions, we showed that a data-driven approach is effective for performing in-home tasks. Aiming for further development of building a versatile and fast-adaptable system, in RoboCup @Home 2023, we unify three technologies that have recently been evaluated as components in the fields of deep learning and robot learning into a real household robot system. In addition, to stimulate research all over the RoboCup@Home community, we build a platform that manages data collected from each site belonging to the community around the world, taking advantage of the characteristics of the community.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03913", "tags": []}, {"title": "Evaluating Multi-Agent Coordination Abilities in Large Language Models", "authors": ["Saaket Agashe", "Yue Fan", "Xin Eric Wang"], "abstract": "A pivotal aim in contemporary AI research is to develop agents proficient in multi-agent coordination, enabling effective collaboration with both humans and other systems. Large Language Models (LLMs), with their notable ability to understand, generate, and interpret language in a human-like manner, stand out as promising candidates for the development of such agents. In this study, we build and assess the effectiveness of agents crafted using LLMs in various coordination scenarios. We introduce the LLM-Coordination (LLM-Co) Framework, specifically designed to enable LLMs to play coordination games. With the LLM-Co framework, we conduct our evaluation with three game environments and organize the evaluation into five aspects: Theory of Mind, Situated Reasoning, Sustained Coordination, Robustness to Partners, and Explicit Assistance. First, the evaluation of the Theory of Mind and Situated Reasoning reveals the capabilities of LLM to infer the partner's intention and reason actions accordingly. Then, the evaluation around Sustained Coordination and Robustness to Partners further showcases the ability of LLMs to coordinate with an unknown partner in complex long-horizon tasks, outperforming Reinforcement Learning baselines. Lastly, to test Explicit Assistance, which refers to the ability of an agent to offer help proactively, we introduce two novel layouts into the Overcooked-AI benchmark, examining if agents can prioritize helping their partners, sacrificing time that could have been spent on their tasks. This research underscores the promising capabilities of LLMs in sophisticated coordination environments and reveals the potential of LLMs in building strong real-world agents for multi-agent coordination.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03903", "tags": ["AI in Education"]}, {"title": "Automatic and Human-AI Interactive Text Generation", "authors": ["Yao Dou", "Philippe Laban", "Claire Gardent", "Wei Xu"], "abstract": "In this tutorial, we focus on text-to-text generation, a class of natural language generation (NLG) tasks, that takes a piece of text as input and then generates a revision that is improved according to some specific criteria (e.g., readability or linguistic styles), while largely retaining the original meaning and the length of the text. This includes many useful applications, such as text simplification, paraphrase generation, style transfer, etc. In contrast to text summarization and open-ended text completion (e.g., story), the text-to-text generation tasks we discuss in this tutorial are more constrained in terms of semantic consistency and targeted language styles. This level of control makes these tasks ideal testbeds for studying the ability of models to generate text that is both semantically adequate and stylistically appropriate. Moreover, these tasks are interesting from a technical standpoint, as they require complex combinations of lexical and syntactical transformations, stylistic control, and adherence to factual knowledge, -- all at once. With a special focus on text simplification and revision, this tutorial aims to provide an overview of the state-of-the-art natural language generation research from four major aspects -- Data, Models, Human-AI Collaboration, and Evaluation -- and to discuss and showcase a few significant and recent advances: (1) the use of non-retrogressive approaches; (2) the shift from fine-tuning to prompting with large language models; (3) the development of new learnable metric and fine-grained human evaluation framework; (4) a growing body of studies and datasets on non-English languages; (5) the rise of HCI+NLP+Accessibility interdisciplinary research to create real-world writing assistant systems.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03878", "tags": ["Natural Language Processing"]}, {"title": "Securing Voice Biometrics: One-Shot Learning Approach for Audio Deepfake Detection", "authors": ["Awais Khan", "Khalid Mahmood Malik"], "abstract": "The Automatic Speaker Verification (ASV) system is vulnerable to fraudulent activities using audio deepfakes, also known as logical-access voice spoofing attacks. These deepfakes pose a concerning threat to voice biometrics due to recent advancements in generative AI and speech synthesis technologies. While several deep learning models for speech synthesis detection have been developed, most of them show poor generalizability, especially when the attacks have different statistical distributions from the ones seen. Therefore, this paper presents Quick-SpoofNet, an approach for detecting both seen and unseen synthetic attacks in the ASV system using one-shot learning and metric learning techniques. By using the effective spectral feature set, the proposed method extracts compact and representative temporal embeddings from the voice samples and utilizes metric learning and triplet loss to assess the similarity index and distinguish different embeddings. The system effectively clusters similar speech embeddings, classifying bona fide speeches as the target class and identifying other clusters as spoofing attacks. The proposed system is evaluated using the ASVspoof 2019 logical access (LA) dataset and tested against unseen deepfake attacks from the ASVspoof 2021 dataset. Additionally, its generalization ability towards unseen bona fide speech is assessed using speech data from the VSDC dataset.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03856", "tags": []}, {"title": "Integrating Audio-Visual Features for Multimodal Deepfake Detection", "authors": ["Sneha Muppalla", "Shan Jia", "Siwei Lyu"], "abstract": "Deepfakes are AI-generated media in which an image or video has been digitally modified. The advancements made in deepfake technology have led to privacy and security issues. Most deepfake detection techniques rely on the detection of a single modality. Existing methods for audio-visual detection do not always surpass that of the analysis based on single modalities. Therefore, this paper proposes an audio-visual-based method for deepfake detection, which integrates fine-grained deepfake identification with binary classification. We categorize the samples into four types by combining labels specific to each single modality. This method enhances the detection under intra-domain and cross-domain testing.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03827", "tags": []}, {"title": "ECAvg: An Edge-Cloud Collaborative Learning Approach using Averaged Weights", "authors": ["Atah Nuh Mih", "Hung Cao", "Asfia Kawnine", "Monica Wachowicz"], "abstract": "The use of edge devices together with cloud provides a collaborative relationship between both classes of devices where one complements the shortcomings of the other. Resource-constraint edge devices can benefit from the abundant computing power provided by servers by offloading computationally intensive tasks to the server. Meanwhile, edge devices can leverage their close proximity to the data source to perform less computationally intensive tasks on the data. In this paper, we propose a collaborative edge-cloud paradigm called ECAvg in which edge devices pre-train local models on their respective datasets and transfer the models to the server for fine-tuning. The server averages the pre-trained weights into a global model, which is fine-tuned on the combined data from the various edge devices. The local (edge) models are then updated with the weights of the global (server) model. We implement a CIFAR-10 classification task using MobileNetV2, a CIFAR-100 classification task using ResNet50, and an MNIST classification using a neural network with a single hidden layer. We observed performance improvement in the CIFAR-10 and CIFAR-100 classification tasks using our approach, where performance improved on the server model with averaged weights and the edge models had a better performance after model update. On the MNIST classification, averaging weights resulted in a drop in performance on both the server and edge models due to negative transfer learning. From the experiment results, we conclude that our approach is successful when implemented on deep neural networks such as MobileNetV2 and ResNet50 instead of simple neural networks.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03823", "tags": []}, {"title": "Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation", "authors": ["Tung Phung", "Victor-Alexandru P\u0103durean", "Anjali Singh", "Christopher Brooks", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Adish Singla", "Gustavo Soares"], "abstract": "Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a ``student'' model to further validate the hint quality -- it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03780", "tags": ["AI in Education"]}, {"title": "PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction", "authors": ["Saifullah Saifullah", "Stefan Agne", "Andreas Dengel", "Sheraz Ahmed"], "abstract": "In this paper, we introduce strategies for developing private Key Information Extraction (KIE) systems by leveraging large pretrained document foundation models in conjunction with differential privacy (DP), federated learning (FL), and Differentially Private Federated Learning (DP-FL). Through extensive experimentation on six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts, XFUND, and DOCILE), we demonstrate that large document foundation models can be effectively fine-tuned for the KIE task under private settings to achieve adequate performance while maintaining strong privacy guarantees. Moreover, by thoroughly analyzing the impact of various training and model parameters on model performance, we propose simple yet effective guidelines for achieving an optimal privacy-utility trade-off for the KIE task under global DP. Finally, we introduce FeAm-DP, a novel DP-FL algorithm that enables efficiently upscaling global DP from a standalone context to a multi-client federated environment. We conduct a comprehensive evaluation of the algorithm across various client and privacy settings, and demonstrate its capability to achieve comparable performance and privacy guarantees to standalone DP, even when accommodating an increasing number of participating clients. Overall, our study offers valuable insights into the development of private KIE systems, and highlights the potential of document foundation models for privacy-preserved Document AI applications. To the best of authors' knowledge, this is the first work that explores privacy preserved document KIE using document foundation models.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03777", "tags": ["AI and Privacy"]}, {"title": "A Deep Learning Sequential Decoder for Transient High-Density Electromyography in Hand Gesture Recognition Using Subject-Embedded Transfer Learning", "authors": ["Golara Ahmadi Azar", "Qin Hu", "Melika Emami", "Alyson Fletcher", "Sundeep Rangan", "S. Farokh Atashzar"], "abstract": "Hand gesture recognition (HGR) has gained significant attention due to the increasing use of AI-powered human-computer interfaces that can interpret the deep spatiotemporal dynamics of biosignals from the peripheral nervous system, such as surface electromyography (sEMG). These interfaces have a range of applications, including the control of extended reality, agile prosthetics, and exoskeletons. However, the natural variability of sEMG among individuals has led researchers to focus on subject-specific solutions. Deep learning methods, which often have complex structures, are particularly data-hungry and can be time-consuming to train, making them less practical for subject-specific applications. In this paper, we propose and develop a generalizable, sequential decoder of transient high-density sEMG (HD-sEMG) that achieves 73% average accuracy on 65 gestures for partially-observed subjects through subject-embedded transfer learning, leveraging pre-knowledge of HGR acquired during pre-training. The use of transient HD-sEMG before gesture stabilization allows us to predict gestures with the ultimate goal of counterbalancing system control delays. The results show that the proposed generalized models significantly outperform subject-specific approaches, especially when the training data is limited, and there is a significant number of gesture classes. By building on pre-knowledge and incorporating a multiplicative subject-embedded structure, our method comparatively achieves more than 13% average accuracy across partially observed subjects with minimal data availability. This work highlights the potential of HD-sEMG and demonstrates the benefits of modeling common patterns across users to reduce the need for large amounts of data for new users, enhancing practicality.", "submitted": "2023-09-23", "link": "https://arxiv.org/pdf/2310.03752", "tags": []}, {"title": "Artificial Intelligence Index Report 2023", "authors": ["Nestor Maslej", "Loredana Fattorini", "Erik Brynjolfsson", "John Etchemendy", "Katrina Ligett", "Terah Lyons", "James Manyika", "Helen Ngo", "Juan Carlos Niebles", "Vanessa Parli", "Yoav Shoham", "Russell Wald", "Jack Clark", "Raymond Perrault"], "abstract": "Welcome to the sixth edition of the AI Index Report. This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records, a study of the environmental impact of AI systems, and more. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03715", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy"]}, {"title": "Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures", "authors": ["Thorsten H\u00e4ndler"], "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities. However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations. Autonomous LLM-powered multi-agent systems represent a strategic response to these challenges. Such systems strive for autonomously tackling user-prompted goals by decomposing them into manageable tasks and orchestrating their execution and result synthesis through a collective of specialized intelligent agents. Equipped with LLM-powered reasoning capabilities, these agents harness the cognitive synergy of collaborating with their peers, enhanced by leveraging contextual resources such as tools and datasets. While these architectures hold promising potential in amplifying AI capabilities, striking the right balance between different levels of autonomy and alignment remains the crucial challenge for their effective operation. This paper proposes a comprehensive multi-dimensional taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems balance the dynamic interplay between autonomy and alignment across various aspects inherent to architectural viewpoints such as goal-driven task management, agent composition, multi-agent collaboration, and context interaction. It also includes a domain-ontology model specifying fundamental architectural concepts. Our taxonomy aims to empower researchers, engineers, and AI practitioners to systematically analyze the architectural dynamics and balancing strategies employed by these increasingly prevalent AI systems. The exploratory taxonomic classification of selected representative LLM-powered multi-agent systems illustrates its practical utility and reveals potential for future research and development.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03659", "tags": ["AI in Education"]}, {"title": "Rethinking Fairness for Human-AI Collaboration", "authors": ["Haosen Ge", "Hamsa Bastani", "Osbert Bastani"], "abstract": "Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions. However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration. Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy. As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern. We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern. We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy. However, we show that it may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy; thus, if our goal is to improve the equity and accuracy of human-AI collaboration, it may not be desirable to enforce traditional fairness constraints.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03647", "tags": []}, {"title": "Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally", "authors": ["Shawqi Al-Maliki", "Adnan Qayyum", "Hassan Ali", "Mohamed Abdallah", "Junaid Qadir", "Dinh Thai Hoang", "Dusit Niyato", "Ala Al-Fuqaha"], "abstract": "Deep Neural Networks (DNNs) have been the driving force behind many of the recent advances in machine learning. However, research has shown that DNNs are vulnerable to adversarial examples -- input samples that have been perturbed to force DNN-based models to make errors. As a result, Adversarial Machine Learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social AI applications. The emergence of new AI technologies that leverage Large Language Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing anti-social applications at scale. AdvML for Social Good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent pro-social applications. Regulators, practitioners, and researchers should collaborate to encourage the development of pro-social applications and hinder the development of anti-social ones. In this work, we provide the first comprehensive review of the emerging field of AdvML4G. This paper encompasses a taxonomy that highlights the emergence of AdvML4G, a discussion of the differences and similarities between AdvML4G and AdvML, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the works that utilize AdvML4G as an auxiliary tool for innovating pro-social applications. Finally, we elaborate upon various challenges and open research issues that require significant attention from the research community.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03614", "tags": []}, {"title": "Impact of Artificial Intelligence on Electrical and Electronics Engineering Productivity in the Construction Industry", "authors": ["Nwosu Obinnaya Chikezie Victor"], "abstract": "Artificial intelligence (AI) can revolutionize the development industry, primarily electrical and electronics engineering. By automating recurring duties, AI can grow productivity and efficiency in creating. For instance, AI can research constructing designs, discover capability troubles, and generate answers, reducing the effort and time required for manual analysis. AI also can be used to optimize electricity consumption in buildings, which is a critical difficulty in the construction enterprise. Via machines gaining knowledge of algorithms to investigate electricity usage patterns, AI can discover areas wherein power may be stored and offer guidelines for enhancements. This can result in significant value financial savings and reduced carbon emissions. Moreover, AI may be used to improve the protection of creation websites. By studying statistics from sensors and cameras, AI can locate capacity dangers and alert workers to take suitable action. This could help save you from injuries and accidents on production sites, lowering the chance for workers and enhancing overall safety in the enterprise. The impact of AI on electric and electronics engineering productivity inside the creation industry is enormous. AI can transform how we layout, build, and function buildings by automating ordinary duties, optimising electricity intake, and enhancing safety. However, ensuring that AI is used ethically and responsibly and that the advantages are shared fairly throughout the enterprise is essential.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03591", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "Redefining Digital Health Interfaces with Large Language Models", "authors": ["Fergus Imrie", "Paulius Rauba", "Mihaela van der Schaar"], "abstract": "Digital health tools have the potential to significantly improve the delivery of healthcare services. However, their use remains comparatively limited due, in part, to challenges surrounding usability and trust. Recently, Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare. Directly applying LLMs in clinical settings is not straightforward, with LLMs susceptible to providing inconsistent or nonsensical answers. We demonstrate how LLMs can utilize external tools to provide a novel interface between clinicians and digital technologies. This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLM in clinical settings such as hallucinations. We illustrate our approach with examples from cardiovascular disease and diabetes risk prediction, highlighting the benefit compared to traditional interfaces for digital tools.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03560", "tags": ["AI in Healthcare"]}, {"title": "Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review", "authors": ["Tita A. Bach", "Jenny K. Kristiansen", "Aleksandar Babic", "Alon Jacovi"], "abstract": "Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, what little research there is on HAII is fragmented and inconsistent. We present here a survey of that literature and recommendations for research best practices that will improve the field. We divided our investigation into the following research areas: (1) terms used to describe HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII, and (4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, five factors influence HAII: user characteristics and background (e.g., user personality, perceptions), AI interface and features (e.g., interactive UI design), AI output (e.g., accuracy, actionable recommendations), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments and user needs). HAII is most commonly measured with user-related subjective metrics (e.g., user perception, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems. Based on this review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03392", "tags": []}, {"title": "Measurement of $e^{+}e^{-}\\rightarrow\u03b7J/\u03c8$ Cross Section from $\\sqrt{s}=$ 3.808 GeV to 4.951 GeV", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "M. R. An", "Q. An", "Y. Bai", "O. Bakina", "I. Balossino", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "et al. (608 additional authors not shown)"], "abstract": "Using data samples with an integrated luminosity of 22.42 fb$^{-1}$ collected by the BESIII detector operating at the BEPCII storage ring, we measure the cross sections of the $e^{+}e^{-}\\rightarrow\\etaJ/\u03c8$ process at center-of-mass energies from 3.808 to 4.951 GeV. Three structures are observed in the line shape of the measured cross sections. A maximum-likelihood fit with $\u03c8(4040)$, two additional resonances, and a non-resonant component is performed. The mass and width of the first additional state are $(4219.7\\pm2.5\\pm4.5) \\rm{MeV}/\\rm{c}^2$ and $(80.7\\pm4.4\\pm1.4) \\rm{MeV}$, respectively, consistent with the $\u03c8(4230)$. For the second state, the mass and width are $(4386\\pm13\\pm17) \\rm{MeV}/\\rm{c}^2$ and $(177\\pm32\\pm13) \\rm{MeV}$, respectively, consistent with the $\u03c8(4360)$. The first uncertainties are statistical and the second ones are systematic. The statistical significance of $\u03c8(4040)$ is $8.0\u03c3$ and those for $\u03c8(4230)$ and $\u03c8(4360)$ are more than $10.0\u03c3$.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03361", "tags": []}, {"title": "Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring", "authors": ["Jia Syuen Lim", "Ziwei Wang", "Jiajun Liu", "Abdelwahed Khamis", "Reza Arablouei", "Robert Barlow", "Ryan McAllister"], "abstract": "Regulatory compliance auditing across diverse industrial domains requires heightened quality assurance and traceability. Present manual and intermittent approaches to such auditing yield significant challenges, potentially leading to oversights in the monitoring process. To address these issues, we introduce a real-time, multi-modal sensing system employing 3D time-of-flight and RGB cameras, coupled with unsupervised learning techniques on edge AI devices. This enables continuous object tracking thereby enhancing efficiency in record-keeping and minimizing manual interventions. While we validate the system in a knife sanitization context within agrifood facilities, emphasizing its prowess against occlusion and low-light issues with RGB cameras, its potential spans various industrial monitoring settings.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03333", "tags": []}, {"title": "Benchmarking Large Language Models As AI Research Agents", "authors": ["Qian Huang", "Jian Vora", "Percy Liang", "Jure Leskovec"], "abstract": "Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-based research agent to automatically perform experimentation loops in such an environment. Empirically, we find that a GPT-4-based research agent can feasibly build compelling ML models over many tasks in MLAgentBench, displaying highly interpretable plans and actions. However, the success rates vary considerably; they span from almost 90\\% on well-established older datasets to as low as 10\\% on recent Kaggle Challenges -- unavailable during the LLM model's pretraining -- and even 0\\% on newer research challenges like BabyLM. Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination. Our code is released at https://github.com/snap-stanford/MLAgentBench.", "submitted": "2023-10-05", "link": "https://arxiv.org/pdf/2310.03302", "tags": ["Natural Language Processing"]}, {"title": "Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization", "authors": ["Quanqi Hu", "Dixian Zhu", "Tianbao Yang"], "abstract": "This paper investigates new families of compositional optimization problems, called $\\underline{\\bf n}$on-$\\underline{\\bf s}$mooth $\\underline{\\bf w}$eakly-$\\underline{\\bf c}$onvex $\\underline{\\bf f}$inite-sum $\\underline{\\bf c}$oupled $\\underline{\\bf c}$ompositional $\\underline{\\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\u03b5$-stationary point of the Moreau envelop of the objective function. Additionally, we also extend the algorithm to solving novel non-smooth weakly-convex tri-level finite-sum coupled compositional optimization problems, which feature a nested arrangement of three functions. Lastly, we explore the applications of our algorithms in deep learning for two-way partial AUC maximization and multi-instance two-way partial AUC maximization, using empirical studies to showcase the effectiveness of the proposed algorithms.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.03234", "tags": []}, {"title": "Generative AI in the Classroom: Can Students Remain Active Learners?", "authors": ["Rania Abdelghani", "H\u00e9l\u00e8ne Sauz\u00e9on", "Pierre-Yves Oudeyer"], "abstract": "Generative Artificial Intelligence (GAI) has high potential to help address a diversity of educational challenges. In principle, GAI could facilitate the implementation of interactive and empowering pedagogical activities to complement the standard teaching strategies and favor students active engagement, understanding and control over their learning processes. These dimensions are indeed fundamental for a better learning experience and longer-lasting cognitive outcomes. However, several characteristics of the interactions with GAI such as continuous confidence in the generated answers, and the lack of pedagogical stance in their behavior may lead students to poor states of control over learning (e.g. over-reliance on pre-generated content, over-estimation of one's own knowledge, loss of curious and critical-thinking sense, etc).\n  The fine line between the two settings seems to lie in how this technology is used to carry out the pedagogical activities (e.g. types of interactions allowed, level of controllability by students, level of involvement of educators, etc) as well as to what extent students have the relevant skills (cognitive, metacognitive and GAI literacy) that allow them to correctly evaluate, analyze and interpret the system behaviors.\n  In this context, this article proposes to identify some of the opportunities and challenges that could arise wrt students control over their learning when using GAI during formal pedagogical activities. In a second step, we also discuss the types of trainings that could be relevant to offer students in order to provide them with the appropriate set of skills that can help them use GAI in informed ways, when pursuing a given learning goal.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.03192", "tags": []}, {"title": "Assessment of Prediction Intervals Using Uncertainty Characteristics Curves", "authors": ["Jiri Navratil", "Benjamin Elder", "Matthew Arnold", "Soumya Ghosh", "Prasanna Sattigeri"], "abstract": "Accurate quantification of model uncertainty has long been recognized as a fundamental requirement for trusted AI. In regression tasks, uncertainty is typically quantified using prediction intervals calibrated to an ad-hoc operating point, making evaluation and comparison across different studies relatively difficult. Our work leverages: (1) the concept of operating characteristics curves and (2) the notion of a gain over a null reference, to derive a novel operating point agnostic assessment methodology for prediction intervals. The paper defines the Uncertainty Characteristics Curve and demonstrates its utility in selected scenarios. We argue that the proposed method addresses the current need for comprehensive assessment of prediction intervals and thus represents a valuable addition to the uncertainty quantification toolbox.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.03158", "tags": []}, {"title": "Towards out-of-distribution generalizable predictions of chemical kinetics properties", "authors": ["Zihao Wang", "Yongqiang Chen", "Yang Duan", "Weijiang Li", "Bo Han", "James Cheng", "Hanghang Tong"], "abstract": "Machine Learning (ML) techniques have found applications in estimating chemical kinetics properties. With the accumulated drug molecules identified through \"AI4drug discovery\", the next imperative lies in AI-driven design for high-throughput chemical synthesis processes, with the estimation of properties of unseen reactions with unexplored molecules. To this end, the existing ML approaches for kinetics property prediction are required to be Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD kinetic property prediction into three levels (structure, condition, and mechanism), revealing unique aspects of such problems. Under this framework, we create comprehensive datasets to benchmark (1) the state-of-the-art ML approaches for reaction prediction in the OOD setting and (2) the state-of-the-art graph OOD methods in kinetics property prediction problems. Our results demonstrated the challenges and opportunities in OOD kinetics property prediction. Our datasets and benchmarks can further support research in this direction.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.03152", "tags": []}, {"title": "Shielding the Unseen: Privacy Protection through Poisoning NeRF with Spatial Deformation", "authors": ["Yihan Wu", "Brandon Y. Feng", "Heng Huang"], "abstract": "In this paper, we introduce an innovative method of safeguarding user privacy against the generative capabilities of Neural Radiance Fields (NeRF) models. Our novel poisoning attack method induces changes to observed views that are imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to accurately reconstruct a 3D scene. To achieve this, we devise a bi-level optimization algorithm incorporating a Projected Gradient Descent (PGD)-based spatial deformation. We extensively test our approach on two common NeRF benchmark datasets consisting of 29 real-world scenes with high-quality images. Our results compellingly demonstrate that our privacy-preserving method significantly impairs NeRF's performance across these benchmark datasets. Additionally, we show that our method is adaptable and versatile, functioning across various perturbation strengths and NeRF architectures. This work offers valuable insights into NeRF's vulnerabilities and emphasizes the need to account for such potential privacy risks when developing robust 3D scene reconstruction algorithms. Our study contributes to the larger conversation surrounding responsible AI and generative machine learning, aiming to protect user privacy and respect creative ownership in the digital age.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.03125", "tags": ["AI and Privacy"]}, {"title": "Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data", "authors": ["Rabia Gondur", "Usama Bin Sikandar", "Evan Schaffer", "Mikio Christian Aoi", "Stephen L Keeley"], "abstract": "Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional time-series data, they are typically only designed for a single type of data, making it difficult to identify structure shared across different experimental data modalities. Here, we address this shortcoming by proposing an unsupervised LVM which extracts temporally evolving shared and independent latents for distinct, simultaneously recorded experimental modalities. We do this by combining Gaussian Process Factor Analysis (GPFA), an interpretable LVM for neural spiking data with temporally smooth latent space, with Gaussian Process Variational Autoencoders (GP-VAEs), which similarly use a GP prior to characterize correlations in a latent space, but admit rich expressivity due to a deep neural network mapping to observations. We achieve interpretability in our model by partitioning latent variability into components that are either shared between or independent to each modality. We parameterize the latents of our model in the Fourier domain, and show improved latent identification using this approach over standard GP-VAE methods. We validate our model on simulated multi-modal data consisting of Poisson spike counts and MNIST images that scale and rotate smoothly over time. We show that the multi-modal GP-VAE (MM-GPVAE) is able to not only identify the shared and independent latent structure across modalities accurately, but provides good reconstructions of both images and neural rates on held-out trials. Finally, we demonstrate our framework on two real world multi-modal experimental settings: Drosophila whole-brain calcium imaging alongside tracked limb positions, and Manduca sexta spike train measurements from ten wing muscles as the animal tracks a visual stimulus.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.03111", "tags": ["Deep Learning"]}, {"title": "Brain development dictates energy constraints on neural architecture search: cross-disciplinary insights on optimization strategies", "authors": ["Martin G. Frasch"], "abstract": "Present day artificial neural architecture search (NAS) strategies are essentially prediction-error-optimized. That holds true for AI functions in general. From the developmental neuroscience perspective, I present evidence for the central role of metabolically, rather than prediction-error-optimized neural architecture search (NAS). Supporting evidence is drawn from the latest insights into the glial-neural organization of the human brain and the dynamic coordination theory which provides a mathematical foundation for the functional expression of this optimization strategy. This is relevant to devising novel NAS strategies in AI, especially in AGI. Additional implications arise for causal reasoning from deep neural nets. Together, the insights from developmental neuroscience offer a new perspective on NAS and the foundational assumptions in AI modeling.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.03042", "tags": []}, {"title": "SemiReward: A General Reward Model for Semi-supervised Learning", "authors": ["Siyuan Li", "Weiyang Jin", "Zedong Wang", "Fang Wu", "Zicheng Liu", "Cheng Tan", "Stan Z. Li"], "abstract": "Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves significant performance gains and faster convergence speeds upon Pseudo Label, FlexMatch, and Free/SoftMatch.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.03013", "tags": []}, {"title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference", "authors": ["Siddharth Samsi", "Dan Zhao", "Joseph McDonald", "Baolin Li", "Adam Michaleas", "Michael Jones", "William Bergeron", "Jeremy Kepner", "Devesh Tiwari", "Vijay Gadepally"], "abstract": "Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs -- despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.\n  In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta AI on two generations of popular GPUs (NVIDIA V100 \\& A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. We present the results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. To our knowledge, our work is the one of the first to study LLM inference performance from the perspective of computational and energy resources at this scale.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.03003", "tags": ["Natural Language Processing", "Large scale Machine Learning"]}, {"title": "Are LLMs Useful in the Poorest Schools? theTeacherAI in Sierra Leone", "authors": ["Jun Ho Choi", "Oliver Garrod", "Paul Atherton", "Andrew Joyce-Gibbons", "Miriam Mason-Sesay", "Daniel Bj\u00f6rkegren"], "abstract": "Education systems in developing countries have few resources to serve large, poor populations. How might generative AI integrate into classrooms? This paper introduces an AI chatbot designed to assist teachers in Sierra Leone with professional development to improve their instruction. We describe initial findings from early implementation across 122 schools and 193 teachers, and analyze its use with qualitative observations and by analyzing queries. Teachers use the system for lesson planning, classroom management, and subject matter. A subset of teachers use the system intensively. We draw conclusions from these findings about how generative AI systems can be integrated into school systems in low income countries.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02982", "tags": ["AI in Education", "Ethical AI and Bias Mitigation"]}, {"title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models", "authors": ["Xianjun Yang", "Xiao Wang", "Qi Zhang", "Linda Petzold", "William Yang Wang", "Xun Zhao", "Dahua Lin"], "abstract": "Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5 different organizations (LLaMa-2, Falcon, InternLM, BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack. Besides, the single-turn English-only attack successfully transfers to multi-turn dialogue and other languages. This study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source LLMs against malicious attackers.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02949", "tags": ["Natural Language Processing"]}, {"title": "Assessing Large Language Models on Climate Information", "authors": ["Jannis Bulian", "Mike S. Sch\u00e4fer", "Afra Amini", "Heidi Lam", "Massimiliano Ciaramita", "Ben Gaiarin", "Michelle Chen Huebscher", "Christian Buck", "Niels Mede", "Markus Leippold", "Nadine Strauss"], "abstract": "Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct a comprehensive analysis of the results, shedding light on both the potential and the limitations of LLMs in the realm of climate communication.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02932", "tags": ["AI in Education"]}, {"title": "Notes on a Path to AI Assistance in Mathematical Reasoning", "authors": ["Alex Kontorovich"], "abstract": "These informal notes are based on the author's lecture at the National Academies of Science, Engineering, and Mathematics workshop on \"AI to Assist Mathematical Reasoning\" in June 2023. The goal is to think through a path by which we might arrive at AI that is useful for the research mathematician.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02896", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric", "authors": ["Shiyun Wa", "Xinai Lu", "Minjuan Wang"], "abstract": "As Artificial Intelligence (AI) integrates deeper into diverse sectors, the quest for powerful models has intensified. While significant strides have been made in boosting model capabilities and their applicability across domains, a glaring challenge persists: many of these state-of-the-art models remain as black boxes. This opacity not only complicates the explanation of model decisions to end-users but also obstructs insights into intermediate processes for model designers. To address these challenges, we introduce InterpreTabNet, a model designed to enhance both classification accuracy and interpretability by leveraging the TabNet architecture with an improved attentive module. This design ensures robust gradient propagation and computational stability. Additionally, we present a novel evaluation metric, InterpreStability, which quantifies the stability of a model's interpretability. The proposed model and metric mark a significant stride forward in explainable models' research, setting a standard for transparency and interpretability in AI model design and application across diverse sectors. InterpreTabNet surpasses other leading solutions in tabular data analysis across varied application scenarios, paving the way for further research into creating deep-learning models that are both highly accurate and inherently explainable. The introduction of the InterpreStability metric ensures that the interpretability of future models can be measured and compared in a consistent and rigorous manner. Collectively, these contributions have the potential to promote the design principles and development of next-generation interpretable AI models, widening the adoption of interpretable AI solutions in critical decision-making environments.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02870", "tags": ["AI in Healthcare", "Explainable AI (XAI):"]}, {"title": "uTalk: Bridging the Gap Between Humans and AI", "authors": ["Hussam Azzuni", "Sharim Jamal", "Abdulmotaleb Elsaddik"], "abstract": "Large Language Models (LLMs) have revolutionized various industries by harnessing their power to improve productivity and facilitate learning across different fields. One intriguing application involves combining LLMs with visual models to create a novel approach to Human-Computer Interaction. The core idea behind this system is to develop an interactive platform that allows the general public to leverage the capabilities of ChatGPT in their daily lives. This is achieved by integrating several technologies such as Whisper, ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking head system, SadTalker, resulting in uTalk, an intelligent AI system. Users will be able to converse with this portrait, receiving answers to whatever questions they have in mind. Additionally, they could use uTalk for content generation by providing an input and their image. This system is hosted on Streamlit, where the user will initially be requested to provide an image to serve as their AI assistant. Then, users could choose whether to have a conversation or generate content based on their preferences. Either way, it starts by providing an input, where a set of operations will be done, and the avatar will provide a precise response. The paper discusses how SadTalker is optimized to improve its running time by 27.72% based on 25FPS generated videos. In addition, the system's initial performance, uTalk, improved further by 9.8% after SadTalker was integrated and parallelized with Streamlit.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02739", "tags": ["AI in Education", "Ethical AI and Bias Mitigation"]}, {"title": "Functional trustworthiness of AI systems by statistically valid testing", "authors": ["Bernhard Nessler", "Thomas Doms", "Sepp Hochreiter"], "abstract": "The authors are concerned about the safety, health, and rights of the European citizens due to inadequate measures and procedures required by the current draft of the EU Artificial Intelligence (AI) Act for the conformity assessment of AI systems. We observe that not only the current draft of the EU AI Act, but also the accompanying standardization efforts in CEN/CENELEC, have resorted to the position that real functional guarantees of AI systems supposedly would be unrealistic and too complex anyways. Yet enacting a conformity assessment procedure that creates the false illusion of trust in insufficiently assessed AI systems is at best naive and at worst grossly negligent. The EU AI Act thus misses the point of ensuring quality by functional trustworthiness and correctly attributing responsibilities.\n  The trustworthiness of an AI decision system lies first and foremost in the correct statistical testing on randomly selected samples and in the precision of the definition of the application domain, which enables drawing samples in the first place. We will subsequently call this testable quality functional trustworthiness. It includes a design, development, and deployment that enables correct statistical testing of all relevant functions.\n  We are firmly convinced and advocate that a reliable assessment of the statistical functional properties of an AI system has to be the indispensable, mandatory nucleus of the conformity assessment. In this paper, we describe the three necessary elements to establish a reliable functional trustworthiness, i.e., (1) the definition of the technical distribution of the application, (2) the risk-based minimum performance requirements, and (3) the statistically valid testing based on independent random samples.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02727", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy"]}, {"title": "PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting", "authors": ["Yujin Tang", "Jiaming Zhou", "Xiang Pan", "Zeying Gong", "Junwei Liang"], "abstract": "Accurate precipitation forecasting is a vital challenge of both scientific and societal importance. Data-driven approaches have emerged as a widely used solution for addressing this challenge. However, solely relying on data-driven approaches has limitations in modeling the underlying physics, making accurate predictions difficult. Coupling AI-based post-processing techniques with traditional Numerical Weather Prediction (NWP) methods offers a more effective solution for improving forecasting accuracy. Despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention Enhanced Multi-task Learning framework with a specially designed weighted loss function. Its flexible design allows for easy plug-and-play integration with various backbones. Extensive experimental results on the proposed benchmark show that our method outperforms state-of-the-art methods by 6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most notably, our model is the first deep learning-based method to outperform traditional Numerical Weather Prediction (NWP) approaches in extreme precipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over NWP predictions in heavy rain CSI on respective datasets. These results highlight the potential impact of our model in reducing the severe consequences of extreme weather events.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02676", "tags": ["Computer Vision", "Deep Learning"]}, {"title": "A ModelOps-based Framework for Intelligent Medical Knowledge Extraction", "authors": ["Hongxin Ding", "Peinie Zou", "Zhiyuan Wang", "Junfeng Zhao", "Yasha Wang", "Qiang Zhou"], "abstract": "Extracting medical knowledge from healthcare texts enhances downstream tasks like medical knowledge graph construction and clinical decision-making. However, the construction and application of knowledge extraction models lack automation, reusability and unified management, leading to inefficiencies for researchers and high barriers for non-AI experts such as doctors, to utilize knowledge extraction. To address these issues, we propose a ModelOps-based intelligent medical knowledge extraction framework that offers a low-code system for model selection, training, evaluation and optimization. Specifically, the framework includes a dataset abstraction mechanism based on multi-layer callback functions, a reusable model training, monitoring and management mechanism. We also propose a model recommendation method based on dataset similarity, which helps users quickly find potentially suitable models for a given dataset. Our framework provides convenience for researchers to develop models and simplifies model access for non-AI experts such as doctors.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.02593", "tags": ["AI in Healthcare", "Natural Language Processing"]}, {"title": "CITING: Large Language Models Create Curriculum for Instruction Tuning", "authors": ["Tao Feng", "Zifeng Wang", "Jimeng Sun"], "abstract": "The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to embody the procedure of CITING. We compare CITING to a series of state-of-the-art baselines on four datasets. Our method demonstrates strong improvement in terms of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically, it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1% over RRHF, and 76.3% over RAFT, respectively.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02527", "tags": ["AI in Education"]}, {"title": "Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem", "authors": ["Sasha Costanza-Chock", "Emma Harvey", "Inioluwa Deborah Raji", "Martha Czernuszenko", "Joy Buolamwini"], "abstract": "AI audits are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02521", "tags": ["Ethical AI and Bias Mitigation"]}, {"title": "Proactive Human-Robot Interaction using Visuo-Lingual Transformers", "authors": ["Pranay Mathur"], "abstract": "Humans possess the innate ability to extract latent visuo-lingual cues to infer context through human interaction. During collaboration, this enables proactive prediction of the underlying intention of a series of tasks. In contrast, robotic agents collaborating with humans naively follow elementary instructions to complete tasks or use specific hand-crafted triggers to initiate proactive collaboration when working towards the completion of a goal. Endowing such robots with the ability to reason about the end goal and proactively suggest intermediate tasks will engender a much more intuitive method for human-robot collaboration. To this end, we propose a learning-based method that uses visual cues from the scene, lingual commands from a user and knowledge of prior object-object interaction to identify and proactively predict the underlying goal the user intends to achieve. Specifically, we propose ViLing-MMT, a vision-language multimodal transformer-based architecture that captures inter and intra-modal dependencies to provide accurate scene descriptions and proactively suggest tasks where applicable. We evaluate our proposed model in simulation and real-world scenarios.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02506", "tags": []}, {"title": "Low-Resource Languages Jailbreak GPT-4", "authors": ["Zheng-Xin Yong", "Cristina Menghini", "Stephen H. Bach"], "abstract": "AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs' safety vulnerabilities. Therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02446", "tags": ["Natural Language Processing"]}, {"title": "Delta-AI: Local objectives for amortized inference in sparse graphical models", "authors": ["Jean-Pierre Falet", "Hae Beom Lee", "Nikolay Malkin", "Chen Sun", "Dragos Secrieru", "Dinghuai Zhang", "Guillaume Lajoie", "Yoshua Bengio"], "abstract": "We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call $\u0394$-amortized inference ($\u0394$-AI). Our approach is based on the observation that when the sampling of variables in a PGM is seen as a sequence of actions taken by an agent, sparsity of the PGM enables local credit assignment in the agent's policy learning objective. This yields a local constraint that can be turned into a local loss in the style of generative flow networks (GFlowNets) that enables off-policy training but avoids the need to instantiate all the random variables for each parameter update, thus speeding up training considerably. The $\u0394$-AI objective matches the conditional distribution of a variable given its Markov blanket in a tractable learned sampler, which has the structure of a Bayesian network, with the same conditional distribution under the target PGM. As such, the trained sampler recovers marginals and conditional distributions of interest and enables inference of partial subsets of variables. We illustrate $\u0394$-AI's effectiveness for sampling from synthetic PGMs and training latent variable models with sparse factor structure.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02423", "tags": []}, {"title": "Autonomous Systems' Safety Cases for use in UK Nuclear Environments", "authors": ["Christopher R. Anderson", "Louise A. Dennis"], "abstract": "An overview of the process to develop a safety case for an autonomous robot deployment on a nuclear site in the UK is described and a safety case for a hypothetical robot incorporating AI is presented. This forms a first step towards a deployment, showing what is possible now and what may be possible with development of tools. It forms the basis for further discussion between nuclear site licensees, the Office for Nuclear Regulation (ONR), industry and academia.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02344", "tags": []}, {"title": "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation", "authors": ["Eric Zelikman", "Eliana Lorch", "Lester Mackey", "Adam Tauman Kalai"], "abstract": "Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a \"scaffolding\" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed \"improver\" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our proof-of-concept experiments, is capable of writing code that can call itself to improve itself. We critically consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02304", "tags": ["Natural Language Processing"]}, {"title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts", "authors": ["Pan Lu", "Hritik Bansal", "Tony Xia", "Jiacheng Liu", "Chunyuan Li", "Hannaneh Hajishirzi", "Hao Cheng", "Kai-Wei Chang", "Michel Galley", "Jianfeng Gao"], "abstract": "Although Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive skills in various domains, their ability for mathematical reasoning within visual contexts has not been formally examined. Equipping LLMs and LMMs with this capability is vital for general-purpose AI assistants and showcases promising potential in education, data analysis, and scientific discovery. To bridge this gap, we present MathVista, a benchmark designed to amalgamate challenges from diverse mathematical and visual tasks. We first taxonomize the key task types, reasoning skills, and visual contexts from the literature to guide our selection from 28 existing math-focused and visual question answering datasets. Then, we construct three new datasets, IQTest, FunctionQA, and PaperQA, to accommodate for missing types of visual contexts. The problems featured often require deep visual understanding beyond OCR or image captioning, and compositional reasoning with rich domain-specific tools, thus posing a notable challenge to existing models. We conduct a comprehensive evaluation of 11 prominent open-source and proprietary foundation models (LLMs, LLMs augmented with tools, and LMMs), and early experiments with GPT-4V. The best-performing model, Multimodal Bard, achieves only 58% of human performance (34.8% vs 60.3%), indicating ample room for further improvement. Given this significant gap, MathVista fuels future research in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. Preliminary tests show that MathVista also presents challenges to GPT-4V, underscoring the benchmark's importance. The project is available at https://mathvista.github.io/.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02255", "tags": ["Natural Language Processing"]}, {"title": "A Survey of Graph Unlearning", "authors": ["Anwar Said", "Tyler Derr", "Mudassir Shabbir", "Waseem Abbas", "Xenofon Koutsoukos"], "abstract": "Graph unlearning emerges as a crucial advancement in the pursuit of responsible AI, providing the means to remove sensitive data traces from trained models, thereby upholding the right to be forgotten. It is evident that graph machine learning exhibits sensitivity to data privacy and adversarial attacks, necessitating the application of graph unlearning techniques to address these concerns effectively. In this comprehensive survey paper, we present the first systematic review of graph unlearning approaches, encompassing a diverse array of methodologies and offering a detailed taxonomy and up-to-date literature overview to facilitate the understanding of researchers new to this field. Additionally, we establish the vital connections between graph unlearning and differential privacy, augmenting our understanding of the relevance of privacy-preserving techniques in this context. To ensure clarity, we provide lucid explanations of the fundamental concepts and evaluation measures used in graph unlearning, catering to a broader audience with varying levels of expertise. Delving into potential applications, we explore the versatility of graph unlearning across various domains, including but not limited to social networks, adversarial settings, and resource-constrained environments like the Internet of Things (IoT), illustrating its potential impact in safeguarding data privacy and enhancing AI systems' robustness. Finally, we shed light on promising research directions, encouraging further progress and innovation within the domain of graph unlearning. By laying a solid foundation and fostering continued progress, this survey seeks to inspire researchers to further advance the field of graph unlearning, thereby instilling confidence in the ethical growth of AI systems and reinforcing the responsible application of machine learning techniques in various domains.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.02164", "tags": ["AI and Privacy"]}, {"title": "Learning Reliable Logical Rules with SATNet", "authors": ["Zhaoyu Li", "Jinpei Guo", "Yuhe Jiang", "Xujie Si"], "abstract": "Bridging logical reasoning and deep learning is crucial for advanced AI systems. In this work, we present a new framework that addresses this goal by generating interpretable and verifiable logical rules through differentiable learning, without relying on pre-specified logical structures. Our approach builds upon SATNet, a differentiable MaxSAT solver that learns the underlying rules from input-output examples. Despite its efficacy, the learned weights in SATNet are not straightforwardly interpretable, failing to produce human-readable rules. To address this, we propose a novel specification method called \"maximum equality\", which enables the interchangeability between the learned weights of SATNet and a set of propositional logical rules in weighted MaxSAT form. With the decoded weighted MaxSAT formula, we further introduce several effective verification techniques to validate it against the ground truth rules. Experiments on stream transformations and Sudoku problems show that our decoded rules are highly reliable: using exact solvers on them could achieve 100% accuracy, whereas the original SATNet fails to give correct solutions in many cases. Furthermore, we formally verify that our decoded logical rules are functionally equivalent to the ground truth ones.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02133", "tags": []}, {"title": "HPCClusterScape: Increasing Transparency and Efficiency of Shared High-Performance Computing Clusters for Large-scale AI Models", "authors": ["Heungseok Park", "Aeree Cho", "Hyojun Jeon", "Hayoung Lee", "Youngil Yang", "Sungjae Lee", "Heungsub Lee", "Jaegul Choo"], "abstract": "The emergence of large-scale AI models, like GPT-4, has significantly impacted academia and industry, driving the demand for high-performance computing (HPC) to accelerate workloads. To address this, we present HPCClusterScape, a visualization system that enhances the efficiency and transparency of shared HPC clusters for large-scale AI models. HPCClusterScape provides a comprehensive overview of system-level (e.g., partitions, hosts, and workload status) and application-level (e.g., identification of experiments and researchers) information, allowing HPC operators and machine learning researchers to monitor resource utilization and identify issues through customizable violation rules. The system includes diagnostic tools to investigate workload imbalances and synchronization bottlenecks in large-scale distributed deep learning experiments. Deployed in industrial-scale HPC clusters, HPCClusterScape incorporates user feedback and meets specific requirements. This paper outlines the challenges and prerequisites for efficient HPC operation, introduces the interactive visualization system, and highlights its contributions in addressing pain points and optimizing resource utilization in shared HPC clusters.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02120", "tags": []}, {"title": "Towards Effective Human-AI Decision-Making: The Role of Human Learning in Appropriate Reliance on AI Advice", "authors": ["Max Schemmer", "Andrea Bartos", "Philipp Spitzer", "Patrick Hemmer", "Niklas K\u00fchl", "Jonas Liebschner", "Gerhard Satzger"], "abstract": "The true potential of human-AI collaboration lies in exploiting the complementary capabilities of humans and AI to achieve a joint performance superior to that of the individual AI or human, i.e., to achieve complementary team performance (CTP). To realize this complementarity potential, humans need to exercise discretion in following AI 's advice, i.e., appropriately relying on the AI's advice. While previous work has focused on building a mental model of the AI to assess AI recommendations, recent research has shown that the mental model alone cannot explain appropriate reliance. We hypothesize that, in addition to the mental model, human learning is a key mediator of appropriate reliance and, thus, CTP. In this study, we demonstrate the relationship between learning and appropriate reliance in an experiment with 100 participants. This work provides fundamental concepts for analyzing reliance and derives implications for the effective design of human-AI decision-making.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02108", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy"]}, {"title": "Security Weaknesses of Copilot Generated Code in GitHub", "authors": ["Yujia Fu", "Peng Liang", "Amjed Tahir", "Zengyang Li", "Mojtaba Shahin", "Jiaxin Yu"], "abstract": "Modern code generation tools use AI models, particularly Large Language Models (LLMs), to generate functional and complete code. While such tools are becoming popular and widely available for developers, using these tools is often accompanied by security challenges. Therefore, it is important to assess the quality of the generated code, especially in terms of its security. Researchers have recently explored various aspects of code generation tools, including security. However, many open questions about the security of the generated code require further investigation, especially the security issues of automatically generated code in the wild. To this end, we conducted an empirical study by analyzing the security weaknesses in code snippets generated by GitHub Copilot that are found as part of publicly available projects hosted on GitHub. The goal is to investigate the types of security issues and their scale in real-world scenarios (rather than crafted scenarios). To this end, we identified 435 code snippets generated by Copilot from publicly available projects. We then conducted extensive security analysis to identify Common Weakness Enumeration (CWE) instances in these code snippets. The results show that (1) 35.8% of Copilot generated code snippets contain CWEs, and those issues are spread across multiple languages, (2) the security weaknesses are diverse and related to 42 different CWEs, in which CWE-78: OS Command Injection, CWE-330: Use of Insufficiently Random Values, and CWE-703: Improper Check or Handling of Exceptional Conditions occurred the most frequently, and (3) among the 42 CWEs identified, 11 of those belong to the currently recognized 2022 CWE Top-25. Our findings confirm that developers should be careful when adding code generated by Copilot (and similar AI code generation tools) and should also run appropriate security checks as they accept the suggested code.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02059", "tags": []}, {"title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model", "authors": ["Zibin Dong", "Yifu Yuan", "Jianye Hao", "Fei Ni", "Yao Mu", "Yan Zheng", "Yujing Hu", "Tangjie Lv", "Changjie Fan", "Zhipeng Hu"], "abstract": "Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RL from Human Feedback (RLHF) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director for preference aligning at the inference phase. We evaluate AlignDiff on various locomotion tasks and demonstrate its superior performance on preference matching, switching, and covering compared to other baselines. Its capability of completing unseen downstream tasks under human instructions also showcases the promising potential for human-AI collaboration. More visualization videos are released on https://aligndiff.github.io/.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02054", "tags": ["Reinforcement Learning"]}, {"title": "The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers", "authors": ["Rickard Br\u00e4nnvall"], "abstract": "To enhance the computational efficiency of quantized Transformers, we replace the dot-product and Softmax-based attention with an alternative mechanism involving addition and ReLU activation only. This side-steps the expansion to double precision often required by matrix multiplication and avoids costly Softmax evaluations but maintains much of the core functionality of conventional dot-product attention. It can enable more efficient execution and support larger quantized Transformer models on resource-constrained hardware or alternative arithmetic systems like homomorphic encryption. Training experiments on four common benchmark tasks show test set prediction scores comparable to those of conventional Transformers with dot-product attention. Our scaling experiments also suggest significant computational savings, both in plaintext and under encryption. In particular, we believe that the ReLU and addition-based attention mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the costly multiplication of encrypted variables.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02041", "tags": []}, {"title": "Between accurate prediction and poor decision making: the AI/ML gap", "authors": ["Gianluca Bontempi"], "abstract": "Intelligent agents rely on AI/ML functionalities to predict the consequence of possible actions and optimise the policy. However, the effort of the research community in addressing prediction accuracy has been so intense (and successful) that it created the illusion that the more accurate the learner prediction (or classification) the better would have been the final decision. Now, such an assumption is valid only if the (human or artificial) decision maker has complete knowledge of the utility of the possible actions. This paper argues that AI/ML community has taken so far a too unbalanced approach by devoting excessive attention to the estimation of the state (or target) probability to the detriment of accurate and reliable estimations of the utility. In particular, few evidence exists about the impact of a wrong utility assessment on the resulting expected utility of the decision strategy. This situation is creating a substantial gap between the expectations and the effective impact of AI solutions, as witnessed by recent criticisms and emphasised by the regulatory legislative efforts. This paper aims to study this gap by quantifying the sensitivity of the expected utility to the utility uncertainty and comparing it to the one due to probability estimation. Theoretical and simulated results show that an inaccurate utility assessment may as (and sometimes) more harmful than a poor probability estimation. The final recommendation to the community is then to undertake a focus shift from a pure accuracy-driven (or obsessed) approach to a more utility-aware methodology.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.02029", "tags": []}, {"title": "Online Multimedia Verification with Computational Tools and OSINT: Russia-Ukraine Conflict Case Studies", "authors": ["Sohail Ahmed Khan", "Jan Gunnar Furuly", "Henrik Brattli Vold", "Rano Tahseen", "Duc-Tien Dang-Nguyen"], "abstract": "This paper investigates the use of computational tools and Open-Source Intelligence (OSINT) techniques for verifying online multimedia content, with a specific focus on real-world cases from the Russia-Ukraine conflict. Over a nine-month period from April to December 2022, we examine verification workflows, tools, and case studies published by \\faktiskbar. Our study showcases the effectiveness of diverse resources, including AI tools, geolocation tools, internet archives, and social media monitoring platforms, in enabling journalists and fact-checkers to efficiently process and corroborate evidence, ensuring the dissemination of accurate information. This research underscores the vital role of computational tools and OSINT techniques in promoting evidence-based reporting and combatting misinformation. We also touch on the current limitations of available tools and prospects for future developments in multimedia verification.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01978", "tags": []}, {"title": "Steganalysis of AI Models LSB Attacks", "authors": ["Daniel Gilkarov", "Ran Dubin"], "abstract": "Artificial intelligence has made significant progress in the last decade, leading to a rise in the popularity of model sharing. The model zoo ecosystem, a repository of pre-trained AI models, has advanced the AI open-source community and opened new avenues for cyber risks. Malicious attackers can exploit shared models to launch cyber-attacks. This work focuses on the steganalysis of injected malicious Least Significant Bit (LSB) steganography into AI models, and it is the first work focusing on AI model attacks. In response to this threat, this paper presents a steganalysis method specifically tailored to detect and mitigate malicious LSB steganography attacks based on supervised and unsupervised AI detection steganalysis methods. Our proposed technique aims to preserve the integrity of shared models, protect user trust, and maintain the momentum of open collaboration within the AI community. In this work, we propose 3 steganalysis methods and open source our code. We found that the success of the steganalysis depends on the LSB attack location. If the attacker decides to exploit the least significant bits in the LSB, the ability to detect the attacks is low. However, if the attack is in the most significant LSB bits, the attack can be detected with almost perfect accuracy.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01969", "tags": ["AI in Healthcare"]}, {"title": "Artificial Intelligence for Prediction of Climate Extremes: State of the art, challenges and future perspectives", "authors": ["Stefano Materia", "Llu\u00eds Palma Garc\u00eda", "Chiem van Straaten", "Sungmin O", "Antonios Mamalakis", "Leone Cavicchia", "Dim Coumou", "Paolo De Luca", "Marlene Kretschmer", "Markus G. Donat"], "abstract": "Scientific and technological advances in numerical modelling have improved the quality of climate predictions over recent decades, but predictive skill remains limited in many aspects. Extreme events such as heat and cold waves, droughts, heavy rain and storms are particularly challenging to predict accurately due to their rarity and non-linear chaotic nature, and because of model limitations. However, recent studies have shown that predictive skill of extremes can be increased when using more sophisticated approaches, indicating that there might be systemic predictability that is not being leveraged. Recently, numerous studies have been devoted to the exploitation of Artificial Intelligence (AI) to study the predictability and make predictions of weather and climate. AI techniques have shown great potential to improve the prediction of extreme events and uncover their links to large-scale and local drivers. Machine and deep learning, causal discovery, explainable AI, are only some of the approaches that have been tested to both improve our understanding of the processes underlying predictability and enhance prediction skill of extreme events. Results are promising, especially for hybrid predictions that combine the AI, which can reveal and exploit unknown spatio-temporal connections from data, and climate models, that provide the theoretical foundation and interpretability of the physical world. On the other hand, challenges are multiple in many aspects, from data curation to model uncertainty and generalizability, to the reproducibility of methods and workflows. A few best practices are identified to increase trust in these novel techniques, and future perspectives are envisaged for further scientific development.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01944", "tags": ["AI in Healthcare", "AI and Privacy", "Ethical AI and Bias Mitigation"]}, {"title": "Hierarchical Evaluation Framework: Best Practices for Human Evaluation", "authors": ["Iva Bojic", "Jessica Chen", "Si Yuan Chang", "Qi Chwen Ong", "Shafiq Joty", "Josip Car"], "abstract": "Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system's performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate both components rather than solely focusing on outputs. In future work, we will investigate the potential time-saving benefits of our proposed framework for evaluators assessing NLP systems.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.01917", "tags": []}, {"title": "Verified completeness in Henkin-style for intuitionistic propositional logic", "authors": ["Huayu Guo", "Dongheng Chen", "Bruno Bentzen"], "abstract": "This paper presents a formalization of the classical proof of completeness in Henkin-style developed by Troelstra and van Dalen for intuitionistic logic with respect to Kripke models. The completeness proof incorporates their insights in a fresh and elegant manner that is better suited for mechanization. We discuss details of our implementation in the Lean theorem prover with emphasis on the prime extension lemma and construction of the canonical model. Our implementation is restricted to a system of intuitionistic propositional logic with implication, conjunction, disjunction, and falsity given in terms of a Hilbert-style axiomatization. As far as we know, our implementation is the first verified Henkin-style proof of completeness for intuitionistic logic following Troelstra and van Dalen's method in the literature. The full source code can be found online at https://github.com/bbentzen/ipl.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01916", "tags": []}, {"title": "Beyond the Benchmark: Detecting Diverse Anomalies in Videos", "authors": ["Yoav Arad", "Michael Werman"], "abstract": "Video Anomaly Detection (VAD) plays a crucial role in modern surveillance systems, aiming to identify various anomalies in real-world situations. However, current benchmark datasets predominantly emphasize simple, single-frame anomalies such as novel object detection. This narrow focus restricts the advancement of VAD models. In this research, we advocate for an expansion of VAD investigations to encompass intricate anomalies that extend beyond conventional benchmark boundaries. To facilitate this, we introduce two datasets, HMDB-AD and HMDB-Violence, to challenge models with diverse action-based anomalies. These datasets are derived from the HMDB51 action recognition dataset. We further present Multi-Frame Anomaly Detection (MFAD), a novel method built upon the AI-VAD framework. AI-VAD utilizes single-frame features such as pose estimation and deep image encoding, and two-frame features such as object velocity. They then apply a density estimation algorithm to compute anomaly scores. To address complex multi-frame anomalies, we add a deep video encoding features capturing long-range temporal dependencies, and logistic regression to enhance final score calculation. Experimental results confirm our assumptions, highlighting existing models limitations with new anomaly types. MFAD excels in both simple and complex anomaly detection scenarios.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01904", "tags": []}, {"title": "Ring Attention with Blockwise Transformers for Near-Infinite Context", "authors": ["Hao Liu", "Matei Zaharia", "Pieter Abbeel"], "abstract": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while overlapping the communication of key-value blocks with the computation of blockwise attention. Ring Attention enables training and inference of sequences that are up to device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effectiveness of Ring Attention in allowing large sequence input size and improving performance.", "submitted": "2023-10-11", "link": "https://arxiv.org/pdf/2310.01889", "tags": []}, {"title": "Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation", "authors": ["Abdul Karim Gizzini", "Mustafa Shukor", "Ali J. Ghandour"], "abstract": "Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite images. To benchmark and compare the performance of the proposed approaches, we introduce a new XAI evaluation methodology and metric based on \"Entropy\" to measure the model uncertainty. Conventional XAI evaluation methods rely mainly on feeding area-of-interest regions from the image back to the pre-trained (utility) model and then calculating the average change in the probability of the target class. Those evaluation metrics lack the needed robustness, and we show that using Entropy to monitor the model uncertainty in segmenting the pixels within the target class is more suitable. We hope this work will pave the way for additional XAI research for image segmentation and applications in the remote sensing discipline.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01837", "tags": ["Explainable AI (XAI):"]}, {"title": "AI-Generated Images as Data Source: The Dawn of Synthetic Era", "authors": ["Zuhao Yang", "Fangneng Zhan", "Kunhao Liu", "Muyu Xu", "Shijian Lu"], "abstract": "The advancement of visual intelligence is intrinsically tethered to the availability of data. In parallel, generative Artificial Intelligence (AI) has unlocked the potential to create synthetic images that closely resemble real-world photographs, which prompts a compelling inquiry: how visual intelligence benefit from the advance of generative AI? This paper explores the innovative concept of harnessing these AI-generated images as a new data source, reshaping traditional model paradigms in visual intelligence. In contrast to real data, AI-generated data sources exhibit remarkable advantages, including unmatched abundance and scalability, the rapid generation of vast datasets, and the effortless simulation of edge cases. Built on the success of generative AI models, we examines the potential of their generated data in a range of applications, from training machine learning models to simulating scenarios for computational modeling, testing, and validation. We probe the technological foundations that support this groundbreaking use of generative AI, engaging in an in-depth discussion on the ethical, legal, and practical considerations that accompany this transformative paradigm shift. Through an exhaustive survey of current technologies and applications, this paper presents a comprehensive view of the synthetic era in visual intelligence. A project associated with this paper can be found at https://github.com/mwxely/AIGS .", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.01830", "tags": ["AI and Privacy", "Ethical AI and Bias Mitigation"]}, {"title": "Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation", "authors": ["Hossein Shreim", "Abdul Karim Gizzini", "Ali J. Ghandour"], "abstract": "eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for image segmentation. This paper adapts the recent gradient-free Sobol XAI method for semantic segmentation. To measure the performance of the Sobol method for segmentation, we propose a quantitative XAI evaluation method based on a learnable noise model. The main objective of this model is to induce noise on the explanation maps, where higher induced noise signifies low accuracy and vice versa. A benchmark analysis is conducted to evaluate and compare performance of three XAI methods, including Seg-Grad-CAM, Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation technique. This constitutes the first attempt to run and evaluate XAI methods using high-resolution satellite images.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01828", "tags": ["Explainable AI (XAI):"]}, {"title": "Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI", "authors": ["Emily Jin", "Jiaheng Hu", "Zhuoyi Huang", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei", "Roberto Mart\u00edn-Mart\u00edn"], "abstract": "We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and development of solutions, simplifying their assessment and development while advancing the field of embodied AI. Code is publicly available at https://github.com/StanfordVL/mini_behavior.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01824", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "High angular momentum coupling for enhanced Rydberg-atom sensing in the VHF band", "authors": ["Nikunjkumar Prajapati", "Jakob W. Kunzler", "Alexandra B. Artusio-Glimpse", "Andrew Rotunno", "Samuel Berweger", "Matthew T. Simons", "Christopher L. Holloway", "Chad M. Gardner", "Michael S. Mcbeth", "Robert A. Younts"], "abstract": "Recent advances in Rydberg atom electrometry detail promising applications in radio frequency (RF) communications. Presently, most applications use carrier frequencies greater than 1~GHz where resonant Autler-Townes splitting provides the highest sensitivity. This letter documents a series of experiments with Rydberg atomic sensors to collect and process waveforms from the automated identification system (AIS) used in maritime navigation in the Very High Frequency (VHF) band. Detection in this band is difficult with conventional resonant Autler-Townes based Rydberg sensing and requires a new approach. We show the results from a new method called High Angular Momentum Matching Excited Raman (HAMMER), which enhances low frequency detection and exhibits superior sensitivity compared to the traditional AC Stark effect. From measurements of electromagnetically induced transparency (EIT) in rubidium and cesium vapor cells, we show the relationship between incident electric field strength and observed signal-to-noise ratio and find that the sensitivity of the HAMMER scheme in rubidium achieved an equivalent single VHF tone sensitivity of $\\mathrm{100~\u03bcV/m/\\sqrt{Hz}}$. With these results, we estimate the usable range of the atomic vapor cell antenna for AIS waveforms given current technology and detection techniques.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01810", "tags": []}, {"title": "Discrete, compositional, and symbolic representations through attractor dynamics", "authors": ["Andrew Nam", "Eric Elmoznino", "Nikolay Malkin", "Chen Sun", "Yoshua Bengio", "Guillaume Lajoie"], "abstract": "Compositionality is an important feature of discrete symbolic systems, such as language and programs, as it enables them to have infinite capacity despite a finite symbol set. It serves as a useful abstraction for reasoning in both cognitive science and in AI, yet the interface between continuous and symbolic processing is often imposed by fiat at the algorithmic level, such as by means of quantization or a softmax sampling step. In this work, we explore how discretization could be implemented in a more neurally plausible manner through the modeling of attractor dynamics that partition the continuous representation space into basins that correspond to sequences of symbols. Building on established work in attractor networks and introducing novel training methods, we show that imposing structure in the symbolic space can produce compositionality in the attractor-supported representation space of rich sensory inputs. Lastly, we argue that our model exhibits the process of an information bottleneck that is thought to play a role in conscious experience, decomposing the rich information of a sensory input into stable components encoding symbolic information.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01807", "tags": []}, {"title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis", "authors": ["Weixin Liang", "Yuhui Zhang", "Hancheng Cao", "Binglu Wang", "Daisy Ding", "Xinyu Yang", "Kailas Vodrahalli", "Siyu He", "Daniel Smith", "Yian Yin", "Daniel McFarland", "James Zou"], "abstract": "Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.", "submitted": "2023-10-03", "link": "https://arxiv.org/pdf/2310.01783", "tags": []}, {"title": "Differentially Encoded Observation Spaces for Perceptive Reinforcement Learning", "authors": ["Lev Grossman", "Brian Plancher"], "abstract": "Perceptive deep reinforcement learning (DRL) has lead to many recent breakthroughs for complex AI systems leveraging image-based input data. Applications of these results range from super-human level video game agents to dexterous, physically intelligent robots. However, training these perceptive DRL-enabled systems remains incredibly compute and memory intensive, often requiring huge training datasets and large experience replay buffers. This poses a challenge for the next generation of field robots that will need to be able to learn on the edge in order to adapt to their environments. In this paper, we begin to address this issue through differentially encoded observation spaces. By reinterpreting stored image-based observations as a video, we leverage lossless differential video encoding schemes to compress the replay buffer without impacting training performance. We evaluate our approach with three state-of-the-art DRL algorithms and find that differential image encoding reduces the memory footprint by as much as 14.2x and 16.7x across tasks from the Atari 2600 benchmark and the DeepMind Control Suite (DMC) respectively. These savings also enable large-scale perceptive DRL that previously required paging between flash and RAM to be run entirely in RAM, improving the latency of DMC tasks by as much as 32%.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01767", "tags": []}, {"title": "Exploring Counterfactual Alignment Loss towards Human-centered AI", "authors": ["Mingzhou Liu", "Xinwei Sun", "Ching-Wen Lee", "Yu Qiao", "Yizhou Wang"], "abstract": "Deep neural networks have demonstrated impressive accuracy in supervised learning tasks. However, their lack of transparency makes it hard for humans to trust their results, especially in safe-critic domains such as healthcare. To address this issue, recent explanation-guided learning approaches proposed to align the gradient-based attention map to image regions annotated by human experts, thereby obtaining an intrinsically human-centered model. However, the attention map these methods are based on may fail to causally attribute the model predictions, thus compromising their validity for alignment. To address this issue, we propose a novel human-centered framework based on counterfactual generation. In particular, we utilize the counterfactual generation's ability for causal attribution to introduce a novel loss called the CounterFactual Alignment (CF-Align) loss. This loss guarantees that the features attributed by the counterfactual generation for the classifier align with the human annotations. To optimize the proposed loss that entails a counterfactual generation with an implicit function form, we leverage the implicit function theorem for backpropagation. Our method is architecture-agnostic and, therefore can be applied to any neural network. We demonstrate the effectiveness of our method on a lung cancer diagnosis dataset, showcasing faithful alignment to humans.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01766", "tags": ["Computer Vision", "Generative Adversarial Networks"]}, {"title": "Health Guardian: Using Multi-modal Data to Understand Individual Health", "authors": ["Vince S. Siu", "Kuan Yu Hsieh", "Italo Buleje", "Takashi Itoh", "Tian Hao", "Ben Civjan", "Nigel Hinds", "Bing Dang", "Jeffrey L. Rogers", "Bo Wen"], "abstract": "Artificial intelligence (AI) has shown great promise in revolutionizing the field of digital health by improving disease diagnosis, treatment, and prevention. This paper describes the Health Guardian platform, a non-commercial, scientific research-based platform developed by the IBM Digital Health team to rapidly translate AI research into cloud-based microservices. The platform can collect health-related data from various digital devices, including wearables and mobile applications. Its flexible architecture supports microservices that accept diverse data types such as text, audio, and video, expanding the range of digital health assessments and enabling holistic health evaluations by capturing voice, facial, and motion bio-signals. These microservices can be deployed to a clinical cohort specified through the Clinical Task Manager (CTM). The CTM then collects multi-modal, clinical data that can iteratively improve the accuracy of AI predictive models, discover new disease mechanisms, or identify novel biomarkers. This paper highlights three microservices with different input data types, including a text-based microservice for depression assessment, a video-based microservice for sit-to-stand mobility assessment, and a wearable-based microservice for functional mobility assessment. The CTM is also discussed as a tool to help design and set up clinical studies to unlock the full potential of the platform. Today, the Health Guardian platform is being leveraged in collaboration with research partners to optimize the development of AI models by utilizing a multitude of input sources. This approach streamlines research efforts, enhances efficiency, and facilitates the development and validation of digital health applications.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01733", "tags": ["AI and Privacy", "Ethical AI and Bias Mitigation", "AI in Healthcare"]}, {"title": "Forecasting Tropical Cyclones with Cascaded Diffusion Models", "authors": ["Pritthijit Nath", "Pancham Shukla", "C\u00e9sar Quilodr\u00e1n-Casas"], "abstract": "As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with critical forecasting needs and financial limitations. Code accessible at \\url{https://github.com/nathzi1505/forecast-diffmodels}.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.01690", "tags": []}, {"title": "Designing User-Centric Behavioral Interventions to Prevent Dysglycemia with Novel Counterfactual Explanations", "authors": ["Asiful Arefeen", "Hassan Ghasemzadeh"], "abstract": "Maintaining normal blood glucose levels through lifestyle behaviors is central to maintaining health and preventing disease. Frequent exposure to dysglycemia (i.e., abnormal glucose events such as hyperlycemia and hypoglycemia) leads to chronic complications including diabetes, kidney disease and need for dialysis, myocardial infarction, stroke, amputation, and death. Therefore, a tool capable of predicting dysglycemia and offering users actionable feedback about how to make changes in their diet, exercise, and medication to prevent abnormal glycemic events could have significant societal impacts. Counterfactual explanations can provide insights into why a model made a particular prediction by generating hypothetical instances that are similar to the original input but lead to a different prediction outcome. Therefore, counterfactuals can be viewed as a means to design AI-driven health interventions to prevent adverse health outcomes such as dysglycemia. In this paper, we design GlyCoach, a framework for generating counterfactual explanations for glucose control. Leveraging insights from adversarial learning, GlyCoach characterizes the decision boundary for high-dimensional health data and performs a grid search to generate actionable interventions. GlyCoach is unique in integrating prior knowledge about user preferences of plausible explanations into the process of counterfactual generation. We evaluate GlyCoach extensively using two real-world datasets and external simulators from prior studies that predict glucose response. GlyCoach achieves 87\\% sensitivity in the simulation-aided validation, surpassing the state-of-the-art techniques for generating counterfactual explanations by at least $10\\%$. Besides, counterfactuals from GlyCoach exhibit a $32\\%$ improved normalized distance compared to previous research.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01684", "tags": []}, {"title": "Adaptive Visual Scene Understanding: Incremental Scene Graph Generation", "authors": ["Naitik Khandelwal", "Xiao Liu", "Mengmi Zhang"], "abstract": "Scene graph generation (SGG) involves analyzing images to extract meaningful information about objects and their relationships. Given the dynamic nature of the visual world, it becomes crucial for AI systems to detect new objects and establish their new relationships with existing objects. To address the lack of continual learning methodologies in SGG, we introduce the comprehensive Continual ScenE Graph Generation (CSEGG) dataset along with 3 learning scenarios and 8 evaluation metrics. Our research investigates the continual learning performances of existing SGG methods on the retention of previous object entities and relationships as they learn new ones. Moreover, we also explore how continual object detection enhances generalization in classifying known relationships on unknown objects. We conduct extensive experiments benchmarking and analyzing the classical two-stage SGG methods and the most recent transformer-based SGG methods in continual learning settings, and gain valuable insights into the CSEGG problem. We invite the research community to explore this emerging field of study.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.01636", "tags": []}, {"title": "3D photonics for ultra-low energy, high bandwidth-density chip data links", "authors": ["Stuart Daudlin", "Anthony Rizzo", "Sunwoo Lee", "Devesh Khilwani", "Christine Ou", "Songli Wang", "Asher Novick", "Vignesh Gopal", "Michael Cullen", "Robert Parsons", "Alyosha Molnar", "Keren Bergman"], "abstract": "Artificial intelligence (AI) hardware is positioned to unlock revolutionary computational abilities across diverse fields ranging from fundamental science [1] to medicine [2] and environmental science [3] by leveraging advanced semiconductor chips interconnected in vast distributed networks. However, AI chip development has far outpaced that of the networks that connect them, as chip computation speeds have accelerated a thousandfold faster than communication bandwidth over the last two decades [4, 5]. This gap is the largest barrier for scaling AI performance [6, 7] and results from the disproportionately high energy expended to transmit data [8], which is two orders of magnitude more intensive than computing [9]. Here, we show a leveling of this long-standing discrepancy and achieve the lowest energy optical data link to date through dense 3D integration of photonic and electronic chips. At 120 fJ of consumed energy per communicated bit and 5.3 Tb/s bandwidth per square millimeter of chip area, our platform simultaneously achieves a twofold improvement in both energy consumption and bandwidth density relative to prior demonstrations [10, 11]. These improvements are realized through employing massively parallel 80 channel microresonator-based transmitter and receiver arrays operating at 10 Gb/s per channel, occupying a combined chip footprint of only 0.32 mm2. Furthermore, commercial complementary metal-oxide-semiconductor (CMOS) foundries fabricate both the electronic and photonic chips on 300 mm wafers, providing a clear avenue to volume scaling. Through these demonstrated ultra-energy efficient, high bandwidth data communication links, this work eliminates the bandwidth bottleneck between spatially distanced compute nodes and will enable a fundamentally new scale of future AI computing hardware without constraints on data locality.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01615", "tags": ["AI and Privacy"]}, {"title": "A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education", "authors": ["Xiaoyi Tian", "Kristy Elizabeth Boyer"], "abstract": "Natural Language Processing (NLP) plays a significant role in our daily lives and has become an essential part of Artificial Intelligence (AI) education in K-12. As children grow up with NLP-powered applications, it is crucial to introduce NLP concepts to them, fostering their understanding of language processing, language generation, and ethical implications of AI and NLP. This paper presents a comprehensive review of digital learning environments for teaching NLP in K-12. Specifically, it explores existing digital learning tools, discusses how they support specific NLP tasks and procedures, and investigates their explainability and evaluation results in educational contexts. By examining the strengths and limitations of these tools, this literature review sheds light on the current state of NLP learning tools in K-12 education. It aims to guide future research efforts to refine existing tools, develop new ones, and explore more effective and inclusive strategies for integrating NLP into K-12 educational contexts.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01603", "tags": ["AI in Education"]}, {"title": "Fusing Models with Complementary Expertise", "authors": ["Hongyi Wang", "Felipe Maia Polo", "Yuekai Sun", "Souvik Kundu", "Eric Xing", "Mikhail Yurochkin"], "abstract": "Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the \"frugal\" setting where it is desired to reduce the number of expert model evaluations at test time.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01542", "tags": ["Natural Language Processing"]}, {"title": "Shaping of Magnetic Field Coils in Fusion Reactors using Bayesian Optimisation", "authors": ["Timothy Nunn", "Vignesh Gopakumar", "Sebastien Kahn"], "abstract": "Nuclear fusion using magnetic confinement holds promise as a viable method for sustainable energy. However, most fusion devices have been experimental and as we move towards energy reactors, we are entering into a new paradigm of engineering. Curating a design for a fusion reactor is a high-dimensional multi-output optimisation process. Through this work we demonstrate a proof-of-concept of an AI-driven strategy to help explore the design search space and identify optimum parameters. By utilising a Multi-Output Bayesian Optimisation scheme, our strategy is capable of identifying the Pareto front associated with the optimisation of the toroidal field coil shape of a tokamak. The optimisation helps to identify design parameters that would minimise the costs incurred while maximising the plasma stability by way of minimising magnetic ripples.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01455", "tags": []}, {"title": "Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile", "authors": ["Samuel Carreira", "Tom\u00e1s Marques", "Jos\u00e9 Ribeiro", "Carlos Grilo"], "abstract": "The field of Artificial Intelligence has witnessed remarkable progress in recent years, especially with the emergence of powerful large language models (LLMs) based on the transformer architecture. Cloud-based LLMs, such as OpenAI's ChatGPT, offer impressive capabilities but come with concerns regarding latency and privacy due to network dependencies. This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity. The article showcases a fine-tuned GPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of memory. Through the integration of native code and model quantization techniques, the application not only serves as a general-purpose assistant but also facilitates seamless mobile interactions with text-to-actions features. The article provides insights into the training pipeline, implementation details, test results, and future directions of on-device LLM inference. This breakthrough technology opens up possibilities for empowering users with sophisticated AI capabilities while preserving their privacy and eliminating latency concerns.", "submitted": "2023-09-29", "link": "https://arxiv.org/pdf/2310.01434", "tags": ["AI in Education", "AI in Healthcare", "AI and Privacy", "Natural Language Processing"]}, {"title": "AI-Aristotle: A Physics-Informed framework for Systems Biology Gray-Box Identification", "authors": ["Nazanin Ahmadi Daryakenari", "Mario De Florio", "Khemraj Shukla", "George Em Karniadakis"], "abstract": "Discovering mathematical equations that govern physical and biological systems from observed data is a fundamental challenge in scientific research. We present a new physics-informed framework for parameter estimation and missing physics identification (gray-box) in the field of Systems Biology. The proposed framework -- named AI-Aristotle -- combines eXtreme Theory of Functional Connections (X-TFC) domain-decomposition and Physics-Informed Neural Networks (PINNs) with symbolic regression (SR) techniques for parameter discovery and gray-box identification. We test the accuracy, speed, flexibility and robustness of AI-Aristotle based on two benchmark problems in Systems Biology: a pharmacokinetics drug absorption model, and an ultradian endocrine model for glucose-insulin interactions. We compare the two machine learning methods (X-TFC and PINNs), and moreover, we employ two different symbolic regression techniques to cross-verify our results. While the current work focuses on the performance of AI-Aristotle based on synthetic data, it can equally handle noisy experimental data and can even be used for black-box identification in just a few minutes on a laptop. More broadly, our work provides insights into the accuracy, cost, scalability, and robustness of integrating neural networks with symbolic regressors, offering a comprehensive guide for researchers tackling gray-box identification challenges in complex dynamical systems in biomedicine and beyond.", "submitted": "2023-09-29", "link": "https://arxiv.org/pdf/2310.01433", "tags": ["Large scale Machine Learning"]}, {"title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators", "authors": ["Zongjie Li", "Chaozheng Wang", "Pingchuan Ma", "Daoyuan Wu", "Shuai Wang", "Cuiyun Gao", "Yang Liu"], "abstract": "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables less advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4 model at just 10% of the cost. Furthermore, it rectifies around 80% of the position bias instances within the GPT-4 model, elevating its consistency rate up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with human evaluators. These findings highlight PORTIA's ability to correct position bias, improve LLM consistency, and boost performance while keeping cost-efficiency. This represents a valuable step toward a more reliable and scalable use of LLMs for automated evaluations across diverse applications.", "submitted": "2023-10-09", "link": "https://arxiv.org/pdf/2310.01432", "tags": ["Natural Language Processing", "Ethical AI and Bias Mitigation", "AI in Healthcare"]}, {"title": "Chatmap : Large Language Model Interaction with Cartographic Data", "authors": ["Eren Unlu"], "abstract": "The swift advancement and widespread availability of foundational Large Language Models (LLMs), complemented by robust fine-tuning methodologies, have catalyzed their adaptation for innovative and industrious applications. Enabling LLMs to recognize and interpret geospatial data, while offering a linguistic access to vast cartographic datasets, is of significant importance. OpenStreetMap (OSM) is the most ambitious open-source global initiative offering detailed urban and rural geographic data, curated by a community of over 10 million contributors, which constitutes a great potential for LLM applications. In this study, we demonstrate the proof of concept and details of the process of fine-tuning a relatively small scale (1B parameters) LLM with a relatively small artificial dataset curated by a more capable teacher model, in order to provide a linguistic interface to the OSM data of an arbitrary urban region. Through this interface, users can inquire about a location's attributes, covering a wide spectrum of concepts, such as its touristic appeal or the potential profitability of various businesses in that vicinity. The study aims to provide an initial guideline for such generative artificial intelligence (AI) adaptations and demonstrate early signs of useful emerging abilities in this context even in minimal computational settings. The embeddings of artificially curated prompts including OSM data are also investigated in detail, which might be instrumental for potential geospatially aware urban Retrieval Augmented Generation (RAG) applications.", "submitted": "2023-09-28", "link": "https://arxiv.org/pdf/2310.01429", "tags": ["AI and Privacy", "Large scale Machine Learning"]}, {"title": "Borges and AI", "authors": ["L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf"], "abstract": "Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.", "submitted": "2023-10-04", "link": "https://arxiv.org/pdf/2310.01425", "tags": ["Ethical AI and Bias Mitigation", "AI in Education"]}, {"title": "An Empirical Study of AI Generated Text Detection Tools", "authors": ["Arslan Akram"], "abstract": "Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial intelligence (AI) text identification systems, including \"GPTkit,\" \"GPTZero,\" \"Originality,\" \"Sapling,\" \"Writer,\" and \"Zylalab,\" have accuracy rates between 55.29 and 97.0%. Although all the tools fared well in the evaluations, originality was particularly effective across the board.", "submitted": "2023-09-27", "link": "https://arxiv.org/pdf/2310.01423", "tags": ["AI in Education"]}, {"title": "Service Pet Robot Design: Queer, Feminine and Sexuality Aspects", "authors": ["Anna-Maria Velentza", "Antigoni Tsagkaropoulou"], "abstract": "The integration of robots and AI in society raises concerns about discrimination and biases mostly affecting underrepresented groups, including queer and feminine figures. Socially assistive robots (SAR) are being used in a variety of service and companion roles, following social norms during their interaction with humans and seem to be beneficial in many roles, such as the pet therapy robots. To promote inclusion and representation, robot design should incorporate queer and feminine characteristics. As a response to these concerns, a pet robot called BB was designed using a multidisciplinary and inclusive approach. BB was presented in a queer architecture and aesthetics environment, emphasizing aspects of techno-touch, vulnerability, and sexuality in human-robot interactions. The audience's perception of both the robot and the female researcher was evaluated through questionnaires and focus groups. This study aims to explore how technology and design can better accommodate diverse perspectives and needs in the field of SAR.", "submitted": "2023-09-27", "link": "https://arxiv.org/pdf/2310.01422", "tags": []}, {"title": "A multi-institutional pediatric dataset of clinical radiology MRIs by the Children's Brain Tumor Network", "authors": ["Ariana M. Familiar", "Anahita Fathi Kazerooni", "Hannah Anderson", "Aliaksandr Lubneuski", "Karthik Viswanathan", "Rocky Breslow", "Nastaran Khalili", "Sina Bagheri", "Debanjan Haldar", "Meen Chul Kim", "Sherjeel Arif", "Rachel Madhogarhia", "Thinh Q. Nguyen", "Elizabeth A. Frenkel", "Zeinab Helili", "Jessica Harrison", "Keyvan Farahani", "Marius George Linguraru", "Ulas Bagci", "Yury Velichko", "Jeffrey Stevens", "Sarah Leary", "Robert M. Lober", "Stephani Campion", "Amy A. Smith", "et al. (15 additional authors not shown)"], "abstract": "Pediatric brain and spinal cancers remain the leading cause of cancer-related death in children. Advancements in clinical decision-support in pediatric neuro-oncology utilizing the wealth of radiology imaging data collected through standard care, however, has significantly lagged other domains. Such data is ripe for use with predictive analytics such as artificial intelligence (AI) methods, which require large datasets. To address this unmet need, we provide a multi-institutional, large-scale pediatric dataset of 23,101 multi-parametric MRI exams acquired through routine care for 1,526 brain tumor patients, as part of the Children's Brain Tumor Network. This includes longitudinal MRIs across various cancer diagnoses, with associated patient-level clinical information, digital pathology slides, as well as tissue genotype and omics data. To facilitate downstream analysis, treatment-na\u00efve images for 370 subjects were processed and released through the NCI Childhood Cancer Data Initiative via the Cancer Data Service. Through ongoing efforts to continuously build these imaging repositories, our aim is to accelerate discovery and translational AI models with real-world data, to ultimately empower precision medicine for children.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01413", "tags": ["AI and Privacy"]}, {"title": "Representation Engineering: A Top-Down Approach to AI Transparency", "authors": ["Andy Zou", "Long Phan", "Sarah Chen", "James Campbell", "Phillip Guo", "Richard Ren", "Alexander Pan", "Xuwang Yin", "Mantas Mazeika", "Ann-Kathrin Dombrowski", "Shashwat Goel", "Nathaniel Li", "Michael J. Byun", "Zifan Wang", "Alex Mallen", "Steven Basart", "Sanmi Koyejo", "Dawn Song", "Matt Fredrikson", "J. Zico Kolter", "Dan Hendrycks"], "abstract": "In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.", "submitted": "2023-10-10", "link": "https://arxiv.org/pdf/2310.01405", "tags": []}, {"title": "Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation", "authors": ["Shenzhi Wang", "Chang Liu", "Zilong Zheng", "Siyuan Qi", "Shuo Chen", "Qisen Yang", "Andrew Zhao", "Chaofei Wang", "Shiji Song", "Gao Huang"], "abstract": "Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a \"Game-of-Thoughts\". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. Additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. Specifically, the first-order allows an LLM agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. After integrating ReCon with different LLMs, extensive experiment results from the Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around deceptive information without extra fine-tuning and data. Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.", "submitted": "2023-10-06", "link": "https://arxiv.org/pdf/2310.01320", "tags": []}, {"title": "Co-audit: tools to help humans double-check AI-generated content", "authors": ["Andrew D. Gordon", "Carina Negreanu", "Jos\u00e9 Cambronero", "Rasika Chakravarthy", "Ian Drosos", "Hao Fang", "Bhaskar Mitra", "Hannah Richardson", "Advait Sarkar", "Stephanie Simmons", "Jack Williams", "Ben Zorn"], "abstract": "Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01297", "tags": []}, {"title": "Grasping AI: experiential exercises for designers", "authors": ["Dave Murray-Rust", "Maria Luce Lupetti", "Iohanna Nicenboim", "Wouter van der Hoog"], "abstract": "Artificial intelligence (AI) and machine learning (ML) are increasingly integrated into the functioning of physical and digital products, creating unprecedented opportunities for interaction and functionality. However, there is a challenge for designers to ideate within this creative landscape, balancing the possibilities of technology with human interactional concerns. We investigate techniques for exploring and reflecting on the interactional affordances, the unique relational possibilities, and the wider social implications of AI systems. We introduced into an interaction design course (n=100) nine 'AI exercises' that draw on more than human design, responsible AI, and speculative enactment to create experiential engagements around AI interaction design. We find that exercises around metaphors and enactments make questions of training and learning, privacy and consent, autonomy and agency more tangible, and thereby help students be more reflective and responsible on how to design with AI and its complex properties in both their design process and outcomes.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01282", "tags": ["AI and Privacy", "Ethical AI and Bias Mitigation", "AI in Education"]}, {"title": "The benefits and costs of explainable artificial intelligence in visual quality control: Evidence from fault detection performance and eye movements", "authors": ["Romy M\u00fcller", "David F. Reindel", "Yannick D. Stadtfeld"], "abstract": "Visual inspection tasks often require humans to cooperate with AI-based image classifiers. To enhance this cooperation, explainable artificial intelligence (XAI) can highlight those image areas that have contributed to an AI decision. However, the literature on visual cueing suggests that such XAI support might come with costs of its own. To better understand how the benefits and cost of XAI depend on the accuracy of AI classifications and XAI highlights, we conducted two experiments that simulated visual quality control in a chocolate factory. Participants had to decide whether chocolate moulds contained faulty bars or not, and were always informed whether the AI had classified the mould as faulty or not. In half of the experiment, they saw additional XAI highlights that justified this classification. While XAI speeded up performance, its effects on error rates were highly dependent on (X)AI accuracy. XAI benefits were observed when the system correctly detected and highlighted the fault, but XAI costs were evident for misplaced highlights that marked an intact area while the actual fault was located elsewhere. Eye movement analyses indicated that participants spent less time searching the rest of the mould and thus looked at the fault less often. However, we also observed large interindividual differences. Taken together, the results suggest that despite its potentials, XAI can discourage people from investing effort into their own information analysis.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01220", "tags": ["Explainable AI (XAI):"]}, {"title": "Making LLaMA SEE and Draw with SEED Tokenizer", "authors": ["Yuying Ge", "Sijie Zhao", "Ziyun Zeng", "Yixiao Ge", "Chen Li", "Xintao Wang", "Ying Shan"], "abstract": "The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce SEED, an elaborate image tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01218", "tags": []}, {"title": "Unified Uncertainty Calibration", "authors": ["Kamalika Chaudhuri", "David Lopez-Paz"], "abstract": "To build robust, fair, and safe AI systems, we would like our classifiers to say ``I don't know'' when facing test examples that are difficult or fall outside of the training classes.The ubiquitous strategy to predict under uncertainty is the simplistic \\emph{reject-or-classify} rule: abstain from prediction if epistemic uncertainty is high, classify otherwise.Unfortunately, this recipe does not allow different sources of uncertainty to communicate with each other, produces miscalibrated predictions, and it does not allow to correct for misspecifications in our uncertainty estimates. To address these three issues, we introduce \\emph{unified uncertainty calibration (U2C)}, a holistic framework to combine aleatoric and epistemic uncertainties. U2C enables a clean learning-theoretical analysis of uncertainty estimation, and outperforms reject-or-classify across a variety of ImageNet benchmarks.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01202", "tags": []}, {"title": "Towards human-like spoken dialogue generation between AI agents from written dialogue", "authors": ["Kentaro Mitsui", "Yukiya Hono", "Kei Sawada"], "abstract": "The advent of large language models (LLMs) has made it possible to generate natural written dialogues between two agents. However, generating human-like spoken dialogues from these written dialogues remains challenging. Spoken dialogues have several unique characteristics: they frequently include backchannels and laughter, and the smoothness of turn-taking significantly influences the fluidity of conversation. This study proposes CHATS - CHatty Agents Text-to-Speech - a discrete token-based system designed to generate spoken dialogues based on written dialogues. Our system can generate speech for both the speaker side and the listener side simultaneously, using only the transcription from the speaker side, which eliminates the need for transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural turn-taking; it determines the appropriate duration of silence after each utterance in the absence of overlap, and it initiates the generation of overlapping speech based on the phoneme sequence of the next utterance in case of overlap. Experimental evaluations indicate that CHATS outperforms the text-to-speech baseline, producing spoken dialogues that are more interactive and fluid while retaining clarity and intelligibility.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01088", "tags": []}, {"title": "On Fulfilling the Exigent Need for Automating and Modernizing Logistics Infrastructure in India: Enabling AI-based Integration, Digitalization, and Smart Automation of Industrial Parks and Robotic Warehouses", "authors": ["Shaurya Shriyam", "Prashant Palkar", "Amber Srivastava"], "abstract": "To stay competitive, the Low- or Middle-Income Countries (LMICs) need to embrace Industry 4.0 and Logistics 4.0. This requires government-level interventions and policy-making to incentivize quality product solutions and drive innovation in traditionally resistant economic sectors. In this position paper, we support the establishment of Smart Industrial Parks (SIPs) with a focus on enhancing operational efficiencies and bringing together MSMEs and startups targeting niche clientele with innovative Industry 4.0 solutions. SIPs along with the phased deployment of well-planned robotic automation technologies shall enable bringing down India's untenable logistics costs. Toward the successful execution of SIPs, we are required to implement the efficient allocation of manufacturing resources and capabilities within SIPs. Thus, we emphasize the importance of efficient resource utilization, collaboration, and technology adoption in industrial parks to promote industrial development and economic growth. We advocate the use of a cloud-based cyber-physical system for real-time data access and analysis in SIPs. Such centralized cloud-based monitoring of factory floors, warehouses, and industrial units using IoT infrastructure shall improve decision-making, efficiency, and safety. Digital Twins (DTs), which are cyber-replicas of physical systems, could play a significant role in enabling simulation, optimization, and real-time monitoring of smart manufacturing and distributed manufacturing systems. However, there are several challenges involved in implementing DTs in distributed manufacturing systems, such as defining data schemas and collaboration protocols, ensuring interoperability, the need for effective authentication technology, distributed machine learning models, and scalability to manage multiple DTs.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01077", "tags": []}, {"title": "Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models", "authors": ["Chenhan Yuan", "Qianqian Xie", "Jimin Huang", "Sophia Ananiadou"], "abstract": "Temporal reasoning is a crucial NLP task, providing a nuanced understanding of time-sensitive contexts within textual data. Although recent advancements in LLMs have demonstrated their potential in temporal reasoning, the predominant focus has been on tasks such as temporal expression and temporal relation extraction. These tasks are primarily designed for the extraction of direct and past temporal cues and to engage in simple reasoning processes. A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp. Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability. In this paper, we introduce the first task of explainable temporal reasoning, to predict an event's occurrence at a future timestamp based on context which requires multiple reasoning over multiple events, and subsequently provide a clear explanation for their prediction. Our task offers a comprehensive evaluation of both the LLMs' complex temporal reasoning ability, the future event prediction ability, and explainability-a critical attribute for AI applications. To support this task, we present the first multi-source instruction-tuning dataset of explainable temporal reasoning (ExpTime) with 26k derived from the temporal knowledge graph datasets and their temporal reasoning paths, using a novel knowledge-graph-instructed-generation strategy. Based on the dataset, we propose the first open-source LLM series TimeLlaMA based on the foundation LlaMA2, with the ability of instruction following for explainable temporal reasoning. We compare the performance of our method and a variety of LLMs, where our method achieves the state-of-the-art performance of temporal prediction and explanation.", "submitted": "2023-10-08", "link": "https://arxiv.org/pdf/2310.01074", "tags": []}, {"title": "Epistemic integration and social segregation of AI in neuroscience", "authors": ["Sylvain Fontaine", "Floriana Gargiulo", "Michel Dubois", "Paola Tubaro"], "abstract": "In recent years, Artificial Intelligence (AI) shows a spectacular ability of insertion inside a variety of disciplines which use it for scientific advancements and which sometimes improve it for their conceptual and methodological needs. According to the transverse science framework originally conceived by Shinn and Joerges, AI can be seen as an instrument which is progressively acquiring an universal character through its diffusion across science. In this paper we address empirically one aspect of this diffusion, namely the penetration of AI into a specific field of research. Taking neuroscience as a case study, we conduct a scientometric analysis of the development of AI in this field We especially study the temporal egocentric citation network around the articles included in this literature, their represented journals and their authors linked together by a temporal collaboration network. We find that AI is driving the constitution of a particular disciplinary ecosystem in neuroscience which is distinct from other subfields when regarding the references, and which is gathering atypical scientific profiles who are coming from neuroscience or outside it. Moreover we observe that this AI community in neuroscience is socially confined in a specific zone of the neuroscience collaboration network, which is also keeping to publish in a small set of dedicated journals that are mostly active in AI research. According to these results, the diffusion of AI in a discipline such as neuroscience didn't really challenge its disciplinary orientations but rather induced the constitution of a dedicated socio-cognitive workforce inside this field.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01046", "tags": ["Ethical AI and Bias Mitigation", "AI in Education", "AI and Privacy"]}, {"title": "Generative AI for Integrated Sensing and Communication: Insights from the Physical Layer Perspective", "authors": ["Jiacheng Wang", "Hongyang Du", "Dusit Niyato", "Jiawen Kang", "Shuguang Cui", "Xuemin", "Shen", "Ping Zhang"], "abstract": "As generative artificial intelligence (GAI) models continue to evolve, their generative capabilities are increasingly enhanced and being used extensively in content generation. Beyond this, GAI also excels in data modeling and analysis, benefitting wireless communication systems. In this article, we investigate applications of GAI in the physical layer and analyze its support for integrated sensing and communications (ISAC) systems. Specifically, we first provide an overview of GAI and ISAC, touching on GAI's potential support across multiple layers of ISAC. We then concentrate on the physical layer, investigating GAI's applications from various perspectives thoroughly, such as channel estimation, and demonstrate the value of these GAI-enhanced physical layer technologies for ISAC systems. In the case study, the proposed diffusion model-based method effectively estimates the signal direction of arrival under the near-field condition based on the uniform linear array, when antenna spacing surpassing half the wavelength. With a mean square error of 1.03 degrees, it confirms GAI's support for the physical layer in near-field sensing and communications.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.01036", "tags": []}, {"title": "EALM: Introducing Multidimensional Ethical Alignment in Conversational Information Retrieval", "authors": ["Yiyao Yu", "Junjie Wang", "Yuxiang Zhang", "Lin Zhang", "Yujiu Yang", "Tetsuya Sakai"], "abstract": "Artificial intelligence (AI) technologies should adhere to human norms to better serve our society and avoid disseminating harmful or misleading information, particularly in Conversational Information Retrieval (CIR). Previous work, including approaches and datasets, has not always been successful or sufficiently robust in taking human norms into consideration. To this end, we introduce a workflow that integrates ethical alignment, with an initial ethical judgment stage for efficient data screening. To address the need for ethical judgment in CIR, we present the QA-ETHICS dataset, adapted from the ETHICS benchmark, which serves as an evaluation tool by unifying scenarios and label meanings. However, each scenario only considers one ethical concept. Therefore, we introduce the MP-ETHICS dataset to evaluate a scenario under multiple ethical concepts, such as justice and Deontology. In addition, we suggest a new approach that achieves top performance in both binary and multi-label ethical judgment tasks. Our research provides a practical method for introducing ethical alignment into the CIR workflow. The data and code are available at https://github.com/wanng-ide/ealm .", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.00970", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy"]}, {"title": "BAAF: A Benchmark Attention Adaptive Framework for Medical Ultrasound Image Segmentation Tasks", "authors": ["Gongping Chen", "Lei Zhao", "Xiaotao Yin", "Liang Cui", "Jianxun Zhang", "Yu Dai"], "abstract": "The AI-based assisted diagnosis programs have been widely investigated on medical ultrasound images. Complex scenario of ultrasound image, in which the coupled interference of internal and external factors is severe, brings a unique challenge for localize the object region automatically and precisely in ultrasound images. In this study, we seek to propose a more general and robust Benchmark Attention Adaptive Framework (BAAF) to assist doctors segment or diagnose lesions and tissues in ultrasound images more quickly and accurately. Different from existing attention schemes, the BAAF consists of a parallel hybrid attention module (PHAM) and an adaptive calibration mechanism (ACM). Specifically, BAAF first coarsely calibrates the input features from the channel and spatial dimensions, and then adaptively selects more robust lesion or tissue characterizations from the coarse-calibrated feature maps. The design of BAAF further optimizes the \"what\" and \"where\" focus and selection problems in CNNs and seeks to improve the segmentation accuracy of lesions or tissues in medical ultrasound images. The method is evaluated on four medical ultrasound segmentation tasks, and the adequate experimental results demonstrate the remarkable performance improvement over existing state-of-the-art methods. In addition, the comparison with existing attention mechanisms also demonstrates the superiority of BAAF. This work provides the possibility for automated medical ultrasound assisted diagnosis and reduces reliance on human accuracy and precision.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.00919", "tags": ["Computer Vision"]}, {"title": "The Participatory Turn in AI Design: Theoretical Foundations and the Current State of Practice", "authors": ["Fernando Delgado", "Stephen Yang", "Michael Madaio", "Qian Yang"], "abstract": "Despite the growing consensus that stakeholders affected by AI systems should participate in their design, enormous variation and implicit disagreements exist among current approaches. For researchers and practitioners who are interested in taking a participatory approach to AI design and development, it remains challenging to assess the extent to which any participatory approach grants substantive agency to stakeholders. This article thus aims to ground what we dub the \"participatory turn\" in AI design by synthesizing existing theoretical literature on participation and through empirical investigation and critique of its current practices. Specifically, we derive a conceptual framework through synthesis of literature across technology design, political theory, and the social sciences that researchers and practitioners can leverage to evaluate approaches to participation in AI design. Additionally, we articulate empirical findings concerning the current state of participatory practice in AI design based on an analysis of recently published research and semi-structured interviews with 12 AI researchers and practitioners. We use these empirical findings to understand the current state of participatory practice and subsequently provide guidance to better align participatory goals and methods in a way that accounts for practical constraints.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.00907", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy", "AI in Education"]}, {"title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models", "authors": ["Yongchan Kwon", "Eric Wu", "Kevin Wu", "James Zou"], "abstract": "Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled.", "submitted": "2023-10-02", "link": "https://arxiv.org/pdf/2310.00902", "tags": []}, {"title": "GRID: A Platform for General Robot Intelligence Development", "authors": ["Sai Vemprala", "Shuhang Chen", "Abhinav Shukla", "Dinesh Narayanan", "Ashish Kapoor"], "abstract": "Developing machine intelligence abilities in robots and autonomous systems is an expensive and time consuming process. Existing solutions are tailored to specific applications and are harder to generalize. Furthermore, scarcity of training data adds a layer of complexity in deploying deep machine learning models. We present a new platform for General Robot Intelligence Development (GRID) to address both of these issues. The platform enables robots to learn, compose and adapt skills to their physical capabilities, environmental constraints and goals. The platform addresses AI problems in robotics via foundation models that know the physical world. GRID is designed from the ground up to be extensible to accommodate new types of robots, vehicles, hardware platforms and software protocols. In addition, the modular design enables various deep ML components and existing foundation models to be easily usable in a wider variety of robot-centric problems. We demonstrate the platform in various aerial robotics scenarios and demonstrate how the platform dramatically accelerates development of machine intelligent robots.", "submitted": "2023-10-07", "link": "https://arxiv.org/pdf/2310.00887", "tags": ["AI in Autonomous Vehicles"]}, {"title": "QCFE: An efficient Feature engineering for query cost estimation", "authors": ["Yu Yan", "Hongzhi Wang", "Junfang Huang", "Dake Zhong", "Man Yang", "Kaixin Zhang", "Tao Yu", "Tianqing Wan"], "abstract": "Query cost estimation is a classical task for database management. Recently, researchers apply the AI-driven model to implement query cost estimation for achieving high accuracy. However, two defects of feature design lead to poor cost estimation accuracy-time efficiency. On the one hand, existing works only encode the query plan and data statistics while ignoring some other important variables, like storage structure, hardware, database knobs, etc. These variables also have significant impact on the query cost. On the other hand, due to the straightforward encoding design, existing works suffer heavy representation learning burden on ineffective dimensions of input. To meet the above two problems, we first propose an efficient feature engineering for query cost estimation, called QCFE. Specifically, we design a novel feature called feature snapshot to efficiently integrate the influences of the ignored variables. Further, we propose a difference-propagation feature reduction method for query cost estimation to filter the useless features. The experimental results demonstrate our QCFE could largely improve the time-accuracy efficiency on extensive benchmarks.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00877", "tags": []}, {"title": "Data Science at the Singularity", "authors": ["David Donoho"], "abstract": "A purported `AI Singularity' has been in the public eye recently. Mass media and US national political attention focused on `AI Doom' narratives hawked by social media influencers. The European Commission is announcing initiatives to forestall `AI Extinction'. In my opinion, `AI Singularity' is the wrong narrative for what's happening now; recent happenings signal something else entirely. Something fundamental to computation-based research really changed in the last ten years. In certain fields, progress is dramatically more rapid than previously, as the fields undergo a transition to frictionless reproducibility (FR). This transition markedly changes the rate of spread of ideas and practices, affects mindsets, and erases memories of much that came before.\n  The emergence of frictionless reproducibility follows from the maturation of 3 data science principles in the last decade. Those principles involve data sharing, code sharing, and competitive challenges, however implemented in the particularly strong form of frictionless open services. Empirical Machine Learning (EML) is todays leading adherent field, and its consequent rapid changes are responsible for the AI progress we see. Still, other fields can and do benefit when they adhere to the same principles.\n  Many rapid changes from this maturation are misidentified. The advent of FR in EML generates a steady flow of innovations; this flow stimulates outsider intuitions that there's an emergent superpower somewhere in AI. This opens the way for PR to push worrying narratives: not only `AI Extinction', but also the supposed monopoly of big tech on AI research. The helpful narrative observes that the superpower of EML is adherence to frictionless reproducibility practices; these practices are responsible for the striking progress in AI that we see everywhere.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00865", "tags": ["Ethical AI and Bias Mitigation", "AI and Privacy"]}, {"title": "Learning to Make Adherence-Aware Advice", "authors": ["Guanting Chen", "Xiaocheng Li", "Chunlin Sun", "Hanzhao Wang"], "abstract": "As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00817", "tags": ["AI and Privacy"]}, {"title": "A Comprehensive Review of Generative AI in Healthcare", "authors": ["Yasin Shokrollahi", "Sahar Yarmohammadtoosky", "Matthew M. Nikahd", "Pengfei Dong", "Xianqi Li", "Linxia Gu"], "abstract": "The advancement of Artificial Intelligence (AI) has catalyzed revolutionary changes across various sectors, notably in healthcare. Among the significant developments in this field are the applications of generative AI models, specifically transformers and diffusion models. These models have played a crucial role in analyzing diverse forms of data, including medical imaging (encompassing image reconstruction, image-to-image translation, image generation, and image classification), protein structure prediction, clinical documentation, diagnostic assistance, radiology interpretation, clinical decision support, medical coding, and billing, as well as drug design and molecular representation. Such applications have enhanced clinical diagnosis, data reconstruction, and drug synthesis. This review paper aims to offer a thorough overview of the generative AI applications in healthcare, focusing on transformers and diffusion models. Additionally, we propose potential directions for future research to tackle the existing limitations and meet the evolving demands of the healthcare sector. Intended to serve as a comprehensive guide for researchers and practitioners interested in the healthcare applications of generative AI, this review provides valuable insights into the current state of the art, challenges faced, and prospective future directions.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00795", "tags": ["AI in Healthcare", "AI and Privacy", "Ethical AI and Bias Mitigation"]}, {"title": "Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models", "authors": ["Soroosh Tayebi Arasteh", "Christiane Kuhl", "Marwin-Jonathan Saehn", "Peter Isfort", "Daniel Truhn", "Sven Nebelung"], "abstract": "Developing robust artificial intelligence (AI) models that generalize well to unseen datasets is challenging and usually requires large and variable datasets, preferably from multiple institutions. In federated learning (FL), a model is trained collaboratively at numerous sites that hold local datasets without exchanging them. So far, the impact of training strategy, i.e., local versus collaborative, on the diagnostic on-domain and off-domain performance of AI models interpreting chest radiographs has not been assessed. Consequently, using 610,000 chest radiographs from five institutions across the globe, we assessed diagnostic performance as a function of training strategy (i.e., local vs. collaborative), network architecture (i.e., convolutional vs. transformer-based), generalization performance (i.e., on-domain vs. off-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia, atelectasis, consolidation, pneumothorax, and no abnormality), dataset size (i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large datasets not only showed minimal performance gains with FL but, in some instances, even exhibited decreases. In contrast, smaller datasets revealed marked improvements. Thus, on-domain performance was mainly driven by training data size. However, off-domain performance leaned more on training diversity. When trained collaboratively across diverse external institutions, AI models consistently surpassed models trained locally for off-domain tasks, emphasizing FL's potential in leveraging data diversity. In conclusion, FL can bolster diagnostic privacy, reproducibility, and off-domain reliability of AI models and, potentially, optimize healthcare outcomes.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00757", "tags": ["AI and Privacy", "AI in Healthcare"]}, {"title": "GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models", "authors": ["Emilio Ferrara"], "abstract": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI's nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.", "submitted": "2023-10-12", "link": "https://arxiv.org/pdf/2310.00737", "tags": []}, {"title": "HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count", "authors": ["Noah Wiederhold", "Ava Megyeri", "DiMaggio Paris", "Sean Banerjee", "Natasha Kholgade Banerjee"], "abstract": "We present the HOH (Human-Object-Human) Handover Dataset, a large object count dataset with 136 objects, to accelerate data-driven research on handover studies, human-robot handover implementation, and artificial intelligence (AI) on handover parameter estimation from 2D and 3D data of person interactions. HOH contains multi-view RGB and depth data, skeletons, fused point clouds, grasp type and handedness labels, object, giver hand, and receiver hand 2D and 3D segmentations, giver and receiver comfort ratings, and paired object metadata and aligned 3D models for 2,720 handover interactions spanning 136 objects and 20 giver-receiver pairs-40 with role-reversal-organized from 40 participants. We also show experimental results of neural networks trained using HOH to perform grasp, orientation, and trajectory prediction. As the only fully markerless handover capture dataset, HOH represents natural human-human handover interactions, overcoming challenges with markered datasets that require specific suiting for body tracking, and lack high-resolution hand tracking. To date, HOH is the largest handover dataset in number of objects, participants, pairs with role reversal accounted for, and total interactions captured.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00723", "tags": []}, {"title": "First measurement of $\u039bN$ inelastic scattering with $\u039b$ from $e^{+} e^{-} \\rightarrow J/\u03c8\\to \u039b\\bar\u039b$", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "O. Afedulidis", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "I. Balossino", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "et al. (626 additional authors not shown)"], "abstract": "Using an $e^+ e^-$ collision data sample of $(10087 \\pm 44)\\times10^6 ~J/\u03c8$ events taken at the center-of-mass energy of $3.097~\\rm{GeV}$ by the BESIII detector at the BEPCII collider, the process $\u039b+N \\rightarrow \u03a3^+ + X$ is studied for the first time employing a novel method. The $\u03a3^{+}$ hyperons are produced by the collisions of $\u039b$ hyperons from $J/\u03c8$ decays with nuclei in the material of the BESIII detector. The total cross section of $\u039b+ ^{9}{\\rm Be} \\rightarrow \u03a3^+ + X$ is measured to be $\u03c3= (37.3 \\pm 4.7 \\pm 3.5)~{\\rm mb}$ at $\u039b$ beam momenta within $[1.057, 1.091]~{\\rm GeV}/c$, where the uncertainties are statistical and systematic, respectively. This analysis is the first study of $\u039b$-nucleon interactions at an $e^+ e^-$ collider, providing information and constraints relevant for the strong-interaction potential, the origin of color confinement, the unified model for baryon-baryon interactions, and the internal structure of neutron stars.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00720", "tags": []}, {"title": "Automatic Data Repair: Are We Ready to Deploy?", "authors": ["Wei Ni", "Xiaoye Miao", "Xiangyu Zhao", "Yangyang Wu", "Jianwei Yin"], "abstract": "Data quality is paramount in today's data-driven world, especially in the era of generative AI. Dirty data with errors and inconsistencies usually leads to flawed insights, unreliable decision-making, and biased or low-quality outputs from generative models. The study of repairing erroneous data has gained significant importance. Existing data repair algorithms differ in information utilization, problem settings, and are tested in limited scenarios. In this paper, we initially compare and summarize these algorithms using a new guided information-based taxonomy. We then systematically conduct a comprehensive evaluation of 12 mainstream data repair algorithms under the settings of various data error rates, error types, and downstream analysis tasks, assessing their error reduction performance with a novel metric. Also, we develop an effective and unified repair optimization strategy that substantially benefits the state of the arts, as empirically confirmed. We demonstrate that, the pure clean data may not necessarily yield the best performance in data analysis tasks and data is always worth repairing regardless of error rate. Based on the found observations and insights, we provide some practical guidelines for 5 scenarios and 2 main data analysis tasks. We anticipate this paper enabling researchers and users to well understand and deploy data repair algorithms in practice. Finally, we outline research challenges and promising future directions in the data repair field.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00711", "tags": ["AI and Privacy", "Large scale Machine Learning"]}, {"title": "The Robots are Here: Navigating the Generative AI Revolution in Computing Education", "authors": ["James Prather", "Paul Denny", "Juho Leinonen", "Brett A. Becker", "Ibrahim Albluwi", "Michelle Craig", "Hieke Keuning", "Natalie Kiesler", "Tobias Kohn", "Andrew Luxton-Reilly", "Stephen MacNeil", "Andrew Peterson", "Raymond Pettit", "Brent N. Reeves", "Jaromir Savelka"], "abstract": "Recent advancements in artificial intelligence (AI) are fundamentally reshaping computing, with large language models (LLMs) now effectively being able to generate and interpret source code and natural language instructions. These emergent capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of LLMs in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents who have already adapted their curricula and assessments. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of LLMs on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating LLMs and LLM-based tools in computing classrooms.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00658", "tags": ["AI in Education"]}, {"title": "Streamlining Attack Tree Generation: A Fragment-Based Approach", "authors": ["Irdin Pekaric", "Markus Frick", "Jubril Gbolahan Adigun", "Raffaela Groner", "Thomas Witte", "Alexander Raschke", "Michael Felderer", "Matthias Tichy"], "abstract": "Attack graphs are a tool for analyzing security vulnerabilities that capture different and prospective attacks on a system. As a threat modeling tool, it shows possible paths that an attacker can exploit to achieve a particular goal. However, due to the large number of vulnerabilities that are published on a daily basis, they have the potential to rapidly expand in size. Consequently, this necessitates a significant amount of resources to generate attack graphs. In addition, generating composited attack models for complex systems such as self-adaptive or AI is very difficult due to their nature to continuously change. In this paper, we present a novel fragment-based attack graph generation approach that utilizes information from publicly available information security databases. Furthermore, we also propose a domain-specific language for attack modeling, which we employ in the proposed attack graph generation approach. Finally, we present a demonstrator example showcasing the attack generator's capability to replicate a verified attack chain, as previously confirmed by security experts.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00654", "tags": ["Cyber Security"]}, {"title": "A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks", "authors": ["Yanjie Li", "Bin Xie", "Songtao Guo", "Yuanyuan Yang", "Bin Xiao"], "abstract": "Benefiting from the rapid development of deep learning, 2D and 3D computer vision applications are deployed in many safe-critical systems, such as autopilot and identity authentication. However, deep learning models are not trustworthy enough because of their limited robustness against adversarial attacks. The physically realizable adversarial attacks further pose fatal threats to the application and human safety. Lots of papers have emerged to investigate the robustness and safety of deep learning models against adversarial attacks. To lead to trustworthy AI, we first construct a general threat model from different perspectives and then comprehensively review the latest progress of both 2D and 3D adversarial attacks. We extend the concept of adversarial examples beyond imperceptive perturbations and collate over 170 papers to give an overview of deep learning model robustness against various adversarial attacks. To the best of our knowledge, we are the first to systematically investigate adversarial attacks for 3D models, a flourishing field applied to many real-world applications. In addition, we examine physical adversarial attacks that lead to safety violations. Last but not least, we summarize present popular topics, give insights on challenges, and shed light on future research on trustworthy AI.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00633", "tags": ["Generative Adversarial Networks"]}, {"title": "A Novel Computational and Modeling Foundation for Automatic Coherence Assessment", "authors": ["Aviya Maimon", "Reut Tsarfaty"], "abstract": "Coherence is an essential property of well-written texts, that refers to the way textual units relate to one another. In the era of generative AI, coherence assessment is essential for many NLP tasks; summarization, generation, long-form question-answering, and more. However, in NLP {coherence} is an ill-defined notion, not having a formal definition or evaluation metrics, that would allow for large-scale automatic and systematic coherence assessment. To bridge this gap, in this work we employ the formal linguistic definition of \\citet{Reinhart:1980} of what makes a discourse coherent, consisting of three conditions -- {\\em cohesion, consistency} and {\\em relevance} -- and formalize these conditions as respective computational tasks. We hypothesize that (i) a model trained on all of these tasks will learn the features required for coherence detection, and that (ii) a joint model for all tasks will exceed the performance of models trained on each task individually. On two benchmarks for coherence scoring rated by humans, one containing 500 automatically-generated short stories and another containing 4k real-world texts, our experiments confirm that jointly training on the proposed tasks leads to better performance on each task compared with task-specific models, and to better performance on assessing coherence overall, compared with strong baselines. We conclude that the formal and computational setup of coherence as proposed here provides a solid foundation for advanced methods of large-scale automatic assessment of coherence.", "submitted": "2023-10-01", "link": "https://arxiv.org/pdf/2310.00598", "tags": ["Natural Language Processing", "Computer Vision"]}]